{
 "cells":[
  {
   "cell_type":"markdown",
   "source":[
    "# Tugas Besar Bagian A"
   ],
   "attachments":{},
   "metadata":{
    "datalore":{
     "node_id":"Tugas Besar Bagian A",
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "sheet_delimiter":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "# Tugas Besar Pembelajaran Mesin Bagian A\n",
    "Kelompok 14\n",
    "\n",
    "**Tentang Program**\n",
    "Program dalam bahasa Python untuk membuat jaringan saraf tiruan pada bagian feed forward neural network (FFNN). \n",
    "\n",
    "\n",
    "**Pembagian Tugas**\n",
    "| NIM     | Pembagian Tugas                            |\n",
    "|---------|--------------------------------------------|\n",
    "| 13521060 | JSON dan Visualisasi Model                 |\n",
    "| 13521089 | Model dan Abstraksi                        |\n",
    "| 13521093 | Teknis untuk Komputasi Aktivation Function |\n",
    "| 13521171 | Laporan dan Test SSE                       |"
   ],
   "attachments":{},
   "metadata":{
    "datalore":{
     "node_id":"jiR4JOaUAxe2AsaLccyY23",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "import json\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple\n",
    "\n",
    "@dataclass\n",
    "class TestCase:\n",
    "    expected_output: List[List[float]]\n",
    "    input_size: int\n",
    "    layers: List[Tuple[int, str]]\n",
    "    weights: List[np.array]\n",
    "    input_data: np.array\n",
    "    max_sse: float\n",
    "\n",
    "    def print_info(self):\n",
    "        print(f\"Layers\\n{self.layers}\\n\\nWeights{self.weights}\\n\\n\")\n",
    "        print(f\"Input size\\n{self.input_size}\\n\\nInput data\\n{self.input_data}\\n\\n\")\n",
    "        print(f\"Expected output\\n{self.expected_output}\\n\\nMax SSE\\n{self.max_sse}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def from_file(filename: str):\n",
    "        json_read = json.load(open(filename, \"r\"))\n",
    "        max_sse = json_read[\"expect\"][\"max_sse\"]\n",
    "        output = json_read[\"expect\"][\"output\"]\n",
    "        input_size = json_read[\"case\"][\"model\"][\"input_size\"]\n",
    "        layers_raw = json_read[\"case\"][\"model\"][\"layers\"]\n",
    "        layers = []\n",
    "        for element in layers_raw:\n",
    "            layers.append(tuple([element[\"number_of_neurons\"], element[\"activation_function\"]]))\n",
    "\n",
    "        weights_raw = json_read[\"case\"][\"weights\"]\n",
    "        weights = []\n",
    "\n",
    "        for weight_raw in weights_raw:\n",
    "            weight = []\n",
    "            for j in weight_raw:\n",
    "                weight.append(np.array(j))\n",
    "            weights.append(np.array(weight).T)\n",
    "\n",
    "        input_data = np.array(json_read[\"case\"][\"input\"])\n",
    "\n",
    "        return TestCase(\n",
    "            expected_output=output,\n",
    "            input_size=input_size,\n",
    "            layers=layers,\n",
    "            weights=weights,\n",
    "            input_data=input_data,\n",
    "            max_sse=max_sse\n",
    "        )"
   ],
   "execution_count":362,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"5QvP9ynRIspDdoO7CyZeXf",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "import graphviz as gv\n",
    "\n",
    "class Neuron:\n",
    "    weight: np.array\n",
    "\n",
    "    def __init__(self, weight: np.array):\n",
    "        self.weight = weight\n",
    "\n",
    "    \"\"\"\n",
    "    return bias + wt•x\n",
    "    \"\"\"\n",
    "    def compute(self, input_data: np.array):\n",
    "        return self.weight[0] + np.dot(input_data, self.weight[1:])\n",
    "\n",
    "class Layer:\n",
    "    neurons: List[Neuron]\n",
    "    activation_function_name: str\n",
    "    activation_functions = {\n",
    "        \"linear\": lambda x: x,\n",
    "        \"relu\": lambda x: np.maximum(0, x),\n",
    "        \"sigmoid\": lambda x: 1\/(1+np.exp(-x)),\n",
    "        \"softmax\": lambda x: np.exp(x)\/np.sum(np.exp(x))\n",
    "    }\n",
    "\n",
    "    def __init__(self, n_neurons: int, activation_function_name: str, weights: List[np.array]):\n",
    "        self.neurons = []\n",
    "        # Initialize neurons in the layer\n",
    "        for i in range(0, n_neurons):\n",
    "            self.neurons.append(Neuron(weights[i]))\n",
    "        self.activation_function_name = activation_function_name\n",
    "\n",
    "    def predict(self, input_data: np.array) -> np.array:\n",
    "        raw_result = np.array([neuron.compute(input_data) for neuron in self.neurons])\n",
    "        return self.activation_functions[self.activation_function_name](raw_result)\n",
    "            \n",
    "           \n",
    "\n",
    "class Model:\n",
    "    input_size: int\n",
    "    layers: List[Layer]\n",
    "\n",
    "    @staticmethod\n",
    "    def from_test_case(test_case: TestCase):\n",
    "        return Model(test_case.input_size, test_case.layers, test_case.weights)\n",
    "\n",
    "    def __init__(self, input_size: int, layers_attr: List[Tuple[int, str]], weights: List[np.array]):\n",
    "        self.input_size = input_size\n",
    "        # Initialize layers\n",
    "        self.layers = []\n",
    "        for i in range(0, len(layers_attr)):\n",
    "            layer: Layer = Layer(layers_attr[i][0], layers_attr[i][1], weights[i])\n",
    "            self.layers.append(layer)\n",
    "    \n",
    "    def _predict(self, input_data: np.array) -> np.array:\n",
    "        layer: Layer = self.layers[0]\n",
    "        temp_array = layer.predict(input_data)\n",
    "        for i in range(1, len(self.layers)):\n",
    "            layer: Layer = self.layers[i]\n",
    "            temp_array = layer.predict(temp_array)\n",
    "        \n",
    "        return temp_array\n",
    "        \n",
    "    def predict_batch(self, input_data: List[np.array]) -> List[np.array]:\n",
    "        # Batch output\n",
    "        final_output = []\n",
    "        for i in input_data:\n",
    "            final_output.append(self._predict(i))\n",
    "        return final_output\n",
    "\n",
    "    def visualize(self):\n",
    "        graph = gv.Digraph(filename=\".\/output\/graph.gv\")\n",
    "        for i in range(self.input_size):\n",
    "            graph.node(f\"IN{i}\", f\"Input Neuron-{i + 1}\")\n",
    "        graph.render()\n",
    "\n",
    "    def visualize_print(self):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            print(f\"\/* Hidden Layer-{i + 1} ({layer.activation_function_name}) *\/\")\n",
    "            for j, neuron in enumerate(layer.neurons):\n",
    "                print(f\"\/* Neuron-{j + 1} *\/\")\n",
    "                if i == 0:\n",
    "                    source_layer = \"Input Layer\"\n",
    "                else:\n",
    "                    source_layer = f\"Hidden Layer-{i}\"\n",
    "                for k, w in enumerate(neuron.weight):\n",
    "                    if k == 0:\n",
    "                        continue\n",
    "                    if w != 0:\n",
    "                        print(f\"{source_layer} Neuron-{k} -> Hidden Layer-{i + 1} Neuron-{j+1} ({w})\")\n",
    "                print(\"\\n\\n\")\n",
    "            print(\"\\n\\n\")\n",
    "\n",
    "def calculate_sse(output_data: List[np.array], expected_data: List[np.array]):\n",
    "    if len(output_data) != len(expected_data):\n",
    "        raise ValueError(\"Output and Expected Data length doesn't match.\")\n",
    "    \n",
    "    sses = []\n",
    "\n",
    "    for output, expected in zip(output_data, expected_data):\n",
    "        delta = output - expected\n",
    "        squared_delta = delta ** 2\n",
    "        sses.append(np.sum(squared_delta))\n",
    "\n",
    "    return sses\n",
    "\n",
    "def evaluate_result(output_data: List[np.array], output_reference_data: np.array, expected_data: List[np.array], max_sse: float):\n",
    "    sses = calculate_sse(output_data, expected_data)\n",
    "\n",
    "    for i in range(len(output_data)):\n",
    "        predicted = output_data[i]\n",
    "        predicted_reference = output_reference_data[i]\n",
    "        expected = expected_data[i]\n",
    "        sse = sses[i]\n",
    "\n",
    "        print(f\"Prediction result:\\n{predicted}\")\n",
    "        print(f\"Prediction reference result:\\n{predicted_reference}\")\n",
    "        print(f\"Expected output:\\n{expected}\")\n",
    "\n",
    "        print(f\"sse {sse}\\tmax sse{max_sse}\")\n",
    "        print(f\"Is below error? {sse < max_sse}\")\n",
    "    "
   ],
   "execution_count":363,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"qX5Dodj7iUGTrEldLGQCtS",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "class ReferenceModel:\n",
    "    mode: Sequential\n",
    "\n",
    "    def __init__(self, input_size: int, layers_attr: List[Tuple[int, str]], weights: List[List[np.array]]):\n",
    "        model = Sequential()\n",
    "\n",
    "        for (i, (n_neuron, activation)) in enumerate(layers_attr):\n",
    "            if i == 0:\n",
    "                model.add(Dense(n_neuron, activation=activation, input_shape=(input_size,)))\n",
    "            else:\n",
    "                model.add(Dense(n_neuron, activation=activation))\n",
    "            \n",
    "        model.compile()\n",
    "\n",
    "        for i in range(len(layers_attr)):\n",
    "            layer = model.layers[i]\n",
    "\n",
    "            main_weight = np.array([each[1:] for each in weights[i]])\n",
    "            bias = np.array([each[0] for each in weights[i]])\n",
    "\n",
    "            layer.set_weights([main_weight.T, bias])\n",
    "            \n",
    "        self.model = model\n",
    "\n",
    "    @staticmethod\n",
    "    def from_test_case(test_case: TestCase):\n",
    "        return ReferenceModel(test_case.input_size, test_case.layers, test_case.weights)"
   ],
   "execution_count":364,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"oEAjMhbIgXXXk4RlJCEAfm",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "# Linear Test Case\n",
    "Test Case menggunakan linear.json dari test case bagian A Asisten."
   ],
   "attachments":{},
   "metadata":{
    "datalore":{
     "node_id":"1W4iC90DeS3AJfiYMoVVTH",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "case = \"linear.json\"\n",
    "\n",
    "print(f\"Testing {case}\\n\\n\")\n",
    "test_case = TestCase.from_file(case)\n",
    "model = Model.from_test_case(test_case)\n",
    "model.visualize_print()\n",
    "\n",
    "reference_model = ReferenceModel.from_test_case(test_case)\n",
    "\n",
    "reference_prediction = reference_model.model.predict(test_case.input_data)\n",
    "prediction = model.predict_batch(test_case.input_data)\n",
    "\n",
    "evaluate_result(prediction, reference_prediction, test_case.expected_output, test_case.max_sse)"
   ],
   "execution_count":365,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "Testing linear.json\n",
      "\n",
      "\n",
      "\/* Hidden Layer-1 (linear) *\/\n",
      "\/* Neuron-1 *\/\n",
      "Input Layer Neuron-1 -> Hidden Layer-1 Neuron-1 (3.0)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\r1\/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1\/1 [==============================] - 0s 380ms\/step\n",
      "Prediction result:\n",
      "[-11.]\n",
      "Prediction reference result:\n",
      "[-11.]\n",
      "Expected output:\n",
      "[-11]\n",
      "sse 0.0\tmax sse1e-12\n",
      "Is below error? True\n",
      "Prediction result:\n",
      "[-8.]\n",
      "Prediction reference result:\n",
      "[-8.]\n",
      "Expected output:\n",
      "[-8]\n",
      "sse 0.0\tmax sse1e-12\n",
      "Is below error? True\n",
      "Prediction result:\n",
      "[-5.]\n",
      "Prediction reference result:\n",
      "[-5.]\n",
      "Expected output:\n",
      "[-5]\n",
      "sse 0.0\tmax sse1e-12\n",
      "Is below error? True\n",
      "Prediction result:\n",
      "[-2.]\n",
      "Prediction reference result:\n",
      "[-2.]\n",
      "Expected output:\n",
      "[-2]\n",
      "sse 0.0\tmax sse1e-12\n",
      "Is below error? True\n",
      "Prediction result:\n",
      "[1.]\n",
      "Prediction reference result:\n",
      "[1.]\n",
      "Expected output:\n",
      "[1]\n",
      "sse 0.0\tmax sse1e-12\n",
      "Is below error? True\n",
      "Prediction result:\n",
      "[4.]\n",
      "Prediction reference result:\n",
      "[4.]\n",
      "Expected output:\n",
      "[4]\n",
      "sse 0.0\tmax sse1e-12\n",
      "Is below error? True\n",
      "Prediction result:\n",
      "[7.]\n",
      "Prediction reference result:\n",
      "[7.]\n",
      "Expected output:\n",
      "[7]\n",
      "sse 0.0\tmax sse1e-12\n",
      "Is below error? True\n",
      "Prediction result:\n",
      "[10.]\n",
      "Prediction reference result:\n",
      "[10.]\n",
      "Expected output:\n",
      "[10]\n",
      "sse 0.0\tmax sse1e-12\n",
      "Is below error? True\n",
      "Prediction result:\n",
      "[13.]\n",
      "Prediction reference result:\n",
      "[13.]\n",
      "Expected output:\n",
      "[13]\n",
      "sse 0.0\tmax sse1e-12\n",
      "Is below error? True\n",
      "Prediction result:\n",
      "[16.]\n",
      "Prediction reference result:\n",
      "[16.]\n",
      "Expected output:\n",
      "[16]\n",
      "sse 0.0\tmax sse1e-12\n",
      "Is below error? True\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"jFHvGAi6i5TXMaj2Ry9oEX",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "# Multilayer Test Case\n",
    "Test Case menggunakan multilayer.json dari test case bagian A Asisten."
   ],
   "attachments":{},
   "metadata":{
    "datalore":{
     "node_id":"74cb0NWIS3K8gnYoDrqkcq",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "case = \"multilayer.json\"\n",
    "\n",
    "print(f\"Testing {case}\\n\\n\")\n",
    "test_case = TestCase.from_file(case)\n",
    "model = Model.from_test_case(test_case)\n",
    "model.visualize_print()\n",
    "\n",
    "reference_model = ReferenceModel.from_test_case(test_case)\n",
    "\n",
    "reference_prediction = reference_model.model.predict(test_case.input_data)\n",
    "prediction = model.predict_batch(test_case.input_data)\n",
    "\n",
    "evaluate_result(prediction, reference_prediction, test_case.expected_output, test_case.max_sse)"
   ],
   "execution_count":366,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "Testing multilayer.json\n",
      "\n",
      "\n",
      "\/* Hidden Layer-1 (relu) *\/\n",
      "\/* Neuron-1 *\/\n",
      "Input Layer Neuron-1 -> Hidden Layer-1 Neuron-1 (-0.5)\n",
      "Input Layer Neuron-2 -> Hidden Layer-1 Neuron-1 (0.9)\n",
      "Input Layer Neuron-3 -> Hidden Layer-1 Neuron-1 (1.3)\n",
      "\n",
      "\n",
      "\n",
      "\/* Neuron-2 *\/\n",
      "Input Layer Neuron-1 -> Hidden Layer-1 Neuron-2 (0.6)\n",
      "Input Layer Neuron-2 -> Hidden Layer-1 Neuron-2 (1.0)\n",
      "Input Layer Neuron-3 -> Hidden Layer-1 Neuron-2 (1.4)\n",
      "\n",
      "\n",
      "\n",
      "\/* Neuron-3 *\/\n",
      "Input Layer Neuron-1 -> Hidden Layer-1 Neuron-3 (0.7)\n",
      "Input Layer Neuron-2 -> Hidden Layer-1 Neuron-3 (-1.1)\n",
      "Input Layer Neuron-3 -> Hidden Layer-1 Neuron-3 (1.5)\n",
      "\n",
      "\n",
      "\n",
      "\/* Neuron-4 *\/\n",
      "Input Layer Neuron-1 -> Hidden Layer-1 Neuron-4 (0.5)\n",
      "Input Layer Neuron-2 -> Hidden Layer-1 Neuron-4 (-1.0)\n",
      "Input Layer Neuron-3 -> Hidden Layer-1 Neuron-4 (0.1)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\/* Hidden Layer-2 (relu) *\/\n",
      "\/* Neuron-1 *\/\n",
      "Hidden Layer-1 Neuron-1 -> Hidden Layer-2 Neuron-1 (-0.4)\n",
      "Hidden Layer-1 Neuron-2 -> Hidden Layer-2 Neuron-1 (0.7)\n",
      "Hidden Layer-1 Neuron-3 -> Hidden Layer-2 Neuron-1 (0.2)\n",
      "Hidden Layer-1 Neuron-4 -> Hidden Layer-2 Neuron-1 (-0.1)\n",
      "\n",
      "\n",
      "\n",
      "\/* Neuron-2 *\/\n",
      "Hidden Layer-1 Neuron-1 -> Hidden Layer-2 Neuron-2 (0.5)\n",
      "Hidden Layer-1 Neuron-2 -> Hidden Layer-2 Neuron-2 (0.4)\n",
      "Hidden Layer-1 Neuron-3 -> Hidden Layer-2 Neuron-2 (0.3)\n",
      "Hidden Layer-1 Neuron-4 -> Hidden Layer-2 Neuron-2 (0.2)\n",
      "\n",
      "\n",
      "\n",
      "\/* Neuron-3 *\/\n",
      "Hidden Layer-1 Neuron-1 -> Hidden Layer-2 Neuron-3 (0.6)\n",
      "Hidden Layer-1 Neuron-2 -> Hidden Layer-2 Neuron-3 (-0.9)\n",
      "Hidden Layer-1 Neuron-3 -> Hidden Layer-2 Neuron-3 (0.4)\n",
      "Hidden Layer-1 Neuron-4 -> Hidden Layer-2 Neuron-3 (0.1)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\/* Hidden Layer-3 (relu) *\/\n",
      "\/* Neuron-1 *\/\n",
      "Hidden Layer-2 Neuron-1 -> Hidden Layer-3 Neuron-1 (-0.3)\n",
      "Hidden Layer-2 Neuron-2 -> Hidden Layer-3 Neuron-1 (0.6)\n",
      "Hidden Layer-2 Neuron-3 -> Hidden Layer-3 Neuron-1 (0.1)\n",
      "\n",
      "\n",
      "\n",
      "\/* Neuron-2 *\/\n",
      "Hidden Layer-2 Neuron-1 -> Hidden Layer-3 Neuron-2 (0.4)\n",
      "Hidden Layer-2 Neuron-2 -> Hidden Layer-3 Neuron-2 (0.1)\n",
      "Hidden Layer-2 Neuron-3 -> Hidden Layer-3 Neuron-2 (-0.4)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\/* Hidden Layer-4 (sigmoid) *\/\n",
      "\/* Neuron-1 *\/\n",
      "Hidden Layer-3 Neuron-1 -> Hidden Layer-4 Neuron-1 (-0.2)\n",
      "Hidden Layer-3 Neuron-2 -> Hidden Layer-4 Neuron-1 (0.3)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\r1\/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1\/1 [==============================] - 1s 639ms\/step\n",
      "Prediction result:\n",
      "[0.4846748]\n",
      "Prediction reference result:\n",
      "[0.4846748]\n",
      "Expected output:\n",
      "[0.4846748]\n",
      "sse 3.1555534707211278e-18\tmax sse1e-06\n",
      "Is below error? True\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"UIun0SIoJgqX4kZKElVWJX",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "# Multilayer_Softmax Test Case\n",
    "Test Case menggunakan multilayer.json dari test case bagian A Asisten."
   ],
   "attachments":{},
   "metadata":{
    "datalore":{
     "node_id":"BfYyB20VBrZq8ex74Gt3ke",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "case = \"multilayer_softmax.json\"\n",
    "\n",
    "print(f\"Testing {case}\\n\\n\")\n",
    "test_case = TestCase.from_file(case)\n",
    "model = Model.from_test_case(test_case)\n",
    "model.visualize_print()\n",
    "\n",
    "reference_model = ReferenceModel.from_test_case(test_case)\n",
    "\n",
    "reference_prediction = reference_model.model.predict(test_case.input_data)\n",
    "prediction = model.predict_batch(test_case.input_data)\n",
    "\n",
    "evaluate_result(prediction, reference_prediction, test_case.expected_output, test_case.max_sse)"
   ],
   "execution_count":367,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "Testing multilayer_softmax.json\n",
      "\n",
      "\n",
      "\/* Hidden Layer-1 (relu) *\/\n",
      "\/* Neuron-1 *\/\n",
      "Input Layer Neuron-1 -> Hidden Layer-1 Neuron-1 (0.8)\n",
      "Input Layer Neuron-2 -> Hidden Layer-1 Neuron-1 (0.3)\n",
      "Input Layer Neuron-3 -> Hidden Layer-1 Neuron-1 (1.1)\n",
      "Input Layer Neuron-4 -> Hidden Layer-1 Neuron-1 (0.5)\n",
      "\n",
      "\n",
      "\n",
      "\/* Neuron-2 *\/\n",
      "Input Layer Neuron-1 -> Hidden Layer-1 Neuron-2 (-0.7)\n",
      "Input Layer Neuron-2 -> Hidden Layer-1 Neuron-2 (-1.4)\n",
      "Input Layer Neuron-3 -> Hidden Layer-1 Neuron-2 (-1.3)\n",
      "Input Layer Neuron-4 -> Hidden Layer-1 Neuron-2 (-0.8)\n",
      "\n",
      "\n",
      "\n",
      "\/* Neuron-3 *\/\n",
      "Input Layer Neuron-1 -> Hidden Layer-1 Neuron-3 (1.1)\n",
      "Input Layer Neuron-2 -> Hidden Layer-1 Neuron-3 (0.7)\n",
      "Input Layer Neuron-3 -> Hidden Layer-1 Neuron-3 (0.9)\n",
      "Input Layer Neuron-4 -> Hidden Layer-1 Neuron-3 (1.4)\n",
      "\n",
      "\n",
      "\n",
      "\/* Neuron-4 *\/\n",
      "Input Layer Neuron-1 -> Hidden Layer-1 Neuron-4 (-1.2)\n",
      "Input Layer Neuron-2 -> Hidden Layer-1 Neuron-4 (1.2)\n",
      "Input Layer Neuron-3 -> Hidden Layer-1 Neuron-4 (0.4)\n",
      "Input Layer Neuron-4 -> Hidden Layer-1 Neuron-4 (-0.9)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\/* Hidden Layer-2 (relu) *\/\n",
      "\/* Neuron-1 *\/\n",
      "Hidden Layer-1 Neuron-1 -> Hidden Layer-2 Neuron-1 (1.3)\n",
      "Hidden Layer-1 Neuron-2 -> Hidden Layer-2 Neuron-1 (-1.2)\n",
      "Hidden Layer-1 Neuron-3 -> Hidden Layer-2 Neuron-1 (0.6)\n",
      "Hidden Layer-1 Neuron-4 -> Hidden Layer-2 Neuron-1 (1.0)\n",
      "\n",
      "\n",
      "\n",
      "\/* Neuron-2 *\/\n",
      "Hidden Layer-1 Neuron-1 -> Hidden Layer-2 Neuron-2 (-0.6)\n",
      "Hidden Layer-1 Neuron-2 -> Hidden Layer-2 Neuron-2 (0.9)\n",
      "Hidden Layer-1 Neuron-3 -> Hidden Layer-2 Neuron-2 (-0.5)\n",
      "Hidden Layer-1 Neuron-4 -> Hidden Layer-2 Neuron-2 (-0.4)\n",
      "\n",
      "\n",
      "\n",
      "\/* Neuron-3 *\/\n",
      "Hidden Layer-1 Neuron-1 -> Hidden Layer-2 Neuron-3 (0.5)\n",
      "Hidden Layer-1 Neuron-2 -> Hidden Layer-2 Neuron-3 (1.4)\n",
      "Hidden Layer-1 Neuron-3 -> Hidden Layer-2 Neuron-3 (1.2)\n",
      "Hidden Layer-1 Neuron-4 -> Hidden Layer-2 Neuron-3 (0.8)\n",
      "\n",
      "\n",
      "\n",
      "\/* Neuron-4 *\/\n",
      "Hidden Layer-1 Neuron-1 -> Hidden Layer-2 Neuron-4 (-1.3)\n",
      "Hidden Layer-1 Neuron-2 -> Hidden Layer-2 Neuron-4 (-0.7)\n",
      "Hidden Layer-1 Neuron-3 -> Hidden Layer-2 Neuron-4 (-1.1)\n",
      "Hidden Layer-1 Neuron-4 -> Hidden Layer-2 Neuron-4 (-1.0)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\/* Hidden Layer-3 (relu) *\/\n",
      "\/* Neuron-1 *\/\n",
      "Hidden Layer-2 Neuron-1 -> Hidden Layer-3 Neuron-1 (0.2)\n",
      "Hidden Layer-2 Neuron-2 -> Hidden Layer-3 Neuron-1 (1.4)\n",
      "Hidden Layer-2 Neuron-3 -> Hidden Layer-3 Neuron-1 (-0.7)\n",
      "Hidden Layer-2 Neuron-4 -> Hidden Layer-3 Neuron-1 (0.9)\n",
      "\n",
      "\n",
      "\n",
      "\/* Neuron-2 *\/\n",
      "Hidden Layer-2 Neuron-1 -> Hidden Layer-3 Neuron-2 (-1.0)\n",
      "Hidden Layer-2 Neuron-2 -> Hidden Layer-3 Neuron-2 (-0.9)\n",
      "Hidden Layer-2 Neuron-3 -> Hidden Layer-3 Neuron-2 (1.2)\n",
      "Hidden Layer-2 Neuron-4 -> Hidden Layer-3 Neuron-2 (-0.7)\n",
      "\n",
      "\n",
      "\n",
      "\/* Neuron-3 *\/\n",
      "Hidden Layer-2 Neuron-1 -> Hidden Layer-3 Neuron-3 (1.1)\n",
      "Hidden Layer-2 Neuron-2 -> Hidden Layer-3 Neuron-3 (0.3)\n",
      "Hidden Layer-2 Neuron-3 -> Hidden Layer-3 Neuron-3 (-1.1)\n",
      "Hidden Layer-2 Neuron-4 -> Hidden Layer-3 Neuron-3 (1.3)\n",
      "\n",
      "\n",
      "\n",
      "\/* Neuron-4 *\/\n",
      "Hidden Layer-2 Neuron-1 -> Hidden Layer-3 Neuron-4 (-0.6)\n",
      "Hidden Layer-2 Neuron-2 -> Hidden Layer-3 Neuron-4 (-1.4)\n",
      "Hidden Layer-2 Neuron-3 -> Hidden Layer-3 Neuron-4 (0.5)\n",
      "Hidden Layer-2 Neuron-4 -> Hidden Layer-3 Neuron-4 (-0.8)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\/* Hidden Layer-4 (softmax) *\/\n",
      "\/* Neuron-1 *\/\n",
      "Hidden Layer-3 Neuron-1 -> Hidden Layer-4 Neuron-1 (-1.4)\n",
      "Hidden Layer-3 Neuron-2 -> Hidden Layer-4 Neuron-1 (0.8)\n",
      "Hidden Layer-3 Neuron-3 -> Hidden Layer-4 Neuron-1 (0.1)\n",
      "Hidden Layer-3 Neuron-4 -> Hidden Layer-4 Neuron-1 (1.2)\n",
      "\n",
      "\n",
      "\n",
      "\/* Neuron-2 *\/\n",
      "Hidden Layer-3 Neuron-1 -> Hidden Layer-4 Neuron-2 (0.3)\n",
      "Hidden Layer-3 Neuron-2 -> Hidden Layer-4 Neuron-2 (1.2)\n",
      "Hidden Layer-3 Neuron-3 -> Hidden Layer-4 Neuron-2 (-1.2)\n",
      "Hidden Layer-3 Neuron-4 -> Hidden Layer-4 Neuron-2 (1.4)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\r1\/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1\/1 [==============================] - 0s 342ms\/step\n",
      "Prediction result:\n",
      "[0.7042294 0.2957706]\n",
      "Prediction reference result:\n",
      "[0.7042295 0.2957706]\n",
      "Expected output:\n",
      "[0.7042294, 0.2957706]\n",
      "sse 1.942281739821482e-19\tmax sse1e-06\n",
      "Is below error? True\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"KGnyCiIYOxs0M0IV4f4GXz",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "# Relu Test Case\n",
    "Test Case menggunakan relu.json dari test case bagian A Asisten."
   ],
   "attachments":{},
   "metadata":{
    "datalore":{
     "node_id":"egRq3kfXs4xMsvmye3K2Pr",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "case = \"relu.json\"\n",
    "\n",
    "print(f\"Testing {case}\\n\\n\")\n",
    "test_case = TestCase.from_file(case)\n",
    "model = Model.from_test_case(test_case)\n",
    "model.visualize_print()\n",
    "\n",
    "reference_model = ReferenceModel.from_test_case(test_case)\n",
    "\n",
    "reference_prediction = reference_model.model.predict(test_case.input_data)\n",
    "prediction = model.predict_batch(test_case.input_data)\n",
    "\n",
    "evaluate_result(prediction, reference_prediction, test_case.expected_output, test_case.max_sse)"
   ],
   "execution_count":368,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "Testing relu.json\n",
      "\n",
      "\n",
      "\/* Hidden Layer-1 (relu) *\/\n",
      "\/* Neuron-1 *\/\n",
      "Input Layer Neuron-1 -> Hidden Layer-1 Neuron-1 (0.47)\n",
      "Input Layer Neuron-2 -> Hidden Layer-1 Neuron-1 (1.1)\n",
      "\n",
      "\n",
      "\n",
      "\/* Neuron-2 *\/\n",
      "Input Layer Neuron-1 -> Hidden Layer-1 Neuron-2 (-0.6)\n",
      "Input Layer Neuron-2 -> Hidden Layer-1 Neuron-2 (-1.3)\n",
      "\n",
      "\n",
      "\n",
      "\/* Neuron-3 *\/\n",
      "Input Layer Neuron-1 -> Hidden Layer-1 Neuron-3 (0.2)\n",
      "Input Layer Neuron-2 -> Hidden Layer-1 Neuron-3 (0.5)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\r1\/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1\/1 [==============================] - 0s 339ms\/step\n",
      "Prediction result:\n",
      "[0.31  0.    0.375]\n",
      "Prediction reference result:\n",
      "[0.30999997 0.         0.37500003]\n",
      "Expected output:\n",
      "[0.31, 0, 0.375]\n",
      "sse 3.0814879110195774e-33\tmax sse1e-06\n",
      "Is below error? True\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"jnTcq8dabuzoYl1U0ZbZ9B",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "# Sigmoid Test Case\n",
    "Test Case menggunakan sigmoid.json dari test case bagian A Asisten."
   ],
   "attachments":{},
   "metadata":{
    "datalore":{
     "node_id":"8KiZ6ErKye8RczHNBhUtB6",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "case = \"sigmoid.json\"\n",
    "\n",
    "print(f\"Testing {case}\\n\\n\")\n",
    "test_case = TestCase.from_file(case)\n",
    "model = Model.from_test_case(test_case)\n",
    "model.visualize_print()\n",
    "\n",
    "reference_model = ReferenceModel.from_test_case(test_case)\n",
    "\n",
    "reference_prediction = reference_model.model.predict(test_case.input_data)\n",
    "prediction = model.predict_batch(test_case.input_data)\n",
    "\n",
    "evaluate_result(prediction, reference_prediction, test_case.expected_output, test_case.max_sse)"
   ],
   "execution_count":369,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "Testing sigmoid.json\n",
      "\n",
      "\n",
      "\/* Hidden Layer-1 (sigmoid) *\/\n",
      "\/* Neuron-1 *\/\n",
      "Input Layer Neuron-1 -> Hidden Layer-1 Neuron-1 (-1.2)\n",
      "Input Layer Neuron-2 -> Hidden Layer-1 Neuron-1 (1.4)\n",
      "Input Layer Neuron-3 -> Hidden Layer-1 Neuron-1 (-0.7)\n",
      "\n",
      "\n",
      "\n",
      "\/* Neuron-2 *\/\n",
      "Input Layer Neuron-1 -> Hidden Layer-1 Neuron-2 (-1.7)\n",
      "Input Layer Neuron-2 -> Hidden Layer-1 Neuron-2 (-1.6)\n",
      "Input Layer Neuron-3 -> Hidden Layer-1 Neuron-2 (1.1)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\/* Hidden Layer-2 (sigmoid) *\/\n",
      "\/* Neuron-1 *\/\n",
      "Hidden Layer-1 Neuron-2 -> Hidden Layer-2 Neuron-1 (2.1)\n",
      "\n",
      "\n",
      "\n",
      "\/* Neuron-2 *\/\n",
      "Hidden Layer-1 Neuron-2 -> Hidden Layer-2 Neuron-2 (-0.2)\n",
      "\n",
      "\n",
      "\n",
      "\/* Neuron-3 *\/\n",
      "Hidden Layer-1 Neuron-1 -> Hidden Layer-2 Neuron-3 (-1.5)\n",
      "\n",
      "\n",
      "\n",
      "\/* Neuron-4 *\/\n",
      "Hidden Layer-1 Neuron-1 -> Hidden Layer-2 Neuron-4 (0.7)\n",
      "Hidden Layer-1 Neuron-2 -> Hidden Layer-2 Neuron-4 (1.8)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\r1\/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1\/1 [==============================] - 0s 460ms\/step\n",
      "Prediction result:\n",
      "[0.41197346 0.8314294  0.53018536 0.31607396]\n",
      "Prediction reference result:\n",
      "[0.41197345 0.8314294  0.53018534 0.31607395]\n",
      "Expected output:\n",
      "[0.41197346, 0.8314294, 0.53018536, 0.31607396]\n",
      "sse 5.448747611215099e-17\tmax sse1e-06\n",
      "Is below error? True\n",
      "Prediction result:\n",
      "[0.78266141 0.80843631 0.55350518 0.64278501]\n",
      "Prediction reference result:\n",
      "[0.78266144 0.8084363  0.5535052  0.642785  ]\n",
      "Expected output:\n",
      "[0.78266141, 0.80843631, 0.55350518, 0.64278501]\n",
      "sse 1.8886184977662707e-17\tmax sse1e-06\n",
      "Is below error? True\n",
      "Prediction result:\n",
      "[0.58987524 0.82160954 0.75436518 0.34919895]\n",
      "Prediction reference result:\n",
      "[0.5898752  0.82160956 0.7543652  0.3491989 ]\n",
      "Expected output:\n",
      "[0.58987524, 0.82160954, 0.75436518, 0.34919895]\n",
      "sse 3.588137593505069e-17\tmax sse1e-06\n",
      "Is below error? True\n",
      "Prediction result:\n",
      "[0.6722004  0.81660439 0.59020258 0.50870988]\n",
      "Prediction reference result:\n",
      "[0.6722004  0.8166044  0.59020257 0.50870985]\n",
      "Expected output:\n",
      "[0.6722004, 0.81660439, 0.59020258, 0.50870988]\n",
      "sse 5.4935267400256686e-17\tmax sse1e-06\n",
      "Is below error? True\n",
      "Prediction result:\n",
      "[0.47322841 0.82808466 0.69105452 0.29358323]\n",
      "Prediction reference result:\n",
      "[0.4732284  0.82808465 0.6910545  0.29358324]\n",
      "Expected output:\n",
      "[0.47322841, 0.82808466, 0.69105452, 0.29358323]\n",
      "sse 5.3370328317207175e-17\tmax sse1e-06\n",
      "Is below error? True\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"6zuwt4jsCIgaLQVFPAfxsN",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "# Softmax Test Case\n",
    "Test Case menggunakan softmax.json dari test case bagian A Asisten."
   ],
   "attachments":{},
   "metadata":{
    "datalore":{
     "node_id":"vhEbKCdOdTufzJTgy968sD",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "case = \"softmax.json\"\n",
    "\n",
    "print(f\"Testing {case}\\n\\n\")\n",
    "test_case = TestCase.from_file(case)\n",
    "model = Model.from_test_case(test_case)\n",
    "model.visualize_print()\n",
    "\n",
    "reference_model = ReferenceModel.from_test_case(test_case)\n",
    "\n",
    "reference_prediction = reference_model.model.predict(test_case.input_data)\n",
    "prediction = model.predict_batch(test_case.input_data)\n",
    "\n",
    "evaluate_result(prediction, reference_prediction, test_case.expected_output, test_case.max_sse)"
   ],
   "execution_count":370,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "Testing softmax.json\n",
      "\n",
      "\n",
      "\/* Hidden Layer-1 (softmax) *\/\n",
      "\/* Neuron-1 *\/\n",
      "Input Layer Neuron-1 -> Hidden Layer-1 Neuron-1 (-0.2)\n",
      "Input Layer Neuron-2 -> Hidden Layer-1 Neuron-1 (0.3)\n",
      "Input Layer Neuron-3 -> Hidden Layer-1 Neuron-1 (0.4)\n",
      "Input Layer Neuron-4 -> Hidden Layer-1 Neuron-1 (0.5)\n",
      "Input Layer Neuron-5 -> Hidden Layer-1 Neuron-1 (-0.6)\n",
      "Input Layer Neuron-6 -> Hidden Layer-1 Neuron-1 (-0.7)\n",
      "Input Layer Neuron-7 -> Hidden Layer-1 Neuron-1 (0.8)\n",
      "Input Layer Neuron-8 -> Hidden Layer-1 Neuron-1 (0.9)\n",
      "\n",
      "\n",
      "\n",
      "\/* Neuron-2 *\/\n",
      "Input Layer Neuron-1 -> Hidden Layer-1 Neuron-2 (0.8)\n",
      "Input Layer Neuron-2 -> Hidden Layer-1 Neuron-2 (-0.7)\n",
      "Input Layer Neuron-3 -> Hidden Layer-1 Neuron-2 (0.6)\n",
      "Input Layer Neuron-4 -> Hidden Layer-1 Neuron-2 (0.5)\n",
      "Input Layer Neuron-5 -> Hidden Layer-1 Neuron-2 (0.4)\n",
      "Input Layer Neuron-6 -> Hidden Layer-1 Neuron-2 (-0.3)\n",
      "Input Layer Neuron-7 -> Hidden Layer-1 Neuron-2 (0.2)\n",
      "Input Layer Neuron-8 -> Hidden Layer-1 Neuron-2 (-0.1)\n",
      "\n",
      "\n",
      "\n",
      "\/* Neuron-3 *\/\n",
      "Input Layer Neuron-1 -> Hidden Layer-1 Neuron-3 (0.2)\n",
      "Input Layer Neuron-2 -> Hidden Layer-1 Neuron-3 (0.3)\n",
      "Input Layer Neuron-3 -> Hidden Layer-1 Neuron-3 (-0.4)\n",
      "Input Layer Neuron-4 -> Hidden Layer-1 Neuron-3 (0.5)\n",
      "Input Layer Neuron-5 -> Hidden Layer-1 Neuron-3 (0.6)\n",
      "Input Layer Neuron-6 -> Hidden Layer-1 Neuron-3 (0.7)\n",
      "Input Layer Neuron-7 -> Hidden Layer-1 Neuron-3 (-0.8)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\r1\/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1\/1 [==============================] - 0s 340ms\/step\n",
      "Prediction result:\n",
      "[0.76439061 0.21168068 0.02392871]\n",
      "Prediction reference result:\n",
      "[0.76439065 0.21168068 0.02392871]\n",
      "Expected output:\n",
      "[0.76439061, 0.21168068, 0.02392871]\n",
      "sse 1.2639167733980301e-17\tmax sse1e-05\n",
      "Is below error? True\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"o32wIBPo9a4L6IHF9gZAkY",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "# Tugas Besar Bagian B"
   ],
   "attachments":{},
   "metadata":{
    "datalore":{
     "node_id":"Tugas Besar Bagian B",
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "sheet_delimiter":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "# Tugas Besar Pembelajaran Mesin Bagian B\n",
    "Kelompok 14\n",
    "\n",
    "**Tentang Program**\n",
    "\n",
    "Program dalam bahasa Python untuk membuat neural network dengan algoritma Backpropagation. \n",
    "\n",
    "\n",
    "**Pembagian Tugas**\n",
    "| NIM     | Pembagian Tugas                            |\n",
    "|---------|--------------------------------------------|\n",
    "| 13521060 | Pembuatan Model                           |\n",
    "| 13521089 | Pembuatan Model, Penjelasan Algoritma     |\n",
    "| 13521093 | Pembuatan Model dan Algoritma |\n",
    "| 13521171 | Laporan dan Testing                    |"
   ],
   "attachments":{},
   "metadata":{
    "datalore":{
     "node_id":"Lw5yE8mb7hT1nA28PLBTn1",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "## Setup Program\n",
    "Bagian ini mengimpor library, dan setup konfigurasi tipe program. \n",
    "\n",
    "***DEBUG_MODE*** : Mode\n",
    "\n",
    "***TESTCASE_MODE***: Mode menjalankan testcase backprogagation\n",
    "\n",
    "***TESTCASE_REFERENCE_MODE***: Mode menjalankan testcase dengan library"
   ],
   "attachments":{},
   "metadata":{
    "datalore":{
     "node_id":"YnHfzIhRnBd9TkwfSyu2sg",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "import tensorflow as tf\n",
    "from typing import List\n",
    "from dataclasses import dataclass\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD\n",
    "from keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "import math"
   ],
   "execution_count":371,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"iTR75tKRFAVsTEYyCbJ8nK",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "# Setup\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "DEBUG_MODE: bool = False\n",
    "TESTCASE_MODE: bool = True\n",
    "TESTCASE_REFERENCE_MODE: bool = True\n",
    "\n",
    "def run_test_case_mode(filename):\n",
    "    if TESTCASE_MODE:\n",
    "        run_test_case(filename)\n",
    "\n",
    "def run_test_case_reference_mode(filename):\n",
    "    if TESTCASE_REFERENCE_MODE:\n",
    "        run_test_on_reference(filename)\n",
    "\n",
    "def print_debug(value: str):\n",
    "    if DEBUG_MODE:\n",
    "        print(value)"
   ],
   "execution_count":372,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"l2X9gb12iRlauVScMQMoIb",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "## Metode Fungsi Aktivasi\n",
    "Berikut merupakan setup untuk metode fungsi aktivasi dan turunannya."
   ],
   "attachments":{},
   "metadata":{
    "datalore":{
     "node_id":"chvW0ZCnSCDoneO1mtWlGk",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "class Methods:\n",
    "    def __init__(self):\n",
    "        self.activation_functions = {\n",
    "            \"linear\": self.activation_linear,\n",
    "            \"relu\": self.activation_relu,\n",
    "            \"sigmoid\": self.activation_sigmoid,\n",
    "            \"softmax\": self.activation_softmax\n",
    "        }\n",
    "\n",
    "        self.derived_activation_functions = {\n",
    "            \"linear\": self.derivation_linear,\n",
    "            \"relu\": self.derivation_relu,\n",
    "            \"sigmoid\": self.derivation_sigmoid,\n",
    "            \"softmax\": self.derivation_softmax\n",
    "        }\n",
    "        \n",
    "        self.loss_functions = {\n",
    "            \"linear\": self.sse,\n",
    "            \"relu\": self.sse,\n",
    "            \"sigmoid\": self.sse,\n",
    "            \"softmax\": self.softmax_loss\n",
    "        }\n",
    "\n",
    "    def activation_linear(self, x: np.array):\n",
    "        return x\n",
    "\n",
    "    def activation_relu(self, x: np.array):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def activation_sigmoid(self, x: np.array):\n",
    "        return 1\/(1+np.exp(-x, dtype=np.float64))\n",
    "\n",
    "    def activation_softmax(self, x: np.array):\n",
    "        return np.exp(x, dtype=np.float64)\/np.sum(np.exp(x), dtype=np.float64)\n",
    "\n",
    "    def derivation_linear(self, x: np.array):\n",
    "        return np.ones(len(x), dtype=np.float64)\n",
    "\n",
    "    def derivation_relu(self, x: np.array):\n",
    "        return 1. * (x > 0)\n",
    "    \n",
    "    def derivation_sigmoid(self, x: np.array):\n",
    "        sigm = self.activation_sigmoid(x)\n",
    "        return sigm*(1-sigm)\n",
    "\n",
    "    def derivation_softmax(self, x: np.array, y: np.array):\n",
    "        return self.activation_softmax(x) - y\n",
    "\n",
    "    def sse(self, o: np.array, t: np.array):\n",
    "        return 0.5*np.sum((t-o)**2, dtype=np.float64)\n",
    "\n",
    "    def softmax_loss(self, o: np.array, t: np.array):\n",
    "        return np.sum(-t*np.log(o))\n",
    "\n",
    "    "
   ],
   "execution_count":373,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"McQg1FK8V0ihKrEczvQeJt",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "## Konfigurasi Model\n",
    "Pada cell berikut merupakan konfigurasi model untuk algoritma backpropagation."
   ],
   "attachments":{},
   "metadata":{
    "datalore":{
     "node_id":"pvShaxMS8qL7m1Ey393Gsi",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "### Kelas Neuron\n",
    "Kelas ini mewakili satu neuron dalam neural network. Disini, mencakup properti untuk bobot dan bias neuron serta metode untuk menghitung nilai output (nilai-z), memperbarui bobot, dan mempertahankan pembaruan."
   ],
   "attachments":{},
   "metadata":{
    "datalore":{
     "node_id":"QOp9cz0Lh1ERWtVzXT6aFI",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "class Neuron:\n",
    "    weight: np.array\n",
    "    weight_size: int\n",
    "    d_weight: np.array\n",
    "    d_bias: np.array\n",
    "    z_val: float\n",
    "    bias: float\n",
    "\n",
    "    def __init__(self, weight_size: int):\n",
    "        self.weight_size = weight_size\n",
    "        self.weight = np.random.rand(weight_size)\n",
    "        self.bias = np.random.rand(1)[0]\n",
    "\n",
    "    \"\"\" Return z value\n",
    "\n",
    "    z = bias + wt•x\n",
    "    \"\"\"\n",
    "    def compute_z(self, input_data: np.array) -> float:\n",
    "        self.z_val = self.bias + np.dot(input_data, self.weight)\n",
    "        return self.z_val\n",
    "\n",
    "    def set_weight(self, weight: np.array, bias: float):\n",
    "        if len(weight) != self.weight_size:\n",
    "            raise Exception(f\"Weight size not match expect {self.weight_size} got {len(weight)}\")\n",
    "        \n",
    "        self.weight = weight\n",
    "        self.bias = bias\n",
    "\n",
    "    def init_d_weight(self, batch_size: int):\n",
    "        self.d_weight = np.zeros((batch_size, self.weight_size), dtype=np.float64)\n",
    "        self.d_bias = np.zeros((batch_size, ), dtype=np.float64)\n",
    "\n",
    "    def persist_d_weight(self, learning_rate: float):\n",
    "        self.weight -= learning_rate * self.d_weight.mean(axis=0, dtype=np.float64)\n",
    "        print_debug(f\"\\t\\tb -= n * d_bias = {learning_rate} * {self.d_bias.mean(axis=0, dtype=np.float64)}\")\n",
    "        self.bias -= learning_rate * self.d_bias.mean(dtype=np.float64)\n",
    "        self.d_weight = None\n",
    "        self.d_bias = None\n",
    "        "
   ],
   "execution_count":374,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"zevIuZr80n1Elw8kvqhRJG",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "### Kelas Layer\n",
    "Kelas ini mewakili layer dari neural network yang terdiri dari beberapa neuron. Disini dikelola operasi yang terkait dengan sekelompok neuron, misalnya mengelola turunan untuk learning."
   ],
   "attachments":{},
   "metadata":{
    "datalore":{
     "node_id":"FTXKO45eO7Fu7qw7oCAMOO",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "class Layer:\n",
    "    activation: str\n",
    "    input_size: int\n",
    "    neurons: List[Neuron]\n",
    "    activation_function: str\n",
    "    methods: Methods\n",
    "    n_neurons: int\n",
    "\n",
    "    result_raw: np.array # dimension matched with number of neurons in the layer\n",
    "    result: np.array # dimension matched with number of neurons in the layer\n",
    "    result_derivative: np.array # dimension matched with number of neurons in the layer\n",
    "    do_c_do_a: np.array # dimension matched with number of neurons in the layer\n",
    "    \n",
    "    def __init__(self,\n",
    "                 n_neurons: int,\n",
    "                 activation_function: str,\n",
    "                 input_size: int = None):\n",
    "        self.input_size = input_size\n",
    "        self.activation_function = activation_function\n",
    "        self.n_neurons = n_neurons\n",
    "        self.neurons = []\n",
    "        self.methods = Methods()\n",
    "\n",
    "    def init_neurons(self, n_weights: int = None):\n",
    "        if self.input_size is not None:\n",
    "            self.neurons = [Neuron(self.input_size) for _ in range(self.n_neurons)]\n",
    "        elif n_weights is None:\n",
    "            raise Exception(\"n_weights should not be none for noninput layer\")\n",
    "        else:\n",
    "            self.neurons = [Neuron(n_weights) for _ in range(self.n_neurons)]\n",
    "\n",
    "    def update_weights(self, weights: np.array, bias: np.array):\n",
    "        for i, neuron in enumerate(self.neurons):\n",
    "            neuron.set_weight(weights[i], bias[i])\n",
    "\n",
    "    def update_weights_from_normalized(self, weights: np.array):\n",
    "        self.update_weights(weights[1:].T, weights[0])\n",
    "\n",
    "    def predict(self, input_data: np.array) -> np.array:\n",
    "        self.result_raw = np.array([neuron.compute_z(input_data) for neuron in self.neurons])\n",
    "        #print_debug(f\"z: {self.result_raw}, activation function: {self.activation_function}\")\n",
    "        self.result = self.methods.activation_functions[self.activation_function](self.result_raw)\n",
    "        print_debug(f\"a: {self.result}\")\n",
    "        return self.result\n",
    "    \n",
    "    def predict_derivative(self, target: np.array = None):\n",
    "        if self.activation_function == 'softmax':\n",
    "            self.result_derivative = self.methods.derived_activation_functions[self.activation_function](self.result_raw, target)\n",
    "        else:\n",
    "            self.result_derivative = self.methods.derived_activation_functions[self.activation_function](self.result_raw)\n",
    "\n",
    "        return self.result_derivative\n",
    "    \n",
    "    def persist_d_weight(self, learning_rate: float):\n",
    "        for i, neuron in enumerate(self.neurons):\n",
    "            print_debug(f\"Persisting for neuron-{i}\")\n",
    "            print_debug(f\"\\tBefore: {neuron.bias} {neuron.weight}\")\n",
    "            neuron.persist_d_weight(learning_rate)\n",
    "            print_debug(f\"\\tAfter: {neuron.bias} {neuron.weight}\")\n",
    "\n",
    "    def init_d_weight(self, batch_size: int):\n",
    "        for neuron in self.neurons:\n",
    "            neuron.init_d_weight(batch_size)\n",
    "\n",
    "    def clear_prev_result(self):\n",
    "        self.result_raw = None\n",
    "        self.result = None\n",
    "        self.result_derivative = None\n",
    "        self.do_c_do_a = None\n",
    "\n",
    "    @property\n",
    "    def normalized_weights(self) -> np.array:\n",
    "        return np.array([np.append(np.array([n.bias], dtype=np.float64), n.weight) for n in self.neurons], dtype=np.float64).T"
   ],
   "execution_count":375,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"lH9mUWV0F1dYhBt5oaBQbO",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "### Kelas SavedLayer dan SavedModel\n",
    "Kelas ini merupakan kelas bantuan untuk menyimpan  pembaruan layer dan pembaruan model"
   ],
   "attachments":{},
   "metadata":{
    "datalore":{
     "node_id":"TpSs4vb9YziWwcKF6eUoMs",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "@dataclass\n",
    "class SavedLayer:\n",
    "    activation_function: str\n",
    "    n_neuron: int\n",
    "    # three dimensional array\n",
    "    # first dimension for every neuron\n",
    "    # second dimension consist of two element\n",
    "    # the first one is the bias value\n",
    "    # the second one is the weights value\n",
    "    weights: np.array\n",
    "\n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            \"activation_function\": self.activation_function,\n",
    "            \"n_neuron\": self.n_neuron,\n",
    "            \"weights\": self.weights.tolist()\n",
    "        }\n",
    "\n",
    "@dataclass\n",
    "class SavedModel:\n",
    "    layers: List[SavedLayer]\n",
    "    input_size: int\n",
    "\n",
    "    @staticmethod\n",
    "    def from_file(filename: str):\n",
    "        f = open(filename, \"r\")\n",
    "        json_read = json.loads(f.read())\n",
    "        f.close()\n",
    "\n",
    "        me = SavedModel(layers=[], input_size=json_read[\"input_size\"])\n",
    "\n",
    "        for layer in json_read[\"layers\"]:\n",
    "            me.layers.append(SavedLayer(\n",
    "                activation_function=layer[\"activation_function\"],\n",
    "                n_neuron=layer[\"n_neuron\"],\n",
    "                weights=np.array(layer[\"weights\"])\n",
    "            ))\n",
    "\n",
    "        me.input_size = json_read[\"input_size\"]\n",
    "        \n",
    "        return me\n",
    "    \n",
    "    @staticmethod\n",
    "    def from_model(model: Model):\n",
    "        me = SavedModel(layers=[], input_size=model.layers[0].input_size)\n",
    "\n",
    "        for layer in model.layers:\n",
    "            me.layers.append(SavedLayer(\n",
    "                activation_function=layer.activation_function,\n",
    "                n_neuron=layer.n_neurons,\n",
    "                weights=layer.normalized_weights\n",
    "            ))\n",
    "\n",
    "        return me\n",
    "    \n",
    "    def save_to(self, filename: str):\n",
    "        layers_dict = [each.to_dict() for each in self.layers]\n",
    "\n",
    "        with open(filename, \"w\") as f:\n",
    "            json.dump({\n",
    "                \"layers\": layers_dict,\n",
    "                \"input_size\": self.input_size\n",
    "            }, f)"
   ],
   "execution_count":376,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"KpUpRygMVZGud7tIGIXSz6",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "### Kelas Model\n",
    "Kelas ini mewakili model dari neural network yang terdiri dari beberapa neuron. Disini dikelola operasi yang terkait dengan sekelompok layer yang membentuk model, misalnya mengelola weight pada model, visualisasikan model, dan memprediksi hasil training."
   ],
   "attachments":{},
   "metadata":{
    "datalore":{
     "node_id":"RtZLjMLrXArDcgpyV4gdxe",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "class Model:\n",
    "    layers: List[Layer]\n",
    "    training_stop_reason: str\n",
    "\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def from_saved_model(saved_model: SavedModel):\n",
    "        model = Model()\n",
    "\n",
    "        for i, layer in enumerate(saved_model.layers):\n",
    "            if i == 0:\n",
    "                model.add(\n",
    "                    Layer(\n",
    "                        n_neurons=layer.n_neuron,\n",
    "                        activation_function=layer.activation_function,\n",
    "                        input_size=saved_model.input_size\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                model.add(\n",
    "                    Layer(\n",
    "                        n_neurons=layer.n_neuron,\n",
    "                        activation_function=layer.activation_function,\n",
    "                    )\n",
    "                )\n",
    "        \n",
    "        model.compile()\n",
    "\n",
    "        for i, layer in enumerate(model.layers):\n",
    "            layer.update_weights_from_normalized(saved_model.layers[i].weights)\n",
    "\n",
    "        return model\n",
    "\n",
    "    def add(self, layer: Layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    \"\"\" Initialize empty weights\n",
    "    \"\"\"\n",
    "    def compile(self):\n",
    "        n_weights: int = None\n",
    "        n_layer = len(self.layers)\n",
    "\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if i == 0:\n",
    "                layer.init_neurons()\n",
    "            else:\n",
    "                layer.init_neurons(n_weights)\n",
    "\n",
    "            if i != n_layer - 1 and layer.activation_function == 'softmax':\n",
    "                raise Exception(\"Cannot use softmax activation function in hidden layer\")\n",
    "            \n",
    "            n_weights = layer.n_neurons\n",
    "    \n",
    "    def update_weights(self, weights: List[np.array], bias: List[np.array]):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            layer.update_weights(weights[i], bias[i])\n",
    "\n",
    "    def clear_prev_result(self):\n",
    "        for layer in self.layers:\n",
    "            layer.clear_prev_result()\n",
    "\n",
    "    def fit(self,\n",
    "            x: np.array,\n",
    "            y: np.array,\n",
    "            learning_rate: float = 0.1,\n",
    "            epochs: int = 5,\n",
    "            batch_size: int = 5,\n",
    "            error_threshold: float = 0\n",
    "            ):\n",
    "        \n",
    "        current_error = np.inf\n",
    "        epoch = 0\n",
    "\n",
    "        print(\"Begin training model\\n\\n\")\n",
    "\n",
    "        while current_error > error_threshold and epoch < epochs:\n",
    "            print(f\"Epoch {epoch}\")\n",
    "\n",
    "            error = 0\n",
    "\n",
    "            for i, x_batch, y_batch in self._generate_mini_batches(x, y, batch_size):\n",
    "                print_debug(f\"\\n----------------------\\nTraining for batch-{i} with size {len(x_batch)}\")\n",
    "                error += self._backpropagate(x_batch, y_batch, learning_rate)\n",
    "    \n",
    "            current_error = error \/ math.ceil(len(x)\/batch_size)\n",
    "                \n",
    "            print(f\"training loss {current_error}\")\n",
    "            epoch += 1\n",
    "            print(\"\\n\")\n",
    "\n",
    "        if current_error <= error_threshold:\n",
    "            self.training_stop_reason = \"error_threshold\"\n",
    "        elif epoch >= epochs:\n",
    "            self.training_stop_reason = \"max_iteration\"\n",
    "            \n",
    "\n",
    "    def _init_d_weight(self, batch_size: int):\n",
    "        for layer in self.layers:\n",
    "            layer.init_d_weight(batch_size)\n",
    "\n",
    "    def _persist_d_weight(self, learning_rate: float):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            print_debug(f\"Persisting d_weight for layer-{i}\")\n",
    "            layer.persist_d_weight(learning_rate)\n",
    "\n",
    "    def _predict(self, input_data: np.array, with_derivative: bool = False, target: np.array = None) -> np.array:\n",
    "        layer: Layer = self.layers[0]\n",
    "        temp_array = layer.predict(input_data)\n",
    "        if with_derivative:\n",
    "            layer.predict_derivative(target)\n",
    "            \n",
    "        for i in range(1, len(self.layers)):\n",
    "            layer: Layer = self.layers[i]\n",
    "            temp_array = layer.predict(temp_array)\n",
    "            if with_derivative:\n",
    "                layer.predict_derivative(target)\n",
    "            print_debug(f\"prediction layer {i}: {temp_array}\")\n",
    "        \n",
    "        return temp_array\n",
    "    \n",
    "    def predict_batch(self, input_data: List[np.array]) -> List[np.array]:\n",
    "        # Batch output\n",
    "        final_output = []\n",
    "        for data in input_data:\n",
    "            final_output.append(self._predict(data))\n",
    "        return final_output\n",
    "    \n",
    "    def visualize(self):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            print(f\"\/* Layer-{i + 1} ({layer.activation_function}) *\/\")\n",
    "            for j, neuron in enumerate(layer.neurons):\n",
    "                print(f\"\/* Neuron-{j + 1} *\/\")\n",
    "                if i == 0:\n",
    "                    source_layer = \"Input Layer\"\n",
    "                else:\n",
    "                    source_layer = f\"Hidden Layer-{i}\"\n",
    "                for k, w in enumerate(neuron.weight):\n",
    "                    if k == 0:\n",
    "                        continue\n",
    "                    if w != 0:\n",
    "                        print(f\"{source_layer} Neuron-{k} -> Hidden Layer-{i + 1} Neuron-{j+1} ({w})\")\n",
    "                print(\"\\n\\n\")\n",
    "            print(\"\\n\\n\")\n",
    "\n",
    "    def _generate_mini_batches(self, x: np.array, y: np.array, batch_size: int):\n",
    "        start_index = 0\n",
    "        batch_number = 0\n",
    "        input_size = len(x)\n",
    "\n",
    "        while start_index < input_size:\n",
    "            end_index = min(start_index + batch_size, input_size)\n",
    "            \n",
    "            yield batch_number, x[start_index:end_index], y[start_index:end_index]\n",
    "\n",
    "            start_index += batch_size\n",
    "            batch_number += 1\n",
    "\n",
    "    def _backpropagate(self, \n",
    "                       x: np.array, \n",
    "                       y: np.array, \n",
    "                       learning_rate: float):\n",
    "        \n",
    "        self._init_d_weight(len(x))\n",
    "\n",
    "        for nth_element, element in enumerate(x):\n",
    "            print_debug(f\"----------------------\\nBackpropagating for element {nth_element} in batch\")\n",
    "\n",
    "            if self.layers[-1].activation_function == 'softmax':\n",
    "                self._predict(x[nth_element], with_derivative=True, target=y[nth_element])\n",
    "            else:\n",
    "                self._predict(x[nth_element], with_derivative=True)\n",
    "\n",
    "            for i in range(len(self.layers)-1, -1, -1):\n",
    "                print_debug(f\"layer current: {i}\")\n",
    "                layer_current = self.layers[i]\n",
    "                layer_previous = None if i == 0 else self.layers[i - 1]\n",
    "                layer_next = None if i == len(self.layers) - 1 else self.layers[i + 1]\n",
    "\n",
    "                a_val_prev = layer_previous.result if layer_previous is not None else element # sepanjang input\/ neuron sebelumnya\n",
    "\n",
    "                if layer_next is not None:\n",
    "                    # Update do_c_do_a for hidden layer\n",
    "\n",
    "                    # Calculate do_c_do_a to be used in calculating do_c_do_w\n",
    "                    # do_c_do_a of output layer has a special value, calculated separately below\n",
    "                    # this is for when layer next is hidden layer\n",
    "                    layer_current.do_c_do_a = np.zeros((layer_current.n_neurons, 1))\n",
    "                    for k, neuron_current in enumerate(layer_current.neurons):\n",
    "                        for j, neuron_next in enumerate(layer_next.neurons):\n",
    "                            layer_current.do_c_do_a[k] += neuron_next.weight[k] * layer_next.result_derivative[j] * layer_next.do_c_do_a[j]\n",
    "                            \n",
    "                else:\n",
    "                    # Update do_c_do_a for output layer\n",
    "                    layer_current.do_c_do_a = 2 * (layer_current.result - y[nth_element]) # dimension same as n_neuron of that layer\n",
    "                #print_debug(f\"layer current doC \/ doA: {layer_current.do_c_do_a}\")\n",
    "                # Update weight the layer\n",
    "                for j, neuron_current in enumerate(layer_current.neurons): \n",
    "                    if layer_next is None and layer_current.activation_function == 'softmax':\n",
    "                        do_c_do_w = a_val_prev * layer_current.do_c_do_a[j] * 0.5\n",
    "                        neuron_current.d_bias[nth_element] = layer_current.do_c_do_a[j] * 0.5\n",
    "                    else:\n",
    "                        do_c_do_w = a_val_prev * layer_current.result_derivative[j] * layer_current.do_c_do_a[j]\n",
    "                        neuron_current.d_bias[nth_element] = layer_current.result_derivative[j] * layer_current.do_c_do_a[j]\n",
    "                    neuron_current.d_weight[nth_element] = do_c_do_w\n",
    "                    #print_debug(f\"doC \/ doW = {a_val_prev} * {layer_current.result_derivative[j]} * {layer_current.do_c_do_a[j]} = {do_c_do_w}\")\n",
    "            self.clear_prev_result()\n",
    "\n",
    "        self._persist_d_weight(learning_rate)\n",
    "        return Methods().loss_functions[self.layers[-1].activation_function](self.predict_batch(x), y)"
   ],
   "execution_count":377,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"q7HNMYyuNDKJ6wxyLz2N2w",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "## Konfigurasi Running Testcase\n",
    "Berikut fungsi untuk menjalankan testcase"
   ],
   "attachments":{},
   "metadata":{
    "datalore":{
     "node_id":"JgQco1wuU20biq5n1fNHqe",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "def run_test_case(filename: str):\n",
    "    print(f\"Running test case for {filename}\\n\\n\")\n",
    "\n",
    "    f = open(filename, \"r\")\n",
    "    data = json.loads(f.read())\n",
    "    f.close()\n",
    "\n",
    "    n_layer = len(data[\"case\"][\"model\"][\"layers\"])\n",
    "\n",
    "    model = Model()\n",
    "\n",
    "    for i in range(n_layer):\n",
    "        layer = data[\"case\"][\"model\"][\"layers\"][i]\n",
    "        if i == 0:\n",
    "            model.add(\n",
    "                Layer(\n",
    "                    n_neurons=layer[\"number_of_neurons\"],\n",
    "                    activation_function=layer[\"activation_function\"],\n",
    "                    input_size=data[\"case\"][\"model\"][\"input_size\"]\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            model.add(\n",
    "                Layer(\n",
    "                    n_neurons=layer[\"number_of_neurons\"],\n",
    "                    activation_function=layer[\"activation_function\"],\n",
    "                )\n",
    "            )\n",
    "\n",
    "    model.compile()\n",
    "\n",
    "    for i in range(n_layer):\n",
    "        model.layers[i].update_weights_from_normalized(np.array(data[\"case\"][\"initial_weights\"][i]))\n",
    "        print(f\"Initial weight for layer {i}\\n{model.layers[i].normalized_weights}\")\n",
    "\n",
    "    model.visualize()\n",
    "\n",
    "    data_input = np.array(data[\"case\"][\"input\"])\n",
    "    data_expect = np.array(data[\"case\"][\"target\"])\n",
    "\n",
    "    print(f\"Training data input {len(data['case']['input'])} rows\")\n",
    "\n",
    "    # begin training\n",
    "    model.fit(\n",
    "        x=data_input,\n",
    "        y=data_expect,\n",
    "        learning_rate=data[\"case\"][\"learning_parameters\"][\"learning_rate\"],\n",
    "        epochs=data[\"case\"][\"learning_parameters\"][\"max_iteration\"],\n",
    "        batch_size=data[\"case\"][\"learning_parameters\"][\"batch_size\"],\n",
    "        error_threshold=data[\"case\"][\"learning_parameters\"][\"error_threshold\"]\n",
    "    )\n",
    "\n",
    "    print(f\"\\nStop reason by {model.training_stop_reason} expected {data['expect']['stopped_by']}\\n\")\n",
    "\n",
    "    for i in range(n_layer):\n",
    "        expect_weight = np.array(data[\"expect\"][\"final_weights\"][i])\n",
    "        result_weight = model.layers[i].normalized_weights\n",
    "        print(f\"result weight for layer-{i}\\n{result_weight}\")\n",
    "        print(f\"expected weight for layer-{i}\\n{expect_weight}\")\n",
    "        print(f\"abs weight difference for layer-{i}\\n{np.abs(result_weight - expect_weight)}\")\n",
    "        rmse = np.mean((expect_weight-result_weight)**2, dtype=np.float64)\n",
    "        print(f\"\\nLayer {i} weights rmse {rmse}\\n\")\n",
    "\n",
    "    output = model.predict_batch(np.array(data[\"case\"][\"input\"]))\n",
    "\n",
    "    print(f\"output {output}\\nexpect {data_expect}\")\n",
    "\n",
    "    error_result = np.mean((output-data_expect)**2)\n",
    "    print(f\"Testing result error {error_result}\")"
   ],
   "execution_count":378,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"Tz2bIevWh40AoYbVabmb0Z",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "def run_test_on_reference(filename: str):\n",
    "    print(f\"Running test case for {filename}\\n\\n\")\n",
    "\n",
    "    f = open(filename, \"r\")\n",
    "    data = json.loads(f.read())\n",
    "    f.close()\n",
    "\n",
    "    input_layer = Input(shape=(data[\"case\"][\"model\"][\"input_size\"],))\n",
    "\n",
    "    n_layer = len(data[\"case\"][\"model\"][\"layers\"])\n",
    "\n",
    "    for i in range(n_layer):\n",
    "        layer = data[\"case\"][\"model\"][\"layers\"][i]\n",
    "        layers = Dense(\n",
    "                layer[\"number_of_neurons\"],\n",
    "                activation=layer[\"activation_function\"],\n",
    "            )(input_layer if i == 0 else layers)       \n",
    "    \n",
    "    model = tf.keras.Model(inputs=input_layer, outputs=layers)    \n",
    "\n",
    "    model.compile(\n",
    "        optimizer=SGD(data[\"case\"][\"learning_parameters\"][\"learning_rate\"]),\n",
    "        loss=MeanSquaredError(\n",
    "            reduction=\"sum_over_batch_size\", \n",
    "            name=\"mean_squared_error\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    for i in range(n_layer):\n",
    "        weights = data[\"case\"][\"initial_weights\"][i]\n",
    "        layer = model.layers[i + 1]\n",
    "        main_weight = np.array(weights[1:])\n",
    "        bias = np.array(weights[0])\n",
    "\n",
    "        layer.set_weights([main_weight, bias])\n",
    "\n",
    "    data_input = np.array(data[\"case\"][\"input\"])\n",
    "    data_expect = np.array(data[\"case\"][\"target\"])\n",
    "        \n",
    "    model.fit(\n",
    "        x=data_input,\n",
    "        y=data_expect,\n",
    "        batch_size=data[\"case\"][\"learning_parameters\"][\"batch_size\"],\n",
    "        epochs=data[\"case\"][\"learning_parameters\"][\"max_iteration\"]\n",
    "    )\n",
    "\n",
    "    output = model.predict(np.array(data[\"case\"][\"input\"]))\n",
    "\n",
    "    for i in range(n_layer):\n",
    "            layer = model.layers[i + 1]\n",
    "            raw_weight = layer.get_weights()\n",
    "            print(f\"raw weight\\n{raw_weight}\\n weight shape {raw_weight[0].shape} bias shape {raw_weight[1].shape}\")\n",
    "            normalized_weight = np.array([raw_weight[1].tolist()] + raw_weight[0].tolist())\n",
    "            print(f\"result weight for layer-{i}\\n{normalized_weight}\")\n",
    "            expect_weight = np.array(data[\"expect\"][\"final_weights\"][i])\n",
    "            print(f\"expected weight for layer-{i}\\n{expect_weight}\")\n",
    "            rmse = np.mean((expect_weight-normalized_weight)**2, dtype=np.float64)\n",
    "            print(f\"\\nLayer {i} weights rmse {rmse}\\n\")\n",
    "\n",
    "    print(f\"output \\n{output}\\nexpect {data_expect}\")\n",
    "\n",
    "    error_result = np.mean((output-data_expect)**2)\n",
    "    print(f\"Testing result error {error_result}\")"
   ],
   "execution_count":379,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"LWHnK6oZjtPjMcro9Jvqvv",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "test_cases = [\n",
    "    \"linear.json\", # 0\n",
    "    \"linear_small_lr.json\", # 1\n",
    "    \"linear_two_iteration.json\", # 2\n",
    "    \"mlp.json\", # 3\n",
    "    \"relu_b.json\", # 4\n",
    "    \"sigmoid.json\", # 5\n",
    "    \"softmax.json\", # 6\n",
    "    \"softmax_two_layer.json\" # 7\n",
    "]"
   ],
   "execution_count":380,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"SIP7ceGxP9Aymxnxyasfqj",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "### Linear.json\n",
    "Testcase untuk linear.json"
   ],
   "attachments":{},
   "metadata":{
    "datalore":{
     "node_id":"zFMqIfWZZ8UIoxU6a2GO6u",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "run_test_case_mode(f\"tc_b\/{test_cases[0]}\")"
   ],
   "execution_count":381,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "Running test case for tc_b\/linear.json\n",
      "\n",
      "\n",
      "Initial weight for layer 0\n",
      "[[ 0.1  0.3  0.2]\n",
      " [ 0.4  0.2 -0.7]\n",
      " [ 0.1 -0.8  0.5]]\n",
      "\/* Layer-1 (linear) *\/\n",
      "\/* Neuron-1 *\/\n",
      "Input Layer Neuron-1 -> Hidden Layer-1 Neuron-1 (0.1)\n",
      "\n",
      "\n",
      "\n",
      "\/* Neuron-2 *\/\n",
      "Input Layer Neuron-1 -> Hidden Layer-1 Neuron-2 (-0.8)\n",
      "\n",
      "\n",
      "\n",
      "\/* Neuron-3 *\/\n",
      "Input Layer Neuron-1 -> Hidden Layer-1 Neuron-3 (0.5)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training data input 2 rows\n",
      "Begin training model\n",
      "\n",
      "\n",
      "Epoch 0\n",
      "training loss 0.18184999999999998\n",
      "\n",
      "\n",
      "\n",
      "Stop reason by max_iteration expected max_iteration\n",
      "\n",
      "result weight for layer-0\n",
      "[[ 0.22  0.36  0.11]\n",
      " [ 0.64  0.3  -0.89]\n",
      " [ 0.28 -0.7   0.37]]\n",
      "expected weight for layer-0\n",
      "[[ 0.22  0.36  0.11]\n",
      " [ 0.64  0.3  -0.89]\n",
      " [ 0.28 -0.7   0.37]]\n",
      "abs weight difference for layer-0\n",
      "[[2.77555756e-17 0.00000000e+00 1.38777878e-17]\n",
      " [1.11022302e-16 5.55111512e-17 0.00000000e+00]\n",
      " [5.55111512e-17 0.00000000e+00 0.00000000e+00]]\n",
      "\n",
      "Layer 0 weights rmse 2.1613213820345645e-33\n",
      "\n",
      "output [array([ 2.42,  0.56, -2.19]), array([ 1.42, -0.74, -0.04])]\n",
      "expect [[ 2.   0.3 -1.9]\n",
      " [ 1.3 -0.7  0.1]]\n",
      "Testing result error 0.06061666666666666\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"sit6VQdBI4gyLxEPBL0wOb",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "run_test_case_reference_mode(f\"tc_b\/{test_cases[0]}\")"
   ],
   "execution_count":382,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "Running test case for tc_b\/linear.json\n",
      "\n",
      "\n",
      "\r1\/1 [==============================] - ETA: 0s - loss: 0.2217\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1\/1 [==============================] - 2s 2s\/step - loss: 0.2217\n",
      "\r1\/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1\/1 [==============================] - 0s 338ms\/step\n",
      "raw weight\n",
      "[array([[ 0.48      ,  0.23333333, -0.7633333 ],\n",
      "       [ 0.16      , -0.7666667 ,  0.45666665]], dtype=float32), array([0.14      , 0.32000002, 0.17      ], dtype=float32)]\n",
      " weight shape (2, 3) bias shape (3,)\n",
      "result weight for layer-0\n",
      "[[ 0.14        0.32000002  0.17      ]\n",
      " [ 0.47999999  0.23333333 -0.76333332]\n",
      " [ 0.16       -0.76666671  0.45666665]]\n",
      "expected weight for layer-0\n",
      "[[ 0.22  0.36  0.11]\n",
      " [ 0.64  0.3  -0.89]\n",
      " [ 0.28 -0.7   0.37]]\n",
      "\n",
      "Layer 0 weights rmse 0.009338272532416321\n",
      "\n",
      "output \n",
      "[[ 1.7399999   0.2533333  -1.6633333 ]\n",
      " [ 0.93999994 -0.98        0.32      ]]\n",
      "expect [[ 2.   0.3 -1.9]\n",
      " [ 1.3 -0.7  0.1]]\n",
      "Testing result error 0.06369816974445938\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"zBIsE03A4o1JuuhCrGCWBk",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "### Linear_small_lr.json\n",
    "Testcase untuk Linear_small_lr.json"
   ],
   "attachments":{},
   "metadata":{
    "datalore":{
     "node_id":"ZxvqVA8zKQRQXLnKm6GQN1",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "run_test_case_mode(f\"tc_b\/{test_cases[1]}\")"
   ],
   "execution_count":383,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "Running test case for tc_b\/linear_small_lr.json\n",
      "\n",
      "\n",
      "Initial weight for layer 0\n",
      "[[ 0.1  0.3  0.2]\n",
      " [ 0.4  0.2 -0.7]\n",
      " [ 0.1 -0.8  0.5]]\n",
      "\/* Layer-1 (linear) *\/\n",
      "\/* Neuron-1 *\/\n",
      "Input Layer Neuron-1 -> Hidden Layer-1 Neuron-1 (0.1)\n",
      "\n",
      "\n",
      "\n",
      "\/* Neuron-2 *\/\n",
      "Input Layer Neuron-1 -> Hidden Layer-1 Neuron-2 (-0.8)\n",
      "\n",
      "\n",
      "\n",
      "\/* Neuron-3 *\/\n",
      "Input Layer Neuron-1 -> Hidden Layer-1 Neuron-3 (0.5)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training data input 2 rows\n",
      "Begin training model\n",
      "\n",
      "\n",
      "Epoch 0\n",
      "training loss 0.6462307850000001\n",
      "\n",
      "\n",
      "\n",
      "Stop reason by max_iteration expected max_iteration\n",
      "\n",
      "result weight for layer-0\n",
      "[[ 0.1012  0.3006  0.1991]\n",
      " [ 0.4024  0.201  -0.7019]\n",
      " [ 0.1018 -0.799   0.4987]]\n",
      "expected weight for layer-0\n",
      "[[ 0.1008  0.3006  0.1991]\n",
      " [ 0.402   0.201  -0.7019]\n",
      " [ 0.101  -0.799   0.4987]]\n",
      "abs weight difference for layer-0\n",
      "[[0.0004 0.     0.    ]\n",
      " [0.0004 0.     0.    ]\n",
      " [0.0008 0.     0.    ]]\n",
      "\n",
      "Layer 0 weights rmse 1.0666666666666786e-07\n",
      "\n",
      "output [array([ 1.4102,  0.1046, -1.4079]), array([ 0.7072, -1.0964,  0.4946])]\n",
      "expect [[ 2.   0.3 -1.9]\n",
      " [ 1.3 -0.7  0.1]]\n",
      "Testing result error 0.21541026166666669\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"ASoLJLtpwldqV7y12qTRAn",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "run_test_case_reference_mode(f\"tc_b\/{test_cases[1]}\")"
   ],
   "execution_count":384,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "Running test case for tc_b\/linear_small_lr.json\n",
      "\n",
      "\n",
      "\r1\/1 [==============================] - ETA: 0s - loss: 0.2217\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1\/1 [==============================] - 1s 1s\/step - loss: 0.2217\n",
      "\r1\/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1\/1 [==============================] - 0s 409ms\/step\n",
      "raw weight\n",
      "[array([[ 0.40080002,  0.20033334, -0.70063335],\n",
      "       [ 0.1006    , -0.7996667 ,  0.49956667]], dtype=float32), array([0.1004    , 0.30020002, 0.1997    ], dtype=float32)]\n",
      " weight shape (2, 3) bias shape (3,)\n",
      "result weight for layer-0\n",
      "[[ 0.1004      0.30020002  0.1997    ]\n",
      " [ 0.40080002  0.20033334 -0.70063335]\n",
      " [ 0.1006     -0.7996667   0.49956667]]\n",
      "expected weight for layer-0\n",
      "[[ 0.1008  0.3006  0.1991]\n",
      " [ 0.402   0.201  -0.7019]\n",
      " [ 0.101  -0.799   0.4987]]\n",
      "\n",
      "Layer 0 weights rmse 6.138215430143493e-07\n",
      "\n",
      "output \n",
      "[[ 1.4034001   0.10153332 -1.4026334 ]\n",
      " [ 0.70239997 -1.0988001   0.4982    ]]\n",
      "expect [[ 2.   0.3 -1.9]\n",
      " [ 1.3 -0.7  0.1]]\n",
      "Testing result error 0.21957075650185745\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"m40sGr0Pk0ovpRKWWfy1KS",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "### linear_two_iteration.json\n",
    "Testcase untuk linear_two_iteration.json"
   ],
   "attachments":{},
   "metadata":{
    "datalore":{
     "node_id":"yPIY75JJMozn3uJlaMViBY",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "run_test_case_mode(f\"tc_b\/{test_cases[2]}\")"
   ],
   "execution_count":385,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "Running test case for tc_b\/linear_two_iteration.json\n",
      "\n",
      "\n",
      "Initial weight for layer 0\n",
      "[[ 0.1  0.3  0.2]\n",
      " [ 0.4  0.2 -0.7]\n",
      " [ 0.1 -0.8  0.5]]\n",
      "\/* Layer-1 (linear) *\/\n",
      "\/* Neuron-1 *\/\n",
      "Input Layer Neuron-1 -> Hidden Layer-1 Neuron-1 (0.1)\n",
      "\n",
      "\n",
      "\n",
      "\/* Neuron-2 *\/\n",
      "Input Layer Neuron-1 -> Hidden Layer-1 Neuron-2 (-0.8)\n",
      "\n",
      "\n",
      "\n",
      "\/* Neuron-3 *\/\n",
      "Input Layer Neuron-1 -> Hidden Layer-1 Neuron-3 (0.5)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training data input 2 rows\n",
      "Begin training model\n",
      "\n",
      "\n",
      "Epoch 0\n",
      "training loss 0.18184999999999998\n",
      "\n",
      "\n",
      "Epoch 1\n",
      "training loss 0.05544650000000001\n",
      "\n",
      "\n",
      "\n",
      "Stop reason by max_iteration expected max_iteration\n",
      "\n",
      "result weight for layer-0\n",
      "[[ 0.166  0.338  0.153]\n",
      " [ 0.502  0.226 -0.789]\n",
      " [ 0.214 -0.718  0.427]]\n",
      "expected weight for layer-0\n",
      "[[ 0.166  0.338  0.153]\n",
      " [ 0.502  0.226 -0.789]\n",
      " [ 0.214 -0.718  0.427]]\n",
      "abs weight difference for layer-0\n",
      "[[0.00000000e+00 5.55111512e-17 0.00000000e+00]\n",
      " [0.00000000e+00 2.77555756e-17 1.11022302e-16]\n",
      " [2.77555756e-17 0.00000000e+00 0.00000000e+00]]\n",
      "\n",
      "Layer 0 weights rmse 1.8831315011786308e-33\n",
      "\n",
      "output [array([ 1.886,  0.298, -1.787]), array([ 1.096, -0.872,  0.218])]\n",
      "expect [[ 2.   0.3 -1.9]\n",
      " [ 1.3 -0.7  0.1]]\n",
      "Testing result error 0.01848216666666667\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"ORFFq2oZOBn0tIQ7GwqlfE",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "run_test_case_reference_mode(f\"tc_b\/{test_cases[2]}\")"
   ],
   "execution_count":386,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "Running test case for tc_b\/linear_two_iteration.json\n",
      "\n",
      "\n",
      "Epoch 1\/2\n",
      "\r1\/1 [==============================] - ETA: 0s - loss: 0.2217\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1\/1 [==============================] - 2s 2s\/step - loss: 0.2217\n",
      "Epoch 2\/2\n",
      "\r1\/1 [==============================] - ETA: 0s - loss: 0.0637\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1\/1 [==============================] - 0s 20ms\/step - loss: 0.0637\n",
      "\r1\/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1\/1 [==============================] - 0s 472ms\/step\n",
      "raw weight\n",
      "[array([[ 0.518     ,  0.24733335, -0.79433334],\n",
      "       [ 0.19266666, -0.74644446,  0.4341111 ]], dtype=float32), array([0.16066667, 0.33088893, 0.15477778], dtype=float32)]\n",
      " weight shape (2, 3) bias shape (3,)\n",
      "result weight for layer-0\n",
      "[[ 0.16066667  0.33088893  0.15477778]\n",
      " [ 0.51800001  0.24733335 -0.79433334]\n",
      " [ 0.19266666 -0.74644446  0.43411109]]\n",
      "expected weight for layer-0\n",
      "[[ 0.166  0.338  0.153]\n",
      " [ 0.502  0.226 -0.789]\n",
      " [ 0.214 -0.718  0.427]]\n",
      "\n",
      "Layer 0 weights rmse 0.00023738832876708866\n",
      "\n",
      "output \n",
      "[[ 1.9073334   0.3264445  -1.794111  ]\n",
      " [ 1.064      -0.9146667   0.22866662]]\n",
      "expect [[ 2.   0.3 -1.9]\n",
      " [ 1.3 -0.7  0.1]]\n",
      "Testing result error 0.02313863068351268\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"7fCvtv0S7cThh4JdL3c6Wy",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "### mlp.json\n",
    "Testcase untuk mlp.json"
   ],
   "attachments":{},
   "metadata":{
    "datalore":{
     "node_id":"aHZ1VMEfrbxWOByGnuHsZp",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "run_test_case_mode(f\"tc_b\/{test_cases[3]}\")"
   ],
   "execution_count":387,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "Running test case for tc_b\/mlp.json\n",
      "\n",
      "\n",
      "Initial weight for layer 0\n",
      "[[ 0.1  0.2]\n",
      " [-0.3  0.5]\n",
      " [ 0.4  0.5]]\n",
      "Initial weight for layer 1\n",
      "[[ 0.2  0.1]\n",
      " [ 0.4 -0.5]\n",
      " [ 0.7  0.8]]\n",
      "\/* Layer-1 (linear) *\/\n",
      "\/* Neuron-1 *\/\n",
      "Input Layer Neuron-1 -> Hidden Layer-1 Neuron-1 (0.4)\n",
      "\n",
      "\n",
      "\n",
      "\/* Neuron-2 *\/\n",
      "Input Layer Neuron-1 -> Hidden Layer-1 Neuron-2 (0.5)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\/* Layer-2 (relu) *\/\n",
      "\/* Neuron-1 *\/\n",
      "Hidden Layer-1 Neuron-1 -> Hidden Layer-2 Neuron-1 (0.7)\n",
      "\n",
      "\n",
      "\n",
      "\/* Neuron-2 *\/\n",
      "Hidden Layer-1 Neuron-1 -> Hidden Layer-2 Neuron-2 (0.8)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training data input 2 rows\n",
      "Begin training model\n",
      "\n",
      "\n",
      "Epoch 0\n",
      "training loss 0.330162168871235\n",
      "\n",
      "\n",
      "\n",
      "Stop reason by max_iteration expected max_iteration\n",
      "\n",
      "result weight for layer-0\n",
      "[[ 0.08592   0.32276 ]\n",
      " [-0.33872   0.46172 ]\n",
      " [ 0.449984  0.440072]]\n",
      "expected weight for layer-0\n",
      "[[ 0.08592   0.32276 ]\n",
      " [-0.33872   0.46172 ]\n",
      " [ 0.449984  0.440072]]\n",
      "abs weight difference for layer-0\n",
      "[[1.38777878e-17 5.55111512e-17]\n",
      " [0.00000000e+00 0.00000000e+00]\n",
      " [5.55111512e-17 0.00000000e+00]]\n",
      "\n",
      "Layer 0 weights rmse 1.0592614694129797e-33\n",
      "\n",
      "result weight for layer-1\n",
      "[[ 0.2748    0.188   ]\n",
      " [ 0.435904 -0.53168 ]\n",
      " [ 0.68504   0.7824  ]]\n",
      "expected weight for layer-1\n",
      "[[ 0.2748    0.188   ]\n",
      " [ 0.435904 -0.53168 ]\n",
      " [ 0.68504   0.7824  ]]\n",
      "abs weight difference for layer-1\n",
      "[[5.55111512e-17 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 1.11022302e-16]]\n",
      "\n",
      "Layer 1 weights rmse 2.5679065925163146e-33\n",
      "\n",
      "output [array([0.46423247, 0.        ]), array([0.06946909, 0.39804871])]\n",
      "expect [[1.  0.1]\n",
      " [0.1 1. ]]\n",
      "Testing result error 0.1650810844356175\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"eoxQ21ZxIWVL3ZaTdYSD48",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "run_test_case_reference_mode(f\"tc_b\/{test_cases[3]}\")"
   ],
   "execution_count":388,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "Running test case for tc_b\/mlp.json\n",
      "\n",
      "\n",
      "\r1\/1 [==============================] - ETA: 0s - loss: 0.3385\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1\/1 [==============================] - 2s 2s\/step - loss: 0.3385\n",
      "\r1\/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1\/1 [==============================] - 0s 380ms\/step\n",
      "raw weight\n",
      "[array([[-0.31936002,  0.48086   ],\n",
      "       [ 0.424992  ,  0.470036  ]], dtype=float32), array([0.09296   , 0.26138002], dtype=float32)]\n",
      " weight shape (2, 2) bias shape (2,)\n",
      "result weight for layer-0\n",
      "[[ 0.09296     0.26138002]\n",
      " [-0.31936002  0.48085999]\n",
      " [ 0.424992    0.470036  ]]\n",
      "expected weight for layer-0\n",
      "[[ 0.08592   0.32276 ]\n",
      " [-0.33872   0.46172 ]\n",
      " [ 0.449984  0.440072]]\n",
      "\n",
      "Layer 0 weights rmse 0.001013442309940077\n",
      "\n",
      "raw weight\n",
      "[array([[ 0.417952  , -0.51584   ],\n",
      "       [ 0.69251996,  0.79120004]], dtype=float32), array([0.2374, 0.144 ], dtype=float32)]\n",
      " weight shape (2, 2) bias shape (2,)\n",
      "result weight for layer-1\n",
      "[[ 0.2374      0.14399999]\n",
      " [ 0.417952   -0.51583999]\n",
      " [ 0.69251996  0.79120004]]\n",
      "expected weight for layer-1\n",
      "[[ 0.2748    0.188   ]\n",
      " [ 0.435904 -0.53168 ]\n",
      " [ 0.68504   0.7824  ]]\n",
      "\n",
      "Layer 1 weights rmse 0.0006735552549592174\n",
      "\n",
      "output \n",
      "[[0.3583628 0.       ]\n",
      " [0.        0.2592258]]\n",
      "expect [[1.  0.1]\n",
      " [0.1 1. ]]\n",
      "Testing result error 0.2451111851867528\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"3m3X136Y6FDHe8iB34Db1W",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "### RelU_b.json\n",
    "Testcase untuk relu_b.json"
   ],
   "attachments":{},
   "metadata":{
    "datalore":{
     "node_id":"iQaesEvdvHb5ylKBmsXKET",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "run_test_case_mode(f\"tc_b\/{test_cases[4]}\")"
   ],
   "execution_count":389,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "Running test case for tc_b\/relu_b.json\n",
      "\n",
      "\n",
      "Initial weight for layer 0\n",
      "[[-0.2  0.2  1. ]\n",
      " [ 0.3  0.5  0.5]\n",
      " [-0.5 -1.   0.5]]\n",
      "\/* Layer-1 (relu) *\/\n",
      "\/* Neuron-1 *\/\n",
      "Input Layer Neuron-1 -> Hidden Layer-1 Neuron-1 (-0.5)\n",
      "\n",
      "\n",
      "\n",
      "\/* Neuron-2 *\/\n",
      "Input Layer Neuron-1 -> Hidden Layer-1 Neuron-2 (-1.0)\n",
      "\n",
      "\n",
      "\n",
      "\/* Neuron-3 *\/\n",
      "Input Layer Neuron-1 -> Hidden Layer-1 Neuron-3 (0.5)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training data input 2 rows\n",
      "Begin training model\n",
      "\n",
      "\n",
      "Epoch 0\n",
      "training loss 1.7831637775499998\n",
      "\n",
      "\n",
      "\n",
      "Stop reason by max_iteration expected max_iteration\n",
      "\n",
      "result weight for layer-0\n",
      "[[-0.211   0.105   0.885 ]\n",
      " [ 0.3033  0.5285  0.3005]\n",
      " [-0.489  -0.905   0.291 ]]\n",
      "expected weight for layer-0\n",
      "[[-0.211   0.105   0.885 ]\n",
      " [ 0.3033  0.5285  0.3005]\n",
      " [-0.489  -0.905   0.291 ]]\n",
      "abs weight difference for layer-0\n",
      "[[2.77555756e-17 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 5.55111512e-17]]\n",
      "\n",
      "Layer 0 weights rmse 4.279844320860524e-34\n",
      "\n",
      "output [array([0.    , 0.    , 1.4183]), array([0.18701, 0.85145, 0.50385])]\n",
      "expect [[1.  0.1 0.1]\n",
      " [0.1 0.1 1. ]]\n",
      "Testing result error 0.5943879258499999\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"DZPEfRWaFxWuh6POKFaa7R",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "run_test_case_reference_mode(f\"tc_b\/{test_cases[4]}\")"
   ],
   "execution_count":390,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "Running test case for tc_b\/relu_b.json\n",
      "\n",
      "\n",
      "\r1\/1 [==============================] - ETA: 0s - loss: 0.9312\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1\/1 [==============================] - 2s 2s\/step - loss: 0.9312\n",
      "\r1\/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1\/1 [==============================] - 0s 240ms\/step\n",
      "raw weight\n",
      "[array([[ 0.30110002,  0.5095    ,  0.4335    ],\n",
      "       [-0.49633333, -0.9683333 ,  0.43033332]], dtype=float32), array([-0.20366667,  0.16833334,  0.96166664], dtype=float32)]\n",
      " weight shape (2, 3) bias shape (3,)\n",
      "result weight for layer-0\n",
      "[[-0.20366667  0.16833334  0.96166664]\n",
      " [ 0.30110002  0.50950003  0.43349999]\n",
      " [-0.49633333 -0.9683333   0.43033332]]\n",
      "expected weight for layer-0\n",
      "[[-0.211   0.105   0.885 ]\n",
      " [ 0.3033  0.5285  0.3005]\n",
      " [-0.489  -0.905   0.291 ]]\n",
      "\n",
      "Layer 0 weights rmse 0.005719573156393852\n",
      "\n",
      "output \n",
      "[[0.         0.         1.7394333 ]\n",
      " [0.20233665 0.9838166  0.40128332]]\n",
      "expect [[1.  0.1 0.1]\n",
      " [0.1 0.1 1. ]]\n",
      "Testing result error 0.8079679635266404\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"GBuet3IVi6LUldcdCOtc9H",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "### Sigmoid.json\n",
    "Testcase untuk sigmoid.json"
   ],
   "attachments":{},
   "metadata":{
    "datalore":{
     "node_id":"ZV5qpk9k98jK4oLlkfEN8R",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "run_test_case_mode(f\"tc_b\/{test_cases[5]}\")"
   ],
   "execution_count":391,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "Running test case for tc_b\/sigmoid.json\n",
      "\n",
      "\n",
      "Initial weight for layer 0\n",
      "[[0.3 0.1]\n",
      " [0.2 0.6]\n",
      " [0.8 0.3]]\n",
      "\/* Layer-1 (sigmoid) *\/\n",
      "\/* Neuron-1 *\/\n",
      "Input Layer Neuron-1 -> Hidden Layer-1 Neuron-1 (0.8)\n",
      "\n",
      "\n",
      "\n",
      "\/* Neuron-2 *\/\n",
      "Input Layer Neuron-1 -> Hidden Layer-1 Neuron-2 (0.3)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training data input 2 rows\n",
      "Begin training model\n",
      "\n",
      "\n",
      "Epoch 0\n",
      "training loss 0.47078073489758887\n",
      "\n",
      "\n",
      "Epoch 1\n",
      "training loss 0.4687675781198633\n",
      "\n",
      "\n",
      "Epoch 2\n",
      "training loss 0.4667706588834645\n",
      "\n",
      "\n",
      "Epoch 3\n",
      "training loss 0.4647899555600068\n",
      "\n",
      "\n",
      "Epoch 4\n",
      "training loss 0.4628254366344452\n",
      "\n",
      "\n",
      "Epoch 5\n",
      "training loss 0.46087706106867526\n",
      "\n",
      "\n",
      "Epoch 6\n",
      "training loss 0.45894477867098016\n",
      "\n",
      "\n",
      "Epoch 7\n",
      "training loss 0.4570285304700267\n",
      "\n",
      "\n",
      "Epoch 8\n",
      "training loss 0.4551282490921642\n",
      "\n",
      "\n",
      "Epoch 9\n",
      "training loss 0.4532438591408193\n",
      "\n",
      "\n",
      "\n",
      "Stop reason by max_iteration expected max_iteration\n",
      "\n",
      "result weight for layer-0\n",
      "[[0.23291176 0.06015346]\n",
      " [0.12884088 0.64849474]\n",
      " [0.837615   0.23158199]]\n",
      "expected weight for layer-0\n",
      "[[0.2329 0.0601]\n",
      " [0.1288 0.6484]\n",
      " [0.8376 0.2315]]\n",
      "abs weight difference for layer-0\n",
      "[[1.17628669e-05 5.34561966e-05]\n",
      " [4.08805607e-05 9.47353810e-05]\n",
      " [1.50008727e-05 8.19927174e-05]]\n",
      "\n",
      "Layer 0 weights rmse 3.4316290881199706e-09\n",
      "\n",
      "output [array([0.57379022, 0.59493409]), array([0.65739779, 0.54387299])]\n",
      "expect [[0. 1.]\n",
      " [1. 0.]]\n",
      "Testing result error 0.22662192957040966\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"pL5Qycm9zk3meZYqin0cZ5",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "run_test_case_reference_mode(f\"tc_b\/{test_cases[5]}\")"
   ],
   "execution_count":392,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "Running test case for tc_b\/sigmoid.json\n",
      "\n",
      "\n",
      "Epoch 1\/10\n",
      "\r1\/1 [==============================] - ETA: 0s - loss: 0.2364\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1\/1 [==============================] - 2s 2s\/step - loss: 0.2364\n",
      "Epoch 2\/10\n",
      "\r1\/1 [==============================] - ETA: 0s - loss: 0.2359\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1\/1 [==============================] - 0s 58ms\/step - loss: 0.2359\n",
      "Epoch 3\/10\n",
      "\r1\/1 [==============================] - ETA: 0s - loss: 0.2354\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1\/1 [==============================] - 0s 36ms\/step - loss: 0.2354\n",
      "Epoch 4\/10\n",
      "\r1\/1 [==============================] - ETA: 0s - loss: 0.2349\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1\/1 [==============================] - 0s 42ms\/step - loss: 0.2349\n",
      "Epoch 5\/10\n",
      "\r1\/1 [==============================] - ETA: 0s - loss: 0.2344\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1\/1 [==============================] - 0s 20ms\/step - loss: 0.2344\n",
      "Epoch 6\/10\n",
      "\r1\/1 [==============================] - ETA: 0s - loss: 0.2339\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1\/1 [==============================] - 0s 19ms\/step - loss: 0.2339\n",
      "Epoch 7\/10\n",
      "\r1\/1 [==============================] - ETA: 0s - loss: 0.2334\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1\/1 [==============================] - 0s 19ms\/step - loss: 0.2334\n",
      "Epoch 8\/10\n",
      "\r1\/1 [==============================] - ETA: 0s - loss: 0.2329\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1\/1 [==============================] - 0s 20ms\/step - loss: 0.2329\n",
      "Epoch 9\/10\n",
      "\r1\/1 [==============================] - ETA: 0s - loss: 0.2324\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1\/1 [==============================] - 0s 79ms\/step - loss: 0.2324\n",
      "Epoch 10\/10\n",
      "\r1\/1 [==============================] - ETA: 0s - loss: 0.2319\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1\/1 [==============================] - 0s 41ms\/step - loss: 0.2319\n",
      "\r1\/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1\/1 [==============================] - 0s 362ms\/step\n",
      "raw weight\n",
      "[array([[0.16422285, 0.62417907],\n",
      "       [0.81860363, 0.26559675]], dtype=float32), array([0.2656529 , 0.07955156], dtype=float32)]\n",
      " weight shape (2, 2) bias shape (2,)\n",
      "result weight for layer-0\n",
      "[[0.26565289 0.07955156]\n",
      " [0.16422285 0.62417907]\n",
      " [0.81860363 0.26559675]]\n",
      "expected weight for layer-0\n",
      "[[0.2329 0.0601]\n",
      " [0.1288 0.6484]\n",
      " [0.8376 0.2315]]\n",
      "\n",
      "Layer 0 weights rmse 0.0008026662140668866\n",
      "\n",
      "output \n",
      "[[0.58607537 0.59667766]\n",
      " [0.6626117  0.55288893]]\n",
      "expect [[0. 1.]\n",
      " [1. 0.]]\n",
      "Testing result error 0.23141756537930824\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"iHrlUe1XqQtp86yXblxH62",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "### Softmax.json\n",
    "Testcase untuk softmax.json"
   ],
   "attachments":{},
   "metadata":{
    "datalore":{
     "node_id":"BXkK476vRaqwYU3iQYj0rE",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "run_test_case_mode(f\"tc_b\/{test_cases[6]}\")"
   ],
   "execution_count":393,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "Running test case for tc_b\/softmax.json\n",
      "\n",
      "\n",
      "Initial weight for layer 0\n",
      "[[ 0.1  0.9 -0.1]\n",
      " [-0.2  0.8  0.2]\n",
      " [ 0.3 -0.7  0.3]\n",
      " [ 0.4  0.6 -0.4]\n",
      " [ 0.5  0.5  0.5]\n",
      " [-0.6  0.4  0.6]\n",
      " [-0.7 -0.3  0.7]\n",
      " [ 0.8  0.2 -0.8]\n",
      " [ 0.9 -0.1  0. ]]\n",
      "\/* Layer-1 (softmax) *\/\n",
      "\/* Neuron-1 *\/\n",
      "Input Layer Neuron-1 -> Hidden Layer-1 Neuron-1 (0.3)\n",
      "Input Layer Neuron-2 -> Hidden Layer-1 Neuron-1 (0.4)\n",
      "Input Layer Neuron-3 -> Hidden Layer-1 Neuron-1 (0.5)\n",
      "Input Layer Neuron-4 -> Hidden Layer-1 Neuron-1 (-0.6)\n",
      "Input Layer Neuron-5 -> Hidden Layer-1 Neuron-1 (-0.7)\n",
      "Input Layer Neuron-6 -> Hidden Layer-1 Neuron-1 (0.8)\n",
      "Input Layer Neuron-7 -> Hidden Layer-1 Neuron-1 (0.9)\n",
      "\n",
      "\n",
      "\n",
      "\/* Neuron-2 *\/\n",
      "Input Layer Neuron-1 -> Hidden Layer-1 Neuron-2 (-0.7)\n",
      "Input Layer Neuron-2 -> Hidden Layer-1 Neuron-2 (0.6)\n",
      "Input Layer Neuron-3 -> Hidden Layer-1 Neuron-2 (0.5)\n",
      "Input Layer Neuron-4 -> Hidden Layer-1 Neuron-2 (0.4)\n",
      "Input Layer Neuron-5 -> Hidden Layer-1 Neuron-2 (-0.3)\n",
      "Input Layer Neuron-6 -> Hidden Layer-1 Neuron-2 (0.2)\n",
      "Input Layer Neuron-7 -> Hidden Layer-1 Neuron-2 (-0.1)\n",
      "\n",
      "\n",
      "\n",
      "\/* Neuron-3 *\/\n",
      "Input Layer Neuron-1 -> Hidden Layer-1 Neuron-3 (0.3)\n",
      "Input Layer Neuron-2 -> Hidden Layer-1 Neuron-3 (-0.4)\n",
      "Input Layer Neuron-3 -> Hidden Layer-1 Neuron-3 (0.5)\n",
      "Input Layer Neuron-4 -> Hidden Layer-1 Neuron-3 (0.6)\n",
      "Input Layer Neuron-5 -> Hidden Layer-1 Neuron-3 (0.7)\n",
      "Input Layer Neuron-6 -> Hidden Layer-1 Neuron-3 (-0.8)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training data input 3 rows\n",
      "Begin training model\n",
      "\n",
      "\n",
      "Epoch 0\n",
      "training loss 2.414367579883245\n",
      "\n",
      "\n",
      "Epoch 1\n",
      "training loss 2.1202018883316325\n",
      "\n",
      "\n",
      "Epoch 2\n",
      "training loss 1.8685433457357066\n",
      "\n",
      "\n",
      "Epoch 3\n",
      "training loss 1.6484909679075355\n",
      "\n",
      "\n",
      "Epoch 4\n",
      "training loss 1.451783118701801\n",
      "\n",
      "\n",
      "Epoch 5\n",
      "training loss 1.2733807672866084\n",
      "\n",
      "\n",
      "Epoch 6\n",
      "training loss 1.1106831427009527\n",
      "\n",
      "\n",
      "Epoch 7\n",
      "training loss 0.9627316093335913\n",
      "\n",
      "\n",
      "Epoch 8\n",
      "training loss 0.8295766493205968\n",
      "\n",
      "\n",
      "Epoch 9\n",
      "training loss 0.7117019158190816\n",
      "\n",
      "\n",
      "\n",
      "Stop reason by max_iteration expected max_iteration\n",
      "\n",
      "result weight for layer-0\n",
      "[[ 0.12674605  0.9149538  -0.14169985]\n",
      " [-0.33551647  0.67700488  0.45851159]\n",
      " [ 0.48314436 -0.85241216  0.2692678 ]\n",
      " [ 0.3400255   0.57237542 -0.31240092]\n",
      " [ 0.31397716  0.46349737  0.72252547]\n",
      " [-0.69652442  0.4789189   0.61760552]\n",
      " [-0.50884515 -0.36354141  0.57238656]\n",
      " [ 0.41891295  0.26354517 -0.48245812]\n",
      " [ 0.90374164 -0.01759501 -0.08614663]]\n",
      "expected weight for layer-0\n",
      "[[ 0.12674605  0.9149538  -0.14169985]\n",
      " [-0.33551647  0.67700488  0.45851159]\n",
      " [ 0.48314436 -0.85241216  0.2692678 ]\n",
      " [ 0.3400255   0.57237542 -0.31240092]\n",
      " [ 0.31397716  0.46349737  0.72252547]\n",
      " [-0.69652442  0.4789189   0.61760552]\n",
      " [-0.50884515 -0.36354141  0.57238656]\n",
      " [ 0.41891295  0.26354517 -0.48245812]\n",
      " [ 0.90374164 -0.01759501 -0.08614663]]\n",
      "abs weight difference for layer-0\n",
      "[[4.63962480e-09 3.08813641e-10 4.33081127e-09]\n",
      " [2.97979080e-09 4.81639384e-09 1.83660320e-09]\n",
      " [2.71090178e-09 2.08553763e-09 4.79643897e-09]\n",
      " [2.80177775e-09 3.02083702e-09 2.19059548e-10]\n",
      " [3.12208087e-09 1.30288952e-09 1.81919135e-09]\n",
      " [2.86779978e-09 1.25826832e-09 1.60953195e-09]\n",
      " [1.74880377e-09 6.43907039e-10 1.10489717e-09]\n",
      " [3.65710712e-09 7.77727382e-10 4.43483450e-09]\n",
      " [3.89174470e-10 7.46242838e-10 1.13541741e-09]]\n",
      "\n",
      "Layer 0 weights rmse 7.223917404145926e-18\n",
      "\n",
      "output [array([0.12952252, 0.86780322, 0.00267426]), array([0.2046424, 0.0040595, 0.7912981]), array([0.18751908, 0.14036204, 0.67211888])]\n",
      "expect [[0 1 0]\n",
      " [1 0 0]\n",
      " [0 0 1]]\n",
      "Testing result error 0.16171033757301193\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"FWzWfiFmdugZHpqoFoM4z1",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "run_test_case_reference_mode(f\"tc_b\/{test_cases[6]}\")"
   ],
   "execution_count":394,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "Running test case for tc_b\/softmax.json\n",
      "\n",
      "\n",
      "Epoch 1\/10\n",
      "\r1\/3 [=========>....................] - ETA: 3s - loss: 0.3792\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r3\/3 [==============================] - 2s 19ms\/step - loss: 0.4418\n",
      "Epoch 2\/10\n",
      "\r1\/3 [=========>....................] - ETA: 0s - loss: 0.2791\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r3\/3 [==============================] - 0s 10ms\/step - loss: 0.4312\n",
      "Epoch 3\/10\n",
      "\r1\/3 [=========>....................] - ETA: 0s - loss: 0.6602\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r3\/3 [==============================] - 0s 10ms\/step - loss: 0.4200\n",
      "Epoch 4\/10\n",
      "\r1\/3 [=========>....................] - ETA: 0s - loss: 0.6602\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r3\/3 [==============================] - 0s 10ms\/step - loss: 0.4086\n",
      "Epoch 5\/10\n",
      "\r1\/3 [=========>....................] - ETA: 0s - loss: 0.6601\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r3\/3 [==============================] - 0s 19ms\/step - loss: 0.3970\n",
      "Epoch 6\/10\n",
      "\r1\/3 [=========>....................] - ETA: 0s - loss: 0.6600\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r3\/3 [==============================] - 0s 10ms\/step - loss: 0.3855\n",
      "Epoch 7\/10\n",
      "\r1\/3 [=========>....................] - ETA: 0s - loss: 0.2179\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r3\/3 [==============================] - 0s 10ms\/step - loss: 0.3744\n",
      "Epoch 8\/10\n",
      "\r1\/3 [=========>....................] - ETA: 0s - loss: 0.2381\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r3\/3 [==============================] - 0s 10ms\/step - loss: 0.3639\n",
      "Epoch 9\/10\n",
      "\r1\/3 [=========>....................] - ETA: 0s - loss: 0.6598\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r3\/3 [==============================] - 0s 10ms\/step - loss: 0.3541\n",
      "Epoch 10\/10\n",
      "\r1\/3 [=========>....................] - ETA: 0s - loss: 0.2240\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r3\/3 [==============================] - 0s 10ms\/step - loss: 0.3451\n",
      "\r1\/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1\/1 [==============================] - 0s 239ms\/step\n",
      "raw weight\n",
      "[array([[-0.1742265 ,  0.74874806,  0.2254784 ],\n",
      "       [ 0.32756075, -0.7609455 ,  0.3333849 ],\n",
      "       [ 0.40776226,  0.5877591 , -0.39552128],\n",
      "       [ 0.4676497 ,  0.49711534,  0.5352349 ],\n",
      "       [-0.6509822 ,  0.44330794,  0.60767424],\n",
      "       [-0.6962544 , -0.3225484 ,  0.71880263],\n",
      "       [ 0.72868127,  0.24355921, -0.7722406 ],\n",
      "       [ 0.8456937 , -0.05408957,  0.00839588]], dtype=float32), array([ 0.07147466,  0.9143963 , -0.08587103], dtype=float32)]\n",
      " weight shape (8, 3) bias shape (3,)\n",
      "result weight for layer-0\n",
      "[[ 0.07147466  0.91439629 -0.08587103]\n",
      " [-0.17422649  0.74874806  0.2254784 ]\n",
      " [ 0.32756075 -0.7609455   0.3333849 ]\n",
      " [ 0.40776226  0.58775908 -0.39552128]\n",
      " [ 0.4676497   0.49711534  0.53523493]\n",
      " [-0.6509822   0.44330794  0.60767424]\n",
      " [-0.69625437 -0.32254839  0.71880263]\n",
      " [ 0.72868127  0.24355921 -0.77224058]\n",
      " [ 0.84569371 -0.05408957  0.00839588]]\n",
      "expected weight for layer-0\n",
      "[[ 0.12674605  0.9149538  -0.14169985]\n",
      " [-0.33551647  0.67700488  0.45851159]\n",
      " [ 0.48314436 -0.85241216  0.2692678 ]\n",
      " [ 0.3400255   0.57237542 -0.31240092]\n",
      " [ 0.31397716  0.46349737  0.72252547]\n",
      " [-0.69652442  0.4789189   0.61760552]\n",
      " [-0.50884515 -0.36354141  0.57238656]\n",
      " [ 0.41891295  0.26354517 -0.48245812]\n",
      " [ 0.90374164 -0.01759501 -0.08614663]]\n",
      "\n",
      "Layer 0 weights rmse 0.01687145185769127\n",
      "\n",
      "output \n",
      "[[4.4691959e-01 5.5053395e-01 2.5464627e-03]\n",
      " [4.9282787e-03 7.7309721e-04 9.9429858e-01]\n",
      " [3.9172384e-01 2.6277745e-01 3.4549874e-01]]\n",
      "expect [[0 1 0]\n",
      " [1 0 0]\n",
      " [0 0 1]]\n",
      "Testing result error 0.3368258640904659\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"AduAX63wdYAfSPRQglYjNP",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "### softmax_two_layer.json\n",
    "Testcase untuk softmax_two_layer.json"
   ],
   "attachments":{},
   "metadata":{
    "datalore":{
     "node_id":"0azdAgWjFzC80Bk28JBTUA",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "run_test_case_mode(f\"tc_b\/{test_cases[7]}\")"
   ],
   "execution_count":395,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "Running test case for tc_b\/softmax_two_layer.json\n",
      "\n",
      "\n",
      "Initial weight for layer 0\n",
      "[[ 0.1 -0.1  0.1 -0.1]\n",
      " [-0.1  0.1 -0.1  0.1]\n",
      " [ 0.1  0.1 -0.1 -0.1]]\n",
      "Initial weight for layer 1\n",
      "[[ 0.12 -0.1 ]\n",
      " [-0.12  0.1 ]\n",
      " [ 0.12 -0.1 ]\n",
      " [-0.12  0.1 ]\n",
      " [ 0.02  0.  ]]\n",
      "\/* Layer-1 (relu) *\/\n",
      "\/* Neuron-1 *\/\n",
      "Input Layer Neuron-1 -> Hidden Layer-1 Neuron-1 (0.1)\n",
      "\n",
      "\n",
      "\n",
      "\/* Neuron-2 *\/\n",
      "Input Layer Neuron-1 -> Hidden Layer-1 Neuron-2 (0.1)\n",
      "\n",
      "\n",
      "\n",
      "\/* Neuron-3 *\/\n",
      "Input Layer Neuron-1 -> Hidden Layer-1 Neuron-3 (-0.1)\n",
      "\n",
      "\n",
      "\n",
      "\/* Neuron-4 *\/\n",
      "Input Layer Neuron-1 -> Hidden Layer-1 Neuron-4 (-0.1)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\/* Layer-2 (softmax) *\/\n",
      "\/* Neuron-1 *\/\n",
      "Hidden Layer-1 Neuron-1 -> Hidden Layer-2 Neuron-1 (0.12)\n",
      "Hidden Layer-1 Neuron-2 -> Hidden Layer-2 Neuron-1 (-0.12)\n",
      "Hidden Layer-1 Neuron-3 -> Hidden Layer-2 Neuron-1 (0.02)\n",
      "\n",
      "\n",
      "\n",
      "\/* Neuron-2 *\/\n",
      "Hidden Layer-1 Neuron-1 -> Hidden Layer-2 Neuron-2 (-0.1)\n",
      "Hidden Layer-1 Neuron-2 -> Hidden Layer-2 Neuron-2 (0.1)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training data input 8 rows\n",
      "Begin training model\n",
      "\n",
      "\n",
      "Epoch 0\n",
      "training loss 0.6744156459068767\n",
      "\n",
      "\n",
      "Epoch 1\n",
      "training loss 0.6600297277600785\n",
      "\n",
      "\n",
      "Epoch 2\n",
      "training loss 0.648657406848746\n",
      "\n",
      "\n",
      "Epoch 3\n",
      "training loss 0.6389316746296829\n",
      "\n",
      "\n",
      "Epoch 4\n",
      "training loss 0.6303276583820985\n",
      "\n",
      "\n",
      "Epoch 5\n",
      "training loss 0.6225852916077184\n",
      "\n",
      "\n",
      "Epoch 6\n",
      "training loss 0.6154680499212197\n",
      "\n",
      "\n",
      "Epoch 7\n",
      "training loss 0.608768891906399\n",
      "\n",
      "\n",
      "Epoch 8\n",
      "training loss 0.6029324916693418\n",
      "\n",
      "\n",
      "Epoch 9\n",
      "training loss 0.5966674607692839\n",
      "\n",
      "\n",
      "Epoch 10\n",
      "training loss 0.58828894648307\n",
      "\n",
      "\n",
      "Epoch 11\n",
      "training loss 0.5799787082932448\n",
      "\n",
      "\n",
      "Epoch 12\n",
      "training loss 0.5717632609798453\n",
      "\n",
      "\n",
      "Epoch 13\n",
      "training loss 0.5636717639738775\n",
      "\n",
      "\n",
      "Epoch 14\n",
      "training loss 0.5557258800227651\n",
      "\n",
      "\n",
      "Epoch 15\n",
      "training loss 0.5479413358051736\n",
      "\n",
      "\n",
      "Epoch 16\n",
      "training loss 0.5403295557872627\n",
      "\n",
      "\n",
      "Epoch 17\n",
      "training loss 0.5328990950341945\n",
      "\n",
      "\n",
      "Epoch 18\n",
      "training loss 0.5256567872975949\n",
      "\n",
      "\n",
      "Epoch 19\n",
      "training loss 0.5186086029353579\n",
      "\n",
      "\n",
      "Epoch 20\n",
      "training loss 0.5117602443245917\n",
      "\n",
      "\n",
      "Epoch 21\n",
      "training loss 0.5051175199331562\n",
      "\n",
      "\n",
      "Epoch 22\n",
      "training loss 0.49868654195146367\n",
      "\n",
      "\n",
      "Epoch 23\n",
      "training loss 0.49244899589257785\n",
      "\n",
      "\n",
      "Epoch 24\n",
      "training loss 0.4864604445891571\n",
      "\n",
      "\n",
      "Epoch 25\n",
      "training loss 0.4807042065709005\n",
      "\n",
      "\n",
      "Epoch 26\n",
      "training loss 0.47503932123567744\n",
      "\n",
      "\n",
      "Epoch 27\n",
      "training loss 0.46937743400776183\n",
      "\n",
      "\n",
      "Epoch 28\n",
      "training loss 0.4639595865575167\n",
      "\n",
      "\n",
      "Epoch 29\n",
      "training loss 0.45879415392975886\n",
      "\n",
      "\n",
      "Epoch 30\n",
      "training loss 0.45389084721419143\n",
      "\n",
      "\n",
      "Epoch 31\n",
      "training loss 0.4492595532002612\n",
      "\n",
      "\n",
      "Epoch 32\n",
      "training loss 0.44491009396978975\n",
      "\n",
      "\n",
      "Epoch 33\n",
      "training loss 0.4408521309605686\n",
      "\n",
      "\n",
      "Epoch 34\n",
      "training loss 0.43709513904908176\n",
      "\n",
      "\n",
      "Epoch 35\n",
      "training loss 0.43364841440525725\n",
      "\n",
      "\n",
      "Epoch 36\n",
      "training loss 0.43052109734313526\n",
      "\n",
      "\n",
      "Epoch 37\n",
      "training loss 0.4277221993856758\n",
      "\n",
      "\n",
      "Epoch 38\n",
      "training loss 0.42526062739544557\n",
      "\n",
      "\n",
      "Epoch 39\n",
      "training loss 0.4231451992885852\n",
      "\n",
      "\n",
      "Epoch 40\n",
      "training loss 0.42138464665094616\n",
      "\n",
      "\n",
      "Epoch 41\n",
      "training loss 0.41998760001320723\n",
      "\n",
      "\n",
      "Epoch 42\n",
      "training loss 0.4189625528473532\n",
      "\n",
      "\n",
      "Epoch 43\n",
      "training loss 0.41831780063914675\n",
      "\n",
      "\n",
      "Epoch 44\n",
      "training loss 0.41806135174722536\n",
      "\n",
      "\n",
      "Epoch 45\n",
      "training loss 0.4182008072431564\n",
      "\n",
      "\n",
      "Epoch 46\n",
      "training loss 0.4187432075982267\n",
      "\n",
      "\n",
      "Epoch 47\n",
      "training loss 0.4196948449976935\n",
      "\n",
      "\n",
      "Epoch 48\n",
      "training loss 0.42106104126767463\n",
      "\n",
      "\n",
      "Epoch 49\n",
      "training loss 0.4228458929209955\n",
      "\n",
      "\n",
      "Epoch 50\n",
      "training loss 0.42505198666376626\n",
      "\n",
      "\n",
      "Epoch 51\n",
      "training loss 0.4276800908108591\n",
      "\n",
      "\n",
      "Epoch 52\n",
      "training loss 0.4307288303420542\n",
      "\n",
      "\n",
      "Epoch 53\n",
      "training loss 0.43419435564222425\n",
      "\n",
      "\n",
      "Epoch 54\n",
      "training loss 0.43807001710696286\n",
      "\n",
      "\n",
      "Epoch 55\n",
      "training loss 0.44234605951869344\n",
      "\n",
      "\n",
      "Epoch 56\n",
      "training loss 0.44700935115276685\n",
      "\n",
      "\n",
      "Epoch 57\n",
      "training loss 0.45204316272517836\n",
      "\n",
      "\n",
      "Epoch 58\n",
      "training loss 0.4574270103735923\n",
      "\n",
      "\n",
      "Epoch 59\n",
      "training loss 0.4631365748056484\n",
      "\n",
      "\n",
      "Epoch 60\n",
      "training loss 0.469143705620989\n",
      "\n",
      "\n",
      "Epoch 61\n",
      "training loss 0.4754165158279686\n",
      "\n",
      "\n",
      "Epoch 62\n",
      "training loss 0.4819195670728301\n",
      "\n",
      "\n",
      "Epoch 63\n",
      "training loss 0.48861414150588484\n",
      "\n",
      "\n",
      "Epoch 64\n",
      "training loss 0.4957189658767784\n",
      "\n",
      "\n",
      "Epoch 65\n",
      "training loss 0.5029937421841736\n",
      "\n",
      "\n",
      "Epoch 66\n",
      "training loss 0.5103349041447549\n",
      "\n",
      "\n",
      "Epoch 67\n",
      "training loss 0.5176979591114975\n",
      "\n",
      "\n",
      "Epoch 68\n",
      "training loss 0.5250378622971482\n",
      "\n",
      "\n",
      "Epoch 69\n",
      "training loss 0.5323093208611228\n",
      "\n",
      "\n",
      "Epoch 70\n",
      "training loss 0.5394672127009946\n",
      "\n",
      "\n",
      "Epoch 71\n",
      "training loss 0.5464670401077547\n",
      "\n",
      "\n",
      "Epoch 72\n",
      "training loss 0.5532653829401106\n",
      "\n",
      "\n",
      "Epoch 73\n",
      "training loss 0.559820334777893\n",
      "\n",
      "\n",
      "Epoch 74\n",
      "training loss 0.5660919143497984\n",
      "\n",
      "\n",
      "Epoch 75\n",
      "training loss 0.5720424490923867\n",
      "\n",
      "\n",
      "Epoch 76\n",
      "training loss 0.5776369301941908\n",
      "\n",
      "\n",
      "Epoch 77\n",
      "training loss 0.582843339813275\n",
      "\n",
      "\n",
      "Epoch 78\n",
      "training loss 0.587632951729231\n",
      "\n",
      "\n",
      "Epoch 79\n",
      "training loss 0.5919806067102754\n",
      "\n",
      "\n",
      "Epoch 80\n",
      "training loss 0.5958649634785881\n",
      "\n",
      "\n",
      "Epoch 81\n",
      "training loss 0.5992687254484668\n",
      "\n",
      "\n",
      "Epoch 82\n",
      "training loss 0.6021788424831163\n",
      "\n",
      "\n",
      "Epoch 83\n",
      "training loss 0.6045866858457912\n",
      "\n",
      "\n",
      "Epoch 84\n",
      "training loss 0.6064881933765718\n",
      "\n",
      "\n",
      "Epoch 85\n",
      "training loss 0.6078839807620703\n",
      "\n",
      "\n",
      "Epoch 86\n",
      "training loss 0.6087794136253373\n",
      "\n",
      "\n",
      "Epoch 87\n",
      "training loss 0.6094416361727668\n",
      "\n",
      "\n",
      "Epoch 88\n",
      "training loss 0.609715760566357\n",
      "\n",
      "\n",
      "Epoch 89\n",
      "training loss 0.6095714079394101\n",
      "\n",
      "\n",
      "Epoch 90\n",
      "training loss 0.609031707487486\n",
      "\n",
      "\n",
      "Epoch 91\n",
      "training loss 0.6081226998070282\n",
      "\n",
      "\n",
      "Epoch 92\n",
      "training loss 0.606872864803312\n",
      "\n",
      "\n",
      "Epoch 93\n",
      "training loss 0.60531266576736\n",
      "\n",
      "\n",
      "Epoch 94\n",
      "training loss 0.6034740701194073\n",
      "\n",
      "\n",
      "Epoch 95\n",
      "training loss 0.6013900467542643\n",
      "\n",
      "\n",
      "Epoch 96\n",
      "training loss 0.599094049614902\n",
      "\n",
      "\n",
      "Epoch 97\n",
      "training loss 0.596619498914613\n",
      "\n",
      "\n",
      "Epoch 98\n",
      "training loss 0.5939992713361318\n",
      "\n",
      "\n",
      "Epoch 99\n",
      "training loss 0.5912652102827949\n",
      "\n",
      "\n",
      "Epoch 100\n",
      "training loss 0.5884476672565396\n",
      "\n",
      "\n",
      "Epoch 101\n",
      "training loss 0.5855750856412503\n",
      "\n",
      "\n",
      "Epoch 102\n",
      "training loss 0.5826736383223494\n",
      "\n",
      "\n",
      "Epoch 103\n",
      "training loss 0.579766930318435\n",
      "\n",
      "\n",
      "Epoch 104\n",
      "training loss 0.5768757765676119\n",
      "\n",
      "\n",
      "Epoch 105\n",
      "training loss 0.5740180629119588\n",
      "\n",
      "\n",
      "Epoch 106\n",
      "training loss 0.5712086950468573\n",
      "\n",
      "\n",
      "Epoch 107\n",
      "training loss 0.5684596358779928\n",
      "\n",
      "\n",
      "Epoch 108\n",
      "training loss 0.5657800267409206\n",
      "\n",
      "\n",
      "Epoch 109\n",
      "training loss 0.563176382869128\n",
      "\n",
      "\n",
      "Epoch 110\n",
      "training loss 0.5606528490196455\n",
      "\n",
      "\n",
      "Epoch 111\n",
      "training loss 0.5582114979045305\n",
      "\n",
      "\n",
      "Epoch 112\n",
      "training loss 0.5558526524748473\n",
      "\n",
      "\n",
      "Epoch 113\n",
      "training loss 0.5535752133330898\n",
      "\n",
      "\n",
      "Epoch 114\n",
      "training loss 0.551376974486086\n",
      "\n",
      "\n",
      "Epoch 115\n",
      "training loss 0.5492549139148485\n",
      "\n",
      "\n",
      "Epoch 116\n",
      "training loss 0.5472054494904227\n",
      "\n",
      "\n",
      "Epoch 117\n",
      "training loss 0.5452246550189045\n",
      "\n",
      "\n",
      "Epoch 118\n",
      "training loss 0.5433084351289218\n",
      "\n",
      "\n",
      "Epoch 119\n",
      "training loss 0.5414526609337573\n",
      "\n",
      "\n",
      "Epoch 120\n",
      "training loss 0.5396532706947962\n",
      "\n",
      "\n",
      "Epoch 121\n",
      "training loss 0.5379063410402973\n",
      "\n",
      "\n",
      "Epoch 122\n",
      "training loss 0.5362081347475895\n",
      "\n",
      "\n",
      "Epoch 123\n",
      "training loss 0.5345551308602097\n",
      "\n",
      "\n",
      "Epoch 124\n",
      "training loss 0.5329440422047614\n",
      "\n",
      "\n",
      "Epoch 125\n",
      "training loss 0.5313718244120457\n",
      "\n",
      "\n",
      "Epoch 126\n",
      "training loss 0.5298356795168869\n",
      "\n",
      "\n",
      "Epoch 127\n",
      "training loss 0.528333056246803\n",
      "\n",
      "\n",
      "Epoch 128\n",
      "training loss 0.5268616482960149\n",
      "\n",
      "\n",
      "Epoch 129\n",
      "training loss 0.5254193912575206\n",
      "\n",
      "\n",
      "Epoch 130\n",
      "training loss 0.5240044584558283\n",
      "\n",
      "\n",
      "Epoch 131\n",
      "training loss 0.5226152556661683\n",
      "\n",
      "\n",
      "Epoch 132\n",
      "training loss 0.5212504145890672\n",
      "\n",
      "\n",
      "Epoch 133\n",
      "training loss 0.5199087849338871\n",
      "\n",
      "\n",
      "Epoch 134\n",
      "training loss 0.5185894250147258\n",
      "\n",
      "\n",
      "Epoch 135\n",
      "training loss 0.5172915908456635\n",
      "\n",
      "\n",
      "Epoch 136\n",
      "training loss 0.5160147238155329\n",
      "\n",
      "\n",
      "Epoch 137\n",
      "training loss 0.5147584371082796\n",
      "\n",
      "\n",
      "Epoch 138\n",
      "training loss 0.5135225011033977\n",
      "\n",
      "\n",
      "Epoch 139\n",
      "training loss 0.5123068280370777\n",
      "\n",
      "\n",
      "Epoch 140\n",
      "training loss 0.5111114562279443\n",
      "\n",
      "\n",
      "Epoch 141\n",
      "training loss 0.5099365341737082\n",
      "\n",
      "\n",
      "Epoch 142\n",
      "training loss 0.5087823048104254\n",
      "\n",
      "\n",
      "Epoch 143\n",
      "training loss 0.5076490901988151\n",
      "\n",
      "\n",
      "Epoch 144\n",
      "training loss 0.5065372768667252\n",
      "\n",
      "\n",
      "Epoch 145\n",
      "training loss 0.5054473019974486\n",
      "\n",
      "\n",
      "Epoch 146\n",
      "training loss 0.5043796406136102\n",
      "\n",
      "\n",
      "Epoch 147\n",
      "training loss 0.5033347938684418\n",
      "\n",
      "\n",
      "Epoch 148\n",
      "training loss 0.5023132785222765\n",
      "\n",
      "\n",
      "Epoch 149\n",
      "training loss 0.5013156176531756\n",
      "\n",
      "\n",
      "Epoch 150\n",
      "training loss 0.500342332627255\n",
      "\n",
      "\n",
      "Epoch 151\n",
      "training loss 0.4993939363364692\n",
      "\n",
      "\n",
      "Epoch 152\n",
      "training loss 0.4984709276989583\n",
      "\n",
      "\n",
      "Epoch 153\n",
      "training loss 0.49757378740888636\n",
      "\n",
      "\n",
      "Epoch 154\n",
      "training loss 0.49670297491814813\n",
      "\n",
      "\n",
      "Epoch 155\n",
      "training loss 0.4958589266304928\n",
      "\n",
      "\n",
      "Epoch 156\n",
      "training loss 0.49504205528855205\n",
      "\n",
      "\n",
      "Epoch 157\n",
      "training loss 0.49425275053510986\n",
      "\n",
      "\n",
      "Epoch 158\n",
      "training loss 0.49349138063086995\n",
      "\n",
      "\n",
      "Epoch 159\n",
      "training loss 0.4927582953112905\n",
      "\n",
      "\n",
      "Epoch 160\n",
      "training loss 0.4920538297641565\n",
      "\n",
      "\n",
      "Epoch 161\n",
      "training loss 0.49137830970698454\n",
      "\n",
      "\n",
      "Epoch 162\n",
      "training loss 0.49073205753875987\n",
      "\n",
      "\n",
      "Epoch 163\n",
      "training loss 0.4901153995336319\n",
      "\n",
      "\n",
      "Epoch 164\n",
      "training loss 0.4895286740349333\n",
      "\n",
      "\n",
      "Epoch 165\n",
      "training loss 0.4889722405961636\n",
      "\n",
      "\n",
      "Epoch 166\n",
      "training loss 0.48844649000146356\n",
      "\n",
      "\n",
      "Epoch 167\n",
      "training loss 0.48795185508171185\n",
      "\n",
      "\n",
      "Epoch 168\n",
      "training loss 0.4874888222238995\n",
      "\n",
      "\n",
      "Epoch 169\n",
      "training loss 0.48705794345116554\n",
      "\n",
      "\n",
      "Epoch 170\n",
      "training loss 0.48665984892915026\n",
      "\n",
      "\n",
      "Epoch 171\n",
      "training loss 0.48629525973158255\n",
      "\n",
      "\n",
      "Epoch 172\n",
      "training loss 0.4859650006747659\n",
      "\n",
      "\n",
      "Epoch 173\n",
      "training loss 0.48567001300748563\n",
      "\n",
      "\n",
      "Epoch 174\n",
      "training loss 0.4854113667205289\n",
      "\n",
      "\n",
      "Epoch 175\n",
      "training loss 0.4851902722192905\n",
      "\n",
      "\n",
      "Epoch 176\n",
      "training loss 0.48500809108477067\n",
      "\n",
      "\n",
      "Epoch 177\n",
      "training loss 0.48486634563367137\n",
      "\n",
      "\n",
      "Epoch 178\n",
      "training loss 0.4847667269784053\n",
      "\n",
      "\n",
      "Epoch 179\n",
      "training loss 0.48471110128387757\n",
      "\n",
      "\n",
      "Epoch 180\n",
      "training loss 0.48470151392116434\n",
      "\n",
      "\n",
      "Epoch 181\n",
      "training loss 0.48474019123002754\n",
      "\n",
      "\n",
      "Epoch 182\n",
      "training loss 0.4848295396238343\n",
      "\n",
      "\n",
      "Epoch 183\n",
      "training loss 0.48497214180311704\n",
      "\n",
      "\n",
      "Epoch 184\n",
      "training loss 0.48517074988864906\n",
      "\n",
      "\n",
      "Epoch 185\n",
      "training loss 0.4854282753422477\n",
      "\n",
      "\n",
      "Epoch 186\n",
      "training loss 0.48574777561373306\n",
      "\n",
      "\n",
      "Epoch 187\n",
      "training loss 0.48613243753523117\n",
      "\n",
      "\n",
      "Epoch 188\n",
      "training loss 0.4865855575781926\n",
      "\n",
      "\n",
      "Epoch 189\n",
      "training loss 0.4871105191921299\n",
      "\n",
      "\n",
      "Epoch 190\n",
      "training loss 0.48771076755420045\n",
      "\n",
      "\n",
      "Epoch 191\n",
      "training loss 0.4883897821713537\n",
      "\n",
      "\n",
      "Epoch 192\n",
      "training loss 0.4891510478868165\n",
      "\n",
      "\n",
      "Epoch 193\n",
      "training loss 0.4899980249442623\n",
      "\n",
      "\n",
      "Epoch 194\n",
      "training loss 0.49093411884948696\n",
      "\n",
      "\n",
      "Epoch 195\n",
      "training loss 0.4919626508339021\n",
      "\n",
      "\n",
      "Epoch 196\n",
      "training loss 0.4930868297598935\n",
      "\n",
      "\n",
      "Epoch 197\n",
      "training loss 0.494309726309121\n",
      "\n",
      "\n",
      "Epoch 198\n",
      "training loss 0.49563425025662095\n",
      "\n",
      "\n",
      "Epoch 199\n",
      "training loss 0.49706313155359294\n",
      "\n",
      "\n",
      "\n",
      "Stop reason by max_iteration expected error_threshold\n",
      "\n",
      "result weight for layer-0\n",
      "[[ 0.68652749 -0.12625552  1.13161446 -0.13518047]\n",
      " [-1.50367245  0.01070289 -0.60800099  0.05372455]\n",
      " [ 0.671383    0.0172951  -1.36224079 -0.02986033]]\n",
      "expected weight for layer-0\n",
      "[[-0.28730211 -0.28822282 -0.70597451  0.42094471]\n",
      " [-0.5790794  -1.1836444  -1.34287961  0.69575311]\n",
      " [-0.41434377  1.51314676 -0.97649086 -1.3043465 ]]\n",
      "abs weight difference for layer-0\n",
      "[[0.9738296  0.1619673  1.83758897 0.55612518]\n",
      " [0.92459305 1.19434729 0.73487862 0.64202856]\n",
      " [1.08572677 1.49585166 0.38574993 1.27448617]]\n",
      "\n",
      "Layer 0 weights rmse 1.0903053148126756\n",
      "\n",
      "result weight for layer-1\n",
      "[[ 0.10216723 -0.08216723]\n",
      " [-0.08353376  0.06353376]\n",
      " [-0.1878645   0.2078645 ]\n",
      " [-0.31672702  0.29672702]\n",
      " [ 0.21641153 -0.19641153]]\n",
      "expected weight for layer-1\n",
      "[[-1.72078607  1.74078607]\n",
      " [-0.50352956  0.48352956]\n",
      " [ 1.25764816 -1.23764816]\n",
      " [-1.16998784  1.14998784]\n",
      " [ 1.0907634  -1.0707634 ]]\n",
      "abs weight difference for layer-1\n",
      "[[1.8229533  1.8229533 ]\n",
      " [0.4199958  0.4199958 ]\n",
      " [1.44551266 1.44551266]\n",
      " [0.85326082 0.85326082]\n",
      " [0.87435187 0.87435187]]\n",
      "\n",
      "Layer 1 weights rmse 1.4163214584858317\n",
      "\n",
      "output [array([0.54595357, 0.45404643]), array([0.41335576, 0.58664424]), array([0.09957747, 0.90042253]), array([0.35537499, 0.64462501]), array([0.25164749, 0.74835251]), array([0.05557216, 0.94442784]), array([0.1188907, 0.8811093]), array([0.54595357, 0.45404643])]\n",
      "expect [[0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]]\n",
      "Testing result error 0.33815156183935957\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"JSZkbKr2aa5DYczsskHAOQ",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "run_test_case_reference_mode(f\"tc_b\/{test_cases[7]}\")"
   ],
   "execution_count":396,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "Running test case for tc_b\/softmax_two_layer.json\n",
      "\n",
      "\n",
      "Epoch 1\/200\n",
      "\r1\/8 [==>...........................] - ETA: 17s - loss: 0.2077\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r6\/8 [=====================>........] - ETA: 0s - loss: 0.2396 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 3s 12ms\/step - loss: 0.2682\n",
      "Epoch 2\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.2201\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r5\/8 [=================>............] - ETA: 0s - loss: 0.2398\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r7\/8 [=========================>....] - ETA: 0s - loss: 0.2592\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 23ms\/step - loss: 0.2596\n",
      "Epoch 3\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.2264\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - ETA: 0s - loss: 0.2554\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 14ms\/step - loss: 0.2554\n",
      "Epoch 4\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.2258\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r4\/8 [==============>...............] - ETA: 0s - loss: 0.2539\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r6\/8 [=====================>........] - ETA: 0s - loss: 0.2512\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - ETA: 0s - loss: 0.2516\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 43ms\/step - loss: 0.2516\n",
      "Epoch 5\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.2193\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r3\/8 [==========>...................] - ETA: 0s - loss: 0.2466\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r5\/8 [=================>............] - ETA: 0s - loss: 0.2593\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r7\/8 [=========================>....] - ETA: 0s - loss: 0.2496\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 31ms\/step - loss: 0.2499\n",
      "Epoch 6\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.2510\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r4\/8 [==============>...............] - ETA: 0s - loss: 0.2291\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r5\/8 [=================>............] - ETA: 0s - loss: 0.2225\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 20ms\/step - loss: 0.2471\n",
      "Epoch 7\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.2771\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - ETA: 0s - loss: 0.2438\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 9ms\/step - loss: 0.2438\n",
      "Epoch 8\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.2809\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r4\/8 [==============>...............] - ETA: 0s - loss: 0.2440\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 14ms\/step - loss: 0.2405\n",
      "Epoch 9\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.1977\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r3\/8 [==========>...................] - ETA: 0s - loss: 0.2007\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - ETA: 0s - loss: 0.2365\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 17ms\/step - loss: 0.2365\n",
      "Epoch 10\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.2384\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - ETA: 0s - loss: 0.2336\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 9ms\/step - loss: 0.2336\n",
      "Epoch 11\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.2385\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r6\/8 [=====================>........] - ETA: 0s - loss: 0.2262\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 12ms\/step - loss: 0.2271\n",
      "Epoch 12\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.1602\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r4\/8 [==============>...............] - ETA: 0s - loss: 0.2267\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 14ms\/step - loss: 0.2179\n",
      "Epoch 13\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.1419\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - ETA: 0s - loss: 0.2158\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 9ms\/step - loss: 0.2158\n",
      "Epoch 14\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.2237\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r6\/8 [=====================>........] - ETA: 0s - loss: 0.1990\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 11ms\/step - loss: 0.2048\n",
      "Epoch 15\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.2626\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r6\/8 [=====================>........] - ETA: 0s - loss: 0.2104\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 12ms\/step - loss: 0.1893\n",
      "Epoch 16\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.2839\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r7\/8 [=========================>....] - ETA: 0s - loss: 0.1755\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 10ms\/step - loss: 0.1883\n",
      "Epoch 17\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0725\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r7\/8 [=========================>....] - ETA: 0s - loss: 0.1873\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 9ms\/step - loss: 0.1748\n",
      "Epoch 18\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0919\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r6\/8 [=====================>........] - ETA: 0s - loss: 0.1591\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 14ms\/step - loss: 0.1598\n",
      "Epoch 19\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.2405\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r7\/8 [=========================>....] - ETA: 0s - loss: 0.1536\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 11ms\/step - loss: 0.1474\n",
      "Epoch 20\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.2357\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 6ms\/step - loss: 0.1403\n",
      "Epoch 21\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0979\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r7\/8 [=========================>....] - ETA: 0s - loss: 0.1389\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 9ms\/step - loss: 0.1302\n",
      "Epoch 22\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.2233\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - ETA: 0s - loss: 0.1282\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 9ms\/step - loss: 0.1282\n",
      "Epoch 23\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0788\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r4\/8 [==============>...............] - ETA: 0s - loss: 0.0770\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 11ms\/step - loss: 0.1153\n",
      "Epoch 24\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.1879\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r6\/8 [=====================>........] - ETA: 0s - loss: 0.1057\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 14ms\/step - loss: 0.1061\n",
      "Epoch 25\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0787\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r2\/8 [======>.......................] - ETA: 0s - loss: 0.0427\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r4\/8 [==============>...............] - ETA: 0s - loss: 0.1123\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 25ms\/step - loss: 0.1059\n",
      "Epoch 26\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0540\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r4\/8 [==============>...............] - ETA: 0s - loss: 0.0900\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r6\/8 [=====================>........] - ETA: 0s - loss: 0.0860\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 26ms\/step - loss: 0.0933\n",
      "Epoch 27\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0364\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r2\/8 [======>.......................] - ETA: 0s - loss: 0.0264\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r5\/8 [=================>............] - ETA: 0s - loss: 0.0630\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 26ms\/step - loss: 0.0877\n",
      "Epoch 28\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0095\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - ETA: 0s - loss: 0.0815\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 9ms\/step - loss: 0.0815\n",
      "Epoch 29\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.1498\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r4\/8 [==============>...............] - ETA: 0s - loss: 0.0696\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 11ms\/step - loss: 0.0717\n",
      "Epoch 30\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0043\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - ETA: 0s - loss: 0.0721\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 17ms\/step - loss: 0.0721\n",
      "Epoch 31\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0572\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r2\/8 [======>.......................] - ETA: 0s - loss: 0.0645\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r6\/8 [=====================>........] - ETA: 0s - loss: 0.0613\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 25ms\/step - loss: 0.0626\n",
      "Epoch 32\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0626\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r3\/8 [==========>...................] - ETA: 0s - loss: 0.0327\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - ETA: 0s - loss: 0.0593\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 17ms\/step - loss: 0.0593\n",
      "Epoch 33\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 8.8446e-04\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r7\/8 [=========================>....] - ETA: 0s - loss: 0.0436    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 9ms\/step - loss: 0.0528\n",
      "Epoch 34\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0100\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r6\/8 [=====================>........] - ETA: 0s - loss: 0.0481\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 14ms\/step - loss: 0.0504\n",
      "Epoch 35\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0231\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r6\/8 [=====================>........] - ETA: 0s - loss: 0.0527\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 19ms\/step - loss: 0.0456\n",
      "Epoch 36\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0353\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r5\/8 [=================>............] - ETA: 0s - loss: 0.0517\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 14ms\/step - loss: 0.0420\n",
      "Epoch 37\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0246\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r7\/8 [=========================>....] - ETA: 0s - loss: 0.0422\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 9ms\/step - loss: 0.0404\n",
      "Epoch 38\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0026\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - ETA: 0s - loss: 0.0374\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 10ms\/step - loss: 0.0374\n",
      "Epoch 39\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0273\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r7\/8 [=========================>....] - ETA: 0s - loss: 0.0291\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 9ms\/step - loss: 0.0359\n",
      "Epoch 40\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0184\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r7\/8 [=========================>....] - ETA: 0s - loss: 0.0335\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 8ms\/step - loss: 0.0332\n",
      "Epoch 41\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0224\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r7\/8 [=========================>....] - ETA: 0s - loss: 0.0245\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 9ms\/step - loss: 0.0310\n",
      "Epoch 42\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0130\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r7\/8 [=========================>....] - ETA: 0s - loss: 0.0228\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 11ms\/step - loss: 0.0290\n",
      "Epoch 43\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0199\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 6ms\/step - loss: 0.0275\n",
      "Epoch 44\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0085\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r5\/8 [=================>............] - ETA: 0s - loss: 0.0178\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 11ms\/step - loss: 0.0256\n",
      "Epoch 45\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0174\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r4\/8 [==============>...............] - ETA: 0s - loss: 0.0253\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - ETA: 0s - loss: 0.0242\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 20ms\/step - loss: 0.0242\n",
      "Epoch 46\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0065\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r4\/8 [==============>...............] - ETA: 0s - loss: 0.0082\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - ETA: 0s - loss: 0.0231\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 22ms\/step - loss: 0.0231\n",
      "Epoch 47\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0145\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r7\/8 [=========================>....] - ETA: 0s - loss: 0.0212\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 15ms\/step - loss: 0.0219\n",
      "Epoch 48\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0155\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r6\/8 [=====================>........] - ETA: 0s - loss: 0.0253\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 17ms\/step - loss: 0.0207\n",
      "Epoch 49\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0619\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r4\/8 [==============>...............] - ETA: 0s - loss: 0.0227\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - ETA: 0s - loss: 0.0215\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 20ms\/step - loss: 0.0215\n",
      "Epoch 50\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0067\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r4\/8 [==============>...............] - ETA: 0s - loss: 0.0074\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 20ms\/step - loss: 0.0196\n",
      "Epoch 51\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0486\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r6\/8 [=====================>........] - ETA: 0s - loss: 0.0149\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 15ms\/step - loss: 0.0187\n",
      "Epoch 52\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0146\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r6\/8 [=====================>........] - ETA: 0s - loss: 0.0206\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 11ms\/step - loss: 0.0177\n",
      "Epoch 53\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0171\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r5\/8 [=================>............] - ETA: 0s - loss: 0.0161\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 13ms\/step - loss: 0.0170\n",
      "Epoch 54\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0095\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r6\/8 [=====================>........] - ETA: 0s - loss: 0.0174\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 14ms\/step - loss: 0.0164\n",
      "Epoch 55\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0416\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r4\/8 [==============>...............] - ETA: 0s - loss: 0.0137\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 16ms\/step - loss: 0.0156\n",
      "Epoch 56\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 3.9532e-04\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r3\/8 [==========>...................] - ETA: 0s - loss: 0.0044    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - ETA: 0s - loss: 0.0150\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 20ms\/step - loss: 0.0150\n",
      "Epoch 57\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 3.1338e-05\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r4\/8 [==============>...............] - ETA: 0s - loss: 0.0118    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 12ms\/step - loss: 0.0145\n",
      "Epoch 58\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0376\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r4\/8 [==============>...............] - ETA: 0s - loss: 0.0137\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 17ms\/step - loss: 0.0139\n",
      "Epoch 59\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0364\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - ETA: 0s - loss: 0.0134\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 12ms\/step - loss: 0.0134\n",
      "Epoch 60\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0078\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r4\/8 [==============>...............] - ETA: 0s - loss: 0.0194\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 14ms\/step - loss: 0.0130\n",
      "Epoch 61\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0071\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r7\/8 [=========================>....] - ETA: 0s - loss: 0.0127\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 11ms\/step - loss: 0.0126\n",
      "Epoch 62\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0099\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 6ms\/step - loss: 0.0125\n",
      "Epoch 63\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 3.0403e-04\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r4\/8 [==============>...............] - ETA: 0s - loss: 0.0179    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - ETA: 0s - loss: 0.0120\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 34ms\/step - loss: 0.0120\n",
      "Epoch 64\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 1.7561e-05\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - ETA: 0s - loss: 0.0116    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 7ms\/step - loss: 0.0116\n",
      "Epoch 65\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0073\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r6\/8 [=====================>........] - ETA: 0s - loss: 0.0127\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - ETA: 0s - loss: 0.0112\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 30ms\/step - loss: 0.0112\n",
      "Epoch 66\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 2.4120e-04\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r3\/8 [==========>...................] - ETA: 0s - loss: 0.0112    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r5\/8 [=================>............] - ETA: 0s - loss: 0.0080\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r7\/8 [=========================>....] - ETA: 0s - loss: 0.0107\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 32ms\/step - loss: 0.0109\n",
      "Epoch 67\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0285\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r3\/8 [==========>...................] - ETA: 0s - loss: 0.0096\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r5\/8 [=================>............] - ETA: 0s - loss: 0.0127\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - ETA: 0s - loss: 0.0105\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 25ms\/step - loss: 0.0105\n",
      "Epoch 68\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0277\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r5\/8 [=================>............] - ETA: 0s - loss: 0.0097\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 15ms\/step - loss: 0.0102\n",
      "Epoch 69\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0270\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r2\/8 [======>.......................] - ETA: 0s - loss: 0.0268\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r5\/8 [=================>............] - ETA: 0s - loss: 0.0134\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r7\/8 [=========================>....] - ETA: 0s - loss: 0.0104\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 31ms\/step - loss: 0.0099\n",
      "Epoch 70\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0263\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r7\/8 [=========================>....] - ETA: 0s - loss: 0.0110\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - ETA: 0s - loss: 0.0096\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 20ms\/step - loss: 0.0096\n",
      "Epoch 71\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 1.0401e-05\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r2\/8 [======>.......................] - ETA: 0s - loss: 0.0029    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r3\/8 [==========>...................] - ETA: 0s - loss: 0.0020\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r6\/8 [=====================>........] - ETA: 0s - loss: 0.0099\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 32ms\/step - loss: 0.0093\n",
      "Epoch 72\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0250\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r4\/8 [==============>...............] - ETA: 0s - loss: 0.0137\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r7\/8 [=========================>....] - ETA: 0s - loss: 0.0095\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 20ms\/step - loss: 0.0091\n",
      "Epoch 73\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 9.2731e-06\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r6\/8 [=====================>........] - ETA: 0s - loss: 0.0113    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 18ms\/step - loss: 0.0089\n",
      "Epoch 74\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0027\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 7ms\/step - loss: 0.0086\n",
      "Epoch 75\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0232\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r6\/8 [=====================>........] - ETA: 0s - loss: 0.0103\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 11ms\/step - loss: 0.0085\n",
      "Epoch 76\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0033\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 6ms\/step - loss: 0.0084\n",
      "Epoch 77\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0221\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r6\/8 [=====================>........] - ETA: 0s - loss: 0.0099\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 11ms\/step - loss: 0.0081\n",
      "Epoch 78\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0074\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r4\/8 [==============>...............] - ETA: 0s - loss: 0.0040\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 17ms\/step - loss: 0.0079\n",
      "Epoch 79\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0211\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r6\/8 [=====================>........] - ETA: 0s - loss: 0.0096\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 12ms\/step - loss: 0.0077\n",
      "Epoch 80\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 1.3309e-04\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r6\/8 [=====================>........] - ETA: 0s - loss: 0.0052    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 14ms\/step - loss: 0.0075\n",
      "Epoch 81\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 1.2407e-04\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r7\/8 [=========================>....] - ETA: 0s - loss: 0.0055    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 11ms\/step - loss: 0.0074\n",
      "Epoch 82\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0068\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r5\/8 [=================>............] - ETA: 0s - loss: 0.0067\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r6\/8 [=====================>........] - ETA: 0s - loss: 0.0063\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 26ms\/step - loss: 0.0072\n",
      "Epoch 83\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0069\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r4\/8 [==============>...............] - ETA: 0s - loss: 0.0035\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 15ms\/step - loss: 0.0070\n",
      "Epoch 84\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 5.6660e-06\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 8ms\/step - loss: 0.0069\n",
      "Epoch 85\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0019\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 6ms\/step - loss: 0.0068\n",
      "Epoch 86\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 9.9623e-05\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r7\/8 [=========================>....] - ETA: 0s - loss: 0.0070    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 11ms\/step - loss: 0.0066\n",
      "Epoch 87\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 9.4216e-05\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r7\/8 [=========================>....] - ETA: 0s - loss: 0.0074    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 11ms\/step - loss: 0.0065\n",
      "Epoch 88\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0036\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 6ms\/step - loss: 0.0063\n",
      "Epoch 89\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 8.4486e-05\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r7\/8 [=========================>....] - ETA: 0s - loss: 0.0067    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 9ms\/step - loss: 0.0063\n",
      "Epoch 90\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0170\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r6\/8 [=====================>........] - ETA: 0s - loss: 0.0044\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 16ms\/step - loss: 0.0062\n",
      "Epoch 91\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0167\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r4\/8 [==============>...............] - ETA: 0s - loss: 0.0091\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 13ms\/step - loss: 0.0061\n",
      "Epoch 92\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 8.7896e-05\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - ETA: 0s - loss: 0.0059    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 11ms\/step - loss: 0.0059\n",
      "Epoch 93\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 3.7455e-06\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r7\/8 [=========================>....] - ETA: 0s - loss: 0.0057    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 9ms\/step - loss: 0.0058\n",
      "Epoch 94\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 8.1638e-05\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 6ms\/step - loss: 0.0057\n",
      "Epoch 95\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 3.4371e-06\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r6\/8 [=====================>........] - ETA: 0s - loss: 0.0023    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 11ms\/step - loss: 0.0056\n",
      "Epoch 96\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0152\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 7ms\/step - loss: 0.0055\n",
      "Epoch 97\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0150\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r6\/8 [=====================>........] - ETA: 0s - loss: 0.0037\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 11ms\/step - loss: 0.0054\n",
      "Epoch 98\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0017\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - ETA: 0s - loss: 0.0053\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 9ms\/step - loss: 0.0053\n",
      "Epoch 99\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0145\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r4\/8 [==============>...............] - ETA: 0s - loss: 0.0053\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 14ms\/step - loss: 0.0052\n",
      "Epoch 100\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0143\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r7\/8 [=========================>....] - ETA: 0s - loss: 0.0054\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 11ms\/step - loss: 0.0051\n",
      "Epoch 101\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 2.5737e-06\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 6ms\/step - loss: 0.0050\n",
      "Epoch 102\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0139\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 6ms\/step - loss: 0.0049\n",
      "Epoch 103\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0034\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r6\/8 [=====================>........] - ETA: 0s - loss: 0.0043\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 12ms\/step - loss: 0.0049\n",
      "Epoch 104\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0041\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r7\/8 [=========================>....] - ETA: 0s - loss: 0.0051\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 17ms\/step - loss: 0.0049\n",
      "Epoch 105\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0132\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r3\/8 [==========>...................] - ETA: 0s - loss: 0.0066\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r5\/8 [=================>............] - ETA: 0s - loss: 0.0050\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r7\/8 [=========================>....] - ETA: 0s - loss: 0.0036\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 40ms\/step - loss: 0.0048\n",
      "Epoch 106\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 2.2887e-06\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r3\/8 [==========>...................] - ETA: 0s - loss: 0.0017    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r5\/8 [=================>............] - ETA: 0s - loss: 0.0013\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r7\/8 [=========================>....] - ETA: 0s - loss: 0.0046\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 40ms\/step - loss: 0.0047\n",
      "Epoch 107\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 2.1785e-06\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r4\/8 [==============>...............] - ETA: 0s - loss: 0.0048    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r6\/8 [=====================>........] - ETA: 0s - loss: 0.0059\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - ETA: 0s - loss: 0.0046\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 31ms\/step - loss: 0.0046\n",
      "Epoch 108\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0043\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r3\/8 [==========>...................] - ETA: 0s - loss: 0.0062\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r5\/8 [=================>............] - ETA: 0s - loss: 0.0043\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r7\/8 [=========================>....] - ETA: 0s - loss: 0.0034\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 40ms\/step - loss: 0.0045\n",
      "Epoch 109\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0014\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r3\/8 [==========>...................] - ETA: 0s - loss: 0.0046\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r5\/8 [=================>............] - ETA: 0s - loss: 0.0042\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r7\/8 [=========================>....] - ETA: 0s - loss: 0.0048\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 38ms\/step - loss: 0.0045\n",
      "Epoch 110\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0123\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r4\/8 [==============>...............] - ETA: 0s - loss: 0.0046\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 15ms\/step - loss: 0.0044\n",
      "Epoch 111\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0121\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r7\/8 [=========================>....] - ETA: 0s - loss: 0.0047\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 12ms\/step - loss: 0.0043\n",
      "Epoch 112\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 1.8170e-06\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r7\/8 [=========================>....] - ETA: 0s - loss: 0.0045    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 10ms\/step - loss: 0.0043\n",
      "Epoch 113\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 1.7536e-06\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r6\/8 [=====================>........] - ETA: 0s - loss: 0.0034    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - ETA: 0s - loss: 0.0042\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 16ms\/step - loss: 0.0042\n",
      "Epoch 114\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 1.7040e-06\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - ETA: 0s - loss: 0.0042    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 9ms\/step - loss: 0.0042\n",
      "Epoch 115\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0115\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r7\/8 [=========================>....] - ETA: 0s - loss: 0.0030\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 11ms\/step - loss: 0.0041\n",
      "Epoch 116\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0113\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r7\/8 [=========================>....] - ETA: 0s - loss: 0.0030\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 9ms\/step - loss: 0.0040\n",
      "Epoch 117\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 4.0522e-05\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r6\/8 [=====================>........] - ETA: 0s - loss: 0.0047    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 11ms\/step - loss: 0.0040\n",
      "Epoch 118\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0019\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - ETA: 0s - loss: 0.0040\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 9ms\/step - loss: 0.0040\n",
      "Epoch 119\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0109\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r7\/8 [=========================>....] - ETA: 0s - loss: 0.0039\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 11ms\/step - loss: 0.0039\n",
      "Epoch 120\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0108\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - ETA: 0s - loss: 0.0039\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 9ms\/step - loss: 0.0039\n",
      "Epoch 121\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0106\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r7\/8 [=========================>....] - ETA: 0s - loss: 0.0043\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 9ms\/step - loss: 0.0038\n",
      "Epoch 122\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 4.0060e-05\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r4\/8 [==============>...............] - ETA: 0s - loss: 0.0035    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 17ms\/step - loss: 0.0038\n",
      "Epoch 123\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0104\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r3\/8 [==========>...................] - ETA: 0s - loss: 0.0044\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r6\/8 [=====================>........] - ETA: 0s - loss: 0.0045\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 19ms\/step - loss: 0.0037\n",
      "Epoch 124\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0034\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - ETA: 0s - loss: 0.0037\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 9ms\/step - loss: 0.0037\n",
      "Epoch 125\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0012\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r7\/8 [=========================>....] - ETA: 0s - loss: 0.0041\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 11ms\/step - loss: 0.0036\n",
      "Epoch 126\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 3.5551e-05\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r7\/8 [=========================>....] - ETA: 0s - loss: 0.0038    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 9ms\/step - loss: 0.0036\n",
      "Epoch 127\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0011\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r7\/8 [=========================>....] - ETA: 0s - loss: 0.0035\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 11ms\/step - loss: 0.0035\n",
      "Epoch 128\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0098\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r6\/8 [=====================>........] - ETA: 0s - loss: 0.0030\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 12ms\/step - loss: 0.0035\n",
      "Epoch 129\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0016\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r7\/8 [=========================>....] - ETA: 0s - loss: 0.0039\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 14ms\/step - loss: 0.0034\n",
      "Epoch 130\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0016\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r6\/8 [=====================>........] - ETA: 0s - loss: 0.0039\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 14ms\/step - loss: 0.0034\n",
      "Epoch 131\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0020\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r7\/8 [=========================>....] - ETA: 0s - loss: 0.0037\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 9ms\/step - loss: 0.0033\n",
      "Epoch 132\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0030\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r6\/8 [=====================>........] - ETA: 0s - loss: 0.0041\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 11ms\/step - loss: 0.0033\n",
      "Epoch 133\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0019\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 6ms\/step - loss: 0.0034\n",
      "Epoch 134\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 3.1522e-05\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r5\/8 [=================>............] - ETA: 0s - loss: 0.0025    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 11ms\/step - loss: 0.0033\n",
      "Epoch 135\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0028\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r6\/8 [=====================>........] - ETA: 0s - loss: 0.0041\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 9ms\/step - loss: 0.0032\n",
      "Epoch 136\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0029\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r5\/8 [=================>............] - ETA: 0s - loss: 0.0030\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 11ms\/step - loss: 0.0032\n",
      "Epoch 137\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0011\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r6\/8 [=====================>........] - ETA: 0s - loss: 0.0022\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 12ms\/step - loss: 0.0032\n",
      "Epoch 138\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0014\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r6\/8 [=====================>........] - ETA: 0s - loss: 0.0040\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 14ms\/step - loss: 0.0031\n",
      "Epoch 139\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0019\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r7\/8 [=========================>....] - ETA: 0s - loss: 0.0023\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 9ms\/step - loss: 0.0031\n",
      "Epoch 140\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0028\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r4\/8 [==============>...............] - ETA: 0s - loss: 0.0050\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r7\/8 [=========================>....] - ETA: 0s - loss: 0.0035\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 20ms\/step - loss: 0.0030\n",
      "Epoch 141\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 8.1611e-07\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r4\/8 [==============>...............] - ETA: 0s - loss: 0.0027    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r7\/8 [=========================>....] - ETA: 0s - loss: 0.0034\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 20ms\/step - loss: 0.0030\n",
      "Epoch 142\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0028\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r4\/8 [==============>...............] - ETA: 0s - loss: 0.0031\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r7\/8 [=========================>....] - ETA: 0s - loss: 0.0031\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 20ms\/step - loss: 0.0030\n",
      "Epoch 143\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 2.5856e-05\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r4\/8 [==============>...............] - ETA: 0s - loss: 0.0030    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r7\/8 [=========================>....] - ETA: 0s - loss: 0.0031\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 20ms\/step - loss: 0.0029\n",
      "Epoch 144\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0013\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r4\/8 [==============>...............] - ETA: 0s - loss: 0.0030\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r7\/8 [=========================>....] - ETA: 0s - loss: 0.0033\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 23ms\/step - loss: 0.0029\n",
      "Epoch 145\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 7.3358e-07\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r4\/8 [==============>...............] - ETA: 0s - loss: 0.0027    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r6\/8 [=====================>........] - ETA: 0s - loss: 0.0031\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 23ms\/step - loss: 0.0029\n",
      "Epoch 146\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0017\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r6\/8 [=====================>........] - ETA: 0s - loss: 0.0023\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - ETA: 0s - loss: 0.0029\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 26ms\/step - loss: 0.0029\n",
      "Epoch 147\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 8.6997e-04\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r3\/8 [==========>...................] - ETA: 0s - loss: 8.2287e-04\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r7\/8 [=========================>....] - ETA: 0s - loss: 0.0033    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 16ms\/step - loss: 0.0029\n",
      "Epoch 148\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0013\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r7\/8 [=========================>....] - ETA: 0s - loss: 0.0021\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 9ms\/step - loss: 0.0028\n",
      "Epoch 149\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 2.4582e-05\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r7\/8 [=========================>....] - ETA: 0s - loss: 0.0029    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 9ms\/step - loss: 0.0028\n",
      "Epoch 150\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0017\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r4\/8 [==============>...............] - ETA: 0s - loss: 0.0043\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 15ms\/step - loss: 0.0028\n",
      "Epoch 151\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0012\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r6\/8 [=====================>........] - ETA: 0s - loss: 0.0029\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 11ms\/step - loss: 0.0027\n",
      "Epoch 152\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 8.9672e-04\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r6\/8 [=====================>........] - ETA: 0s - loss: 0.0033    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 12ms\/step - loss: 0.0027\n",
      "Epoch 153\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 8.7608e-04\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r7\/8 [=========================>....] - ETA: 0s - loss: 0.0028    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 12ms\/step - loss: 0.0027\n",
      "Epoch 154\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 6.1319e-07\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r7\/8 [=========================>....] - ETA: 0s - loss: 0.0029    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 11ms\/step - loss: 0.0027\n",
      "Epoch 155\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0016\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - ETA: 0s - loss: 0.0026\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 11ms\/step - loss: 0.0026\n",
      "Epoch 156\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0073\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r3\/8 [==========>...................] - ETA: 0s - loss: 0.0053\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r7\/8 [=========================>....] - ETA: 0s - loss: 0.0027\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 23ms\/step - loss: 0.0026\n",
      "Epoch 157\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 8.1869e-04\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r7\/8 [=========================>....] - ETA: 0s - loss: 0.0026    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 11ms\/step - loss: 0.0026\n",
      "Epoch 158\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0024\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r6\/8 [=====================>........] - ETA: 0s - loss: 0.0022\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - ETA: 0s - loss: 0.0026\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 20ms\/step - loss: 0.0026\n",
      "Epoch 159\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0024\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r6\/8 [=====================>........] - ETA: 0s - loss: 0.0022\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 10ms\/step - loss: 0.0025\n",
      "Epoch 160\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0071\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r7\/8 [=========================>....] - ETA: 0s - loss: 0.0025\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 9ms\/step - loss: 0.0025\n",
      "Epoch 161\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 1.8842e-05\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r4\/8 [==============>...............] - ETA: 0s - loss: 0.0022    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - ETA: 0s - loss: 0.0025\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 18ms\/step - loss: 0.0025\n",
      "Epoch 162\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 2.0157e-05\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r6\/8 [=====================>........] - ETA: 0s - loss: 0.0029    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 14ms\/step - loss: 0.0025\n",
      "Epoch 163\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0011\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r5\/8 [=================>............] - ETA: 0s - loss: 0.0012\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 11ms\/step - loss: 0.0025\n",
      "Epoch 164\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0022\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r4\/8 [==============>...............] - ETA: 0s - loss: 0.0027\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r6\/8 [=====================>........] - ETA: 0s - loss: 0.0029\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - ETA: 0s - loss: 0.0024\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 32ms\/step - loss: 0.0024\n",
      "Epoch 165\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0068\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r3\/8 [==========>...................] - ETA: 0s - loss: 0.0030\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - ETA: 0s - loss: 0.0024\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 20ms\/step - loss: 0.0024\n",
      "Epoch 166\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 1.8786e-05\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r4\/8 [==============>...............] - ETA: 0s - loss: 0.0019    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r6\/8 [=====================>........] - ETA: 0s - loss: 0.0025\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - ETA: 0s - loss: 0.0024\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 26ms\/step - loss: 0.0024\n",
      "Epoch 167\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0010\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r2\/8 [======>.......................] - ETA: 0s - loss: 5.3144e-04\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r4\/8 [==============>...............] - ETA: 0s - loss: 0.0021    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 26ms\/step - loss: 0.0024\n",
      "Epoch 168\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0066\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r4\/8 [==============>...............] - ETA: 0s - loss: 0.0037\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - ETA: 0s - loss: 0.0023\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 17ms\/step - loss: 0.0023\n",
      "Epoch 169\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0066\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r4\/8 [==============>...............] - ETA: 0s - loss: 0.0024\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 14ms\/step - loss: 0.0023\n",
      "Epoch 170\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0065\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r5\/8 [=================>............] - ETA: 0s - loss: 0.0032\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 12ms\/step - loss: 0.0023\n",
      "Epoch 171\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 4.2962e-07\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r5\/8 [=================>............] - ETA: 0s - loss: 0.0029    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 14ms\/step - loss: 0.0023\n",
      "Epoch 172\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0064\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r7\/8 [=========================>....] - ETA: 0s - loss: 0.0024\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 9ms\/step - loss: 0.0023\n",
      "Epoch 173\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0064\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r4\/8 [==============>...............] - ETA: 0s - loss: 0.0040\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 14ms\/step - loss: 0.0022\n",
      "Epoch 174\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 9.8198e-04\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r5\/8 [=================>............] - ETA: 0s - loss: 0.0016    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 12ms\/step - loss: 0.0022\n",
      "Epoch 175\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 9.7416e-04\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r4\/8 [==============>...............] - ETA: 0s - loss: 0.0023    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r7\/8 [=========================>....] - ETA: 0s - loss: 0.0016\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 20ms\/step - loss: 0.0022\n",
      "Epoch 176\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0020\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r4\/8 [==============>...............] - ETA: 0s - loss: 0.0023\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r7\/8 [=========================>....] - ETA: 0s - loss: 0.0024\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 20ms\/step - loss: 0.0022\n",
      "Epoch 177\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0062\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r4\/8 [==============>...............] - ETA: 0s - loss: 0.0033\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r7\/8 [=========================>....] - ETA: 0s - loss: 0.0023\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 20ms\/step - loss: 0.0022\n",
      "Epoch 178\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0061\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r4\/8 [==============>...............] - ETA: 0s - loss: 0.0036\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r7\/8 [=========================>....] - ETA: 0s - loss: 0.0025\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 20ms\/step - loss: 0.0022\n",
      "Epoch 179\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0019\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r5\/8 [=================>............] - ETA: 0s - loss: 0.0030\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 11ms\/step - loss: 0.0021\n",
      "Epoch 180\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0060\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r6\/8 [=====================>........] - ETA: 0s - loss: 0.0024\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 11ms\/step - loss: 0.0021\n",
      "Epoch 181\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0013\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r6\/8 [=====================>........] - ETA: 0s - loss: 8.2682e-04\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 14ms\/step - loss: 0.0021\n",
      "Epoch 182\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0013\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r6\/8 [=====================>........] - ETA: 0s - loss: 8.1867e-04\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 11ms\/step - loss: 0.0021\n",
      "Epoch 183\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 9.0869e-04\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r5\/8 [=================>............] - ETA: 0s - loss: 0.0017    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 12ms\/step - loss: 0.0021\n",
      "Epoch 184\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0059\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r7\/8 [=========================>....] - ETA: 0s - loss: 0.0015\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 9ms\/step - loss: 0.0021\n",
      "Epoch 185\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 8.9427e-04\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r6\/8 [=====================>........] - ETA: 0s - loss: 0.0018    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 11ms\/step - loss: 0.0020\n",
      "Epoch 186\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0058\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - ETA: 0s - loss: 0.0020\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 9ms\/step - loss: 0.0020\n",
      "Epoch 187\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0057\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r7\/8 [=========================>....] - ETA: 0s - loss: 0.0022\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 9ms\/step - loss: 0.0020\n",
      "Epoch 188\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 3.1347e-07\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r7\/8 [=========================>....] - ETA: 0s - loss: 0.0022    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 11ms\/step - loss: 0.0020\n",
      "Epoch 189\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 1.3049e-05\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r7\/8 [=========================>....] - ETA: 0s - loss: 0.0021    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 9ms\/step - loss: 0.0020\n",
      "Epoch 190\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 6.0891e-04\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r4\/8 [==============>...............] - ETA: 0s - loss: 9.1858e-04\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 11ms\/step - loss: 0.0020\n",
      "Epoch 191\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0056\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r6\/8 [=====================>........] - ETA: 0s - loss: 0.0021\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 11ms\/step - loss: 0.0020\n",
      "Epoch 192\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0055\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 7ms\/step - loss: 0.0020\n",
      "Epoch 193\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 2.9463e-07\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 3ms\/step - loss: 0.0019\n",
      "Epoch 194\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0017\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 3ms\/step - loss: 0.0019\n",
      "Epoch 195\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0054\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 3ms\/step - loss: 0.0019\n",
      "Epoch 196\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0054\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 3ms\/step - loss: 0.0019\n",
      "Epoch 197\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 2.7757e-07\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 2ms\/step - loss: 0.0019\n",
      "Epoch 198\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 5.9247e-04\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 1ms\/step - loss: 0.0019\n",
      "Epoch 199\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 1.1988e-05\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 2ms\/step - loss: 0.0019\n",
      "Epoch 200\/200\n",
      "\r1\/8 [==>...........................] - ETA: 0s - loss: 0.0017\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8\/8 [==============================] - 0s 1ms\/step - loss: 0.0018\n",
      "\r1\/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1\/1 [==============================] - 0s 51ms\/step\n",
      "raw weight\n",
      "[array([[-0.4781558 , -0.9405012 , -1.011174  ,  0.5810619 ],\n",
      "       [-0.3133666 ,  1.2319456 , -0.66996014, -1.0719705 ]],\n",
      "      dtype=float32), array([-0.16903117, -0.23714553, -0.37655187,  0.32053724], dtype=float32)]\n",
      " weight shape (2, 4) bias shape (4,)\n",
      "result weight for layer-0\n",
      "[[-0.16903117 -0.23714553 -0.37655187  0.32053724]\n",
      " [-0.47815579 -0.94050121 -1.01117396  0.5810619 ]\n",
      " [-0.31336659  1.23194563 -0.66996014 -1.07197046]]\n",
      "expected weight for layer-0\n",
      "[[-0.28730211 -0.28822282 -0.70597451  0.42094471]\n",
      " [-0.5790794  -1.1836444  -1.34287961  0.69575311]\n",
      " [-0.41434377  1.51314676 -0.97649086 -1.3043465 ]]\n",
      "\n",
      "Layer 0 weights rmse 0.047076242017247784\n",
      "\n",
      "raw weight\n",
      "[array([[-0.41146794,  0.39146814],\n",
      "       [ 1.0709723 , -1.0509719 ],\n",
      "       [-0.8693183 ,  0.84931827],\n",
      "       [ 0.89356214, -0.8735621 ]], dtype=float32), array([-1.2674518,  1.2874519], dtype=float32)]\n",
      " weight shape (4, 2) bias shape (2,)\n",
      "result weight for layer-1\n",
      "[[-1.26745176  1.28745186]\n",
      " [-0.41146794  0.39146814]\n",
      " [ 1.07097232 -1.05097187]\n",
      " [-0.86931831  0.84931827]\n",
      " [ 0.89356214 -0.8735621 ]]\n",
      "expected weight for layer-1\n",
      "[[-1.72078607  1.74078607]\n",
      " [-0.50352956  0.48352956]\n",
      " [ 1.25764816 -1.23764816]\n",
      " [-1.16998784  1.14998784]\n",
      " [ 1.0907634  -1.0707634 ]]\n",
      "\n",
      "Layer 1 weights rmse 0.07562515036121947\n",
      "\n",
      "output \n",
      "[[7.2097749e-02 9.2790216e-01]\n",
      " [9.9659240e-01 3.4074972e-03]\n",
      " [3.2766417e-02 9.6723366e-01]\n",
      " [9.9949104e-01 5.0892896e-04]\n",
      " [9.5867199e-01 4.1328005e-02]\n",
      " [2.3846747e-02 9.7615319e-01]\n",
      " [9.7194678e-01 2.8053250e-02]\n",
      " [7.2097749e-02 9.2790216e-01]]\n",
      "expect [[0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]]\n",
      "Testing result error 0.0018181683494877072\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"vCo9xV3ScYoDCFN4RclLJU",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "## Konfigurasi Dataset Iris\n",
    "Berikut merupakan konfigurasi dataset Iris"
   ],
   "attachments":{},
   "metadata":{
    "datalore":{
     "node_id":"hW8e1zXH9uF2vae0YioawQ",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "### Preprocessing Dataset"
   ],
   "attachments":{},
   "metadata":{
    "datalore":{
     "node_id":"9aShJEI3xohk6XYEum4GgS",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "# Fetch and process dataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Testcase Iris\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"iris.csv\")\n",
    "\n",
    "dict_target = {'Iris-setosa': 0, 'Iris-versicolor': 1, 'Iris-virginica' : 2 }\n",
    "\n",
    "x = df.iloc[:,1:-1]\n",
    "x=(x-x.mean())\/x.std()\n",
    "y = df.iloc[:, -1].replace(dict_target)\n",
    "\n",
    "target_count = len(y.unique())\n",
    "\n",
    "def generate_target(target: int):\n",
    "    target_array = np.zeros((target_count,), dtype=float)\n",
    "    target_array[target] = 1.0\n",
    "    return target_array\n",
    "\n",
    "y_processed = np.zeros(shape=(len(y), target_count), dtype=float)\n",
    "\n",
    "for i, element in enumerate(y):\n",
    "    y_processed[i] = generate_target(element)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x.to_numpy(), y_processed, test_size = 0.2, random_state=0)\n",
    "\n",
    "# print_debug(x_train)\n",
    "\n",
    "input_size = len(x_train[0])"
   ],
   "execution_count":397,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"OTkZXvU5N0kiwWNPp3j8HL",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "### Konfigurasi Model"
   ],
   "attachments":{},
   "metadata":{
    "datalore":{
     "node_id":"B0mvdc8ubsKt68yLejOVdC",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "# Set configs for the model\n",
    "learning_rate = 0.1\n",
    "batch_size = 1\n",
    "max_iteration = 15\n",
    "error_threshold = 0.05"
   ],
   "execution_count":398,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"0au9XzdsyMHiUul4ieADY7",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "### Training Model"
   ],
   "attachments":{},
   "metadata":{
    "datalore":{
     "node_id":"HQoVrto8xvUvBUR5Jtnmut",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "# Train and save model\n",
    "\n",
    "model = Model()\n",
    "\n",
    "layers = [\n",
    "    {\n",
    "        \"number_of_neurons\": 3,\n",
    "        \"activation_function\": \"linear\",\n",
    "        \"input_size\": input_size\n",
    "    },\n",
    "    {\n",
    "        \"number_of_neurons\": target_count,\n",
    "        \"activation_function\": \"sigmoid\"\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "for i, layer in enumerate(layers):\n",
    "        if i == 0:\n",
    "            model.add(\n",
    "                Layer(\n",
    "                    n_neurons=layer[\"number_of_neurons\"],\n",
    "                    activation_function=layer[\"activation_function\"],\n",
    "                    input_size=layer[\"input_size\"]\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            model.add(\n",
    "                Layer(\n",
    "                    n_neurons=layer[\"number_of_neurons\"],\n",
    "                    activation_function=layer[\"activation_function\"],\n",
    "                )\n",
    "            )\n",
    "\n",
    "\n",
    "model.compile()\n",
    "\n",
    "# The initial weight isn't manually set because it's already set randomly during the initialization of the layer\n",
    "\n",
    "model.visualize()\n",
    "\n",
    "model.fit(\n",
    "        x=x_train,\n",
    "        y=y_train,\n",
    "        learning_rate=learning_rate,\n",
    "        epochs=max_iteration,\n",
    "        batch_size=batch_size,\n",
    "        error_threshold=error_threshold\n",
    "    )\n",
    "\n",
    "saved_filename = \"model_iris.json\"\n",
    "\n",
    "saved_model = SavedModel.from_model(model)\n",
    "saved_model.save_to(saved_filename)\n",
    "\n",
    "# Lalu lakukan load kembali trained model (jadi ga perlu fit lagi)\n",
    "# Model yang di-load ini langsung bisa digunakan untuk predict menggunakan data test"
   ],
   "execution_count":399,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "\/* Layer-1 (linear) *\/\n",
      "\/* Neuron-1 *\/\n",
      "Input Layer Neuron-1 -> Hidden Layer-1 Neuron-1 (0.24875314351995803)\n",
      "Input Layer Neuron-2 -> Hidden Layer-1 Neuron-1 (0.5761573344178369)\n",
      "Input Layer Neuron-3 -> Hidden Layer-1 Neuron-1 (0.592041931271839)\n",
      "\n",
      "\n",
      "\n",
      "\/* Neuron-2 *\/\n",
      "Input Layer Neuron-1 -> Hidden Layer-1 Neuron-2 (0.952749011516985)\n",
      "Input Layer Neuron-2 -> Hidden Layer-1 Neuron-2 (0.44712537861762736)\n",
      "Input Layer Neuron-3 -> Hidden Layer-1 Neuron-2 (0.8464086724711278)\n",
      "\n",
      "\n",
      "\n",
      "\/* Neuron-3 *\/\n",
      "Input Layer Neuron-1 -> Hidden Layer-1 Neuron-3 (0.8137978197024772)\n",
      "Input Layer Neuron-2 -> Hidden Layer-1 Neuron-3 (0.39650574084698464)\n",
      "Input Layer Neuron-3 -> Hidden Layer-1 Neuron-3 (0.8811031971111616)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\/* Layer-2 (sigmoid) *\/\n",
      "\/* Neuron-1 *\/\n",
      "Hidden Layer-1 Neuron-1 -> Hidden Layer-2 Neuron-1 (0.6925315900777659)\n",
      "Hidden Layer-1 Neuron-2 -> Hidden Layer-2 Neuron-1 (0.7252542798196405)\n",
      "\n",
      "\n",
      "\n",
      "\/* Neuron-2 *\/\n",
      "Hidden Layer-1 Neuron-1 -> Hidden Layer-2 Neuron-2 (0.6439901992296374)\n",
      "Hidden Layer-1 Neuron-2 -> Hidden Layer-2 Neuron-2 (0.4238550485581797)\n",
      "\n",
      "\n",
      "\n",
      "\/* Neuron-3 *\/\n",
      "Hidden Layer-1 Neuron-1 -> Hidden Layer-2 Neuron-3 (0.30157481667454933)\n",
      "Hidden Layer-1 Neuron-2 -> Hidden Layer-2 Neuron-3 (0.660173537492685)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Begin training model\n",
      "\n",
      "\n",
      "Epoch 0\n",
      "training loss 0.4067101958867599\n",
      "\n",
      "\n",
      "Epoch 1\n",
      "training loss 0.12859966101623188\n",
      "\n",
      "\n",
      "Epoch 2\n",
      "training loss 0.11055157594387102\n",
      "\n",
      "\n",
      "Epoch 3\n",
      "training loss 0.10299746469705484\n",
      "\n",
      "\n",
      "Epoch 4\n",
      "training loss 0.09755105906580355\n",
      "\n",
      "\n",
      "Epoch 5\n",
      "training loss 0.09340963631087841\n",
      "\n",
      "\n",
      "Epoch 6\n",
      "training loss 0.0903186726674997\n",
      "\n",
      "\n",
      "Epoch 7\n",
      "training loss 0.08801077114957272\n",
      "\n",
      "\n",
      "Epoch 8\n",
      "training loss 0.08625321397561629\n",
      "\n",
      "\n",
      "Epoch 9\n",
      "training loss 0.08487998225631936\n",
      "\n",
      "\n",
      "Epoch 10\n",
      "training loss 0.08378056237148848\n",
      "\n",
      "\n",
      "Epoch 11\n",
      "training loss 0.08288167141251121\n",
      "\n",
      "\n",
      "Epoch 12\n",
      "training loss 0.08213363023361689\n",
      "\n",
      "\n",
      "Epoch 13\n",
      "training loss 0.08150176503961623\n",
      "\n",
      "\n",
      "Epoch 14\n",
      "training loss 0.08096117285969014\n",
      "\n",
      "\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"CYFgZb9xriXO1IOk543aZ2",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "### Load Model dan Predict"
   ],
   "attachments":{},
   "metadata":{
    "datalore":{
     "node_id":"feBplC3lkF8uZAUUu5qdUB",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "# Load model and use it to make predictions\n",
    "saved_model = SavedModel.from_file(saved_filename)\n",
    "iris_model = Model.from_saved_model(saved_model)\n",
    "\n",
    "results = iris_model.predict_batch(x_test)\n",
    "for result in results:\n",
    "    max_element_idx = np.argmax(result)\n",
    "    for i in range(len(result)):\n",
    "        if i != max_element_idx:\n",
    "            result[i] = 0\n",
    "        else:\n",
    "            result[i] = 1\n",
    "\n",
    "counter = 0\n",
    "for i, test in enumerate(y_test):\n",
    "    print(f\"y test: {test} \\t y prediction: {results[i]} \\t {np.argmax(test) == np.argmax(results[i])}\")\n",
    "    if np.argmax(test) == np.argmax(results[i]):\n",
    "        counter+=1\n",
    "    \n",
    "print(f\"accuracy: {counter \/ len(y_test) * 100}%\")"
   ],
   "execution_count":400,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "y test: [0. 0. 1.] \t y prediction: [0. 0. 1.] \t True\n",
      "y test: [0. 1. 0.] \t y prediction: [0. 1. 0.] \t True\n",
      "y test: [1. 0. 0.] \t y prediction: [1. 0. 0.] \t True\n",
      "y test: [0. 0. 1.] \t y prediction: [0. 0. 1.] \t True\n",
      "y test: [1. 0. 0.] \t y prediction: [1. 0. 0.] \t True\n",
      "y test: [0. 0. 1.] \t y prediction: [0. 0. 1.] \t True\n",
      "y test: [1. 0. 0.] \t y prediction: [1. 0. 0.] \t True\n",
      "y test: [0. 1. 0.] \t y prediction: [0. 1. 0.] \t True\n",
      "y test: [0. 1. 0.] \t y prediction: [0. 1. 0.] \t True\n",
      "y test: [0. 1. 0.] \t y prediction: [0. 1. 0.] \t True\n",
      "y test: [0. 0. 1.] \t y prediction: [0. 1. 0.] \t False\n",
      "y test: [0. 1. 0.] \t y prediction: [0. 1. 0.] \t True\n",
      "y test: [0. 1. 0.] \t y prediction: [0. 1. 0.] \t True\n",
      "y test: [0. 1. 0.] \t y prediction: [0. 1. 0.] \t True\n",
      "y test: [0. 1. 0.] \t y prediction: [0. 1. 0.] \t True\n",
      "y test: [1. 0. 0.] \t y prediction: [1. 0. 0.] \t True\n",
      "y test: [0. 1. 0.] \t y prediction: [0. 1. 0.] \t True\n",
      "y test: [0. 1. 0.] \t y prediction: [0. 1. 0.] \t True\n",
      "y test: [1. 0. 0.] \t y prediction: [1. 0. 0.] \t True\n",
      "y test: [1. 0. 0.] \t y prediction: [1. 0. 0.] \t True\n",
      "y test: [0. 0. 1.] \t y prediction: [0. 0. 1.] \t True\n",
      "y test: [0. 1. 0.] \t y prediction: [0. 1. 0.] \t True\n",
      "y test: [1. 0. 0.] \t y prediction: [1. 0. 0.] \t True\n",
      "y test: [1. 0. 0.] \t y prediction: [1. 0. 0.] \t True\n",
      "y test: [0. 0. 1.] \t y prediction: [0. 0. 1.] \t True\n",
      "y test: [1. 0. 0.] \t y prediction: [1. 0. 0.] \t True\n",
      "y test: [1. 0. 0.] \t y prediction: [1. 0. 0.] \t True\n",
      "y test: [0. 1. 0.] \t y prediction: [0. 1. 0.] \t True\n",
      "y test: [0. 1. 0.] \t y prediction: [0. 1. 0.] \t True\n",
      "y test: [1. 0. 0.] \t y prediction: [1. 0. 0.] \t True\n",
      "accuracy: 96.66666666666667%\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"jQQi7G5nIhZaMqWpQHAW1b",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "# lama"
   ],
   "attachments":{},
   "metadata":{
    "datalore":{
     "node_id":"lama",
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "sheet_delimiter":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "def encode_target(data, target_name):\n",
    "    label_encoder = LabelEncoder()\n",
    "    data['encoded_target'] = label_encoder.fit_transform(data[target_name])\n",
    "    data_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
    "    return data, data_mapping\n",
    "\n",
    "def generate_weight(layers, input_size):\n",
    "    initial_weights = []\n",
    "    previous_size = input_size\n",
    "    for layer in layers:\n",
    "        layer_weights = np.random.randn(layer['number_of_neurons'], previous_size).tolist()\n",
    "        initial_weights.append(layer_weights)\n",
    "        previous_size = layer['number_of_neurons']\n",
    "    return initial_weights\n",
    "\n",
    "def generate_layers(layers_info):\n",
    "    layers = []\n",
    "    for layer_info in layers_info:\n",
    "        layers.append({\n",
    "            \"number_of_neurons\": layer_info[\"number_of_neurons\"],\n",
    "            \"activation_function\": layer_info[\"activation_function\"]\n",
    "        })\n",
    "    return layers\n",
    "\n",
    "def convert_to_json(data, input_columns, layers_info, target_column, learning_rate, batch_size, max_iteration, error_threshold):\n",
    "    input_size = len(input_columns)\n",
    "    layers = generate_layers(layers_info)\n",
    "    initial_weights = generate_weight(layers, input_size)\n",
    "    \n",
    "    data, _ = encode_target(data, target_column)\n",
    "    \n",
    "    json_structure = {\n",
    "        \"case\": {\n",
    "            \"model\": {\n",
    "                \"input_size\": input_size,\n",
    "                \"layers\": layers\n",
    "            },\n",
    "            \"input\": data[input_columns].values.tolist(),\n",
    "            \"initial_weights\": initial_weights,\n",
    "            \"target\": data['encoded_target'].values.reshape(-1, 1).tolist(),\n",
    "            \"learning_parameters\": {\n",
    "                \"learning_rate\": learning_rate,\n",
    "                \"batch_size\": batch_size,\n",
    "                \"max_iteration\": max_iteration,\n",
    "                \"error_threshold\": error_threshold\n",
    "            }\n",
    "        },\n",
    "        \"expect\": {\n",
    "            \"stopped_by\": \"max_iteration\",\n",
    "            \"final_weights\": []  # This needs actual implementation to compute final weights\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return json_structure"
   ],
   "execution_count":401,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"TCUBhuovtN6SkTIaEn0rzO",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "input_columns = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']\n",
    "layers_info = [\n",
    "    {\"number_of_neurons\": 5, \"activation_function\": \"relu\"},\n",
    "    {\"number_of_neurons\": 3, \"activation_function\": \"sigmoid\"}\n",
    "]\n",
    "learning_rate = 0.1\n",
    "batch_size = 2\n",
    "max_iteration = 1\n",
    "error_threshold = 0.0\n",
    "\n",
    "data = pd.read_csv('iris.csv')\n",
    "\n",
    "model_config = convert_to_json(data, input_columns, layers_info, 'Species', learning_rate, batch_size, max_iteration, error_threshold)\n",
    "print(json.dumps(model_config, indent=4))"
   ],
   "execution_count":402,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "{\n",
      "    \"case\": {\n",
      "        \"model\": {\n",
      "            \"input_size\": 4,\n",
      "            \"layers\": [\n",
      "                {\n",
      "                    \"number_of_neurons\": 5,\n",
      "                    \"activation_function\": \"relu\"\n",
      "                },\n",
      "                {\n",
      "                    \"number_of_neurons\": 3,\n",
      "                    \"activation_function\": \"sigmoid\"\n",
      "                }\n",
      "            ]\n",
      "        },\n",
      "        \"input\": [\n",
      "            [\n",
      "                5.1,\n",
      "                3.5,\n",
      "                1.4,\n",
      "                0.2\n",
      "            ],\n",
      "            [\n",
      "                4.9,\n",
      "                3.0,\n",
      "                1.4,\n",
      "                0.2\n",
      "            ],\n",
      "            [\n",
      "                4.7,\n",
      "                3.2,\n",
      "                1.3,\n",
      "                0.2\n",
      "            ],\n",
      "            [\n",
      "                4.6,\n",
      "                3.1,\n",
      "                1.5,\n",
      "                0.2\n",
      "            ],\n",
      "            [\n",
      "                5.0,\n",
      "                3.6,\n",
      "                1.4,\n",
      "                0.2\n",
      "            ],\n",
      "            [\n",
      "                5.4,\n",
      "                3.9,\n",
      "                1.7,\n",
      "                0.4\n",
      "            ],\n",
      "            [\n",
      "                4.6,\n",
      "                3.4,\n",
      "                1.4,\n",
      "                0.3\n",
      "            ],\n",
      "            [\n",
      "                5.0,\n",
      "                3.4,\n",
      "                1.5,\n",
      "                0.2\n",
      "            ],\n",
      "            [\n",
      "                4.4,\n",
      "                2.9,\n",
      "                1.4,\n",
      "                0.2\n",
      "            ],\n",
      "            [\n",
      "                4.9,\n",
      "                3.1,\n",
      "                1.5,\n",
      "                0.1\n",
      "            ],\n",
      "            [\n",
      "                5.4,\n",
      "                3.7,\n",
      "                1.5,\n",
      "                0.2\n",
      "            ],\n",
      "            [\n",
      "                4.8,\n",
      "                3.4,\n",
      "                1.6,\n",
      "                0.2\n",
      "            ],\n",
      "            [\n",
      "                4.8,\n",
      "                3.0,\n",
      "                1.4,\n",
      "                0.1\n",
      "            ],\n",
      "            [\n",
      "                4.3,\n",
      "                3.0,\n",
      "                1.1,\n",
      "                0.1\n",
      "            ],\n",
      "            [\n",
      "                5.8,\n",
      "                4.0,\n",
      "                1.2,\n",
      "                0.2\n",
      "            ],\n",
      "            [\n",
      "                5.7,\n",
      "                4.4,\n",
      "                1.5,\n",
      "                0.4\n",
      "            ],\n",
      "            [\n",
      "                5.4,\n",
      "                3.9,\n",
      "                1.3,\n",
      "                0.4\n",
      "            ],\n",
      "            [\n",
      "                5.1,\n",
      "                3.5,\n",
      "                1.4,\n",
      "                0.3\n",
      "            ],\n",
      "            [\n",
      "                5.7,\n",
      "                3.8,\n",
      "                1.7,\n",
      "                0.3\n",
      "            ],\n",
      "            [\n",
      "                5.1,\n",
      "                3.8,\n",
      "                1.5,\n",
      "                0.3\n",
      "            ],\n",
      "            [\n",
      "                5.4,\n",
      "                3.4,\n",
      "                1.7,\n",
      "                0.2\n",
      "            ],\n",
      "            [\n",
      "                5.1,\n",
      "                3.7,\n",
      "                1.5,\n",
      "                0.4\n",
      "            ],\n",
      "            [\n",
      "                4.6,\n",
      "                3.6,\n",
      "                1.0,\n",
      "                0.2\n",
      "            ],\n",
      "            [\n",
      "                5.1,\n",
      "                3.3,\n",
      "                1.7,\n",
      "                0.5\n",
      "            ],\n",
      "            [\n",
      "                4.8,\n",
      "                3.4,\n",
      "                1.9,\n",
      "                0.2\n",
      "            ],\n",
      "            [\n",
      "                5.0,\n",
      "                3.0,\n",
      "                1.6,\n",
      "                0.2\n",
      "            ],\n",
      "            [\n",
      "                5.0,\n",
      "                3.4,\n",
      "                1.6,\n",
      "                0.4\n",
      "            ],\n",
      "            [\n",
      "                5.2,\n",
      "                3.5,\n",
      "                1.5,\n",
      "                0.2\n",
      "            ],\n",
      "            [\n",
      "                5.2,\n",
      "                3.4,\n",
      "                1.4,\n",
      "                0.2\n",
      "            ],\n",
      "            [\n",
      "                4.7,\n",
      "                3.2,\n",
      "                1.6,\n",
      "                0.2\n",
      "            ],\n",
      "            [\n",
      "                4.8,\n",
      "                3.1,\n",
      "                1.6,\n",
      "                0.2\n",
      "            ],\n",
      "            [\n",
      "                5.4,\n",
      "                3.4,\n",
      "                1.5,\n",
      "                0.4\n",
      "            ],\n",
      "            [\n",
      "                5.2,\n",
      "                4.1,\n",
      "                1.5,\n",
      "                0.1\n",
      "            ],\n",
      "            [\n",
      "                5.5,\n",
      "                4.2,\n",
      "                1.4,\n",
      "                0.2\n",
      "            ],\n",
      "            [\n",
      "                4.9,\n",
      "                3.1,\n",
      "                1.5,\n",
      "                0.1\n",
      "            ],\n",
      "            [\n",
      "                5.0,\n",
      "                3.2,\n",
      "                1.2,\n",
      "                0.2\n",
      "            ],\n",
      "            [\n",
      "                5.5,\n",
      "                3.5,\n",
      "                1.3,\n",
      "                0.2\n",
      "            ],\n",
      "            [\n",
      "                4.9,\n",
      "                3.1,\n",
      "                1.5,\n",
      "                0.1\n",
      "            ],\n",
      "            [\n",
      "                4.4,\n",
      "                3.0,\n",
      "                1.3,\n",
      "                0.2\n",
      "            ],\n",
      "            [\n",
      "                5.1,\n",
      "                3.4,\n",
      "                1.5,\n",
      "                0.2\n",
      "            ],\n",
      "            [\n",
      "                5.0,\n",
      "                3.5,\n",
      "                1.3,\n",
      "                0.3\n",
      "            ],\n",
      "            [\n",
      "                4.5,\n",
      "                2.3,\n",
      "                1.3,\n",
      "                0.3\n",
      "            ],\n",
      "            [\n",
      "                4.4,\n",
      "                3.2,\n",
      "                1.3,\n",
      "                0.2\n",
      "            ],\n",
      "            [\n",
      "                5.0,\n",
      "                3.5,\n",
      "                1.6,\n",
      "                0.6\n",
      "            ],\n",
      "            [\n",
      "                5.1,\n",
      "                3.8,\n",
      "                1.9,\n",
      "                0.4\n",
      "            ],\n",
      "            [\n",
      "                4.8,\n",
      "                3.0,\n",
      "                1.4,\n",
      "                0.3\n",
      "            ],\n",
      "            [\n",
      "                5.1,\n",
      "                3.8,\n",
      "                1.6,\n",
      "                0.2\n",
      "            ],\n",
      "            [\n",
      "                4.6,\n",
      "                3.2,\n",
      "                1.4,\n",
      "                0.2\n",
      "            ],\n",
      "            [\n",
      "                5.3,\n",
      "                3.7,\n",
      "                1.5,\n",
      "                0.2\n",
      "            ],\n",
      "            [\n",
      "                5.0,\n",
      "                3.3,\n",
      "                1.4,\n",
      "                0.2\n",
      "            ],\n",
      "            [\n",
      "                7.0,\n",
      "                3.2,\n",
      "                4.7,\n",
      "                1.4\n",
      "            ],\n",
      "            [\n",
      "                6.4,\n",
      "                3.2,\n",
      "                4.5,\n",
      "                1.5\n",
      "            ],\n",
      "            [\n",
      "                6.9,\n",
      "                3.1,\n",
      "                4.9,\n",
      "                1.5\n",
      "            ],\n",
      "            [\n",
      "                5.5,\n",
      "                2.3,\n",
      "                4.0,\n",
      "                1.3\n",
      "            ],\n",
      "            [\n",
      "                6.5,\n",
      "                2.8,\n",
      "                4.6,\n",
      "                1.5\n",
      "            ],\n",
      "            [\n",
      "                5.7,\n",
      "                2.8,\n",
      "                4.5,\n",
      "                1.3\n",
      "            ],\n",
      "            [\n",
      "                6.3,\n",
      "                3.3,\n",
      "                4.7,\n",
      "                1.6\n",
      "            ],\n",
      "            [\n",
      "                4.9,\n",
      "                2.4,\n",
      "                3.3,\n",
      "                1.0\n",
      "            ],\n",
      "            [\n",
      "                6.6,\n",
      "                2.9,\n",
      "                4.6,\n",
      "                1.3\n",
      "            ],\n",
      "            [\n",
      "                5.2,\n",
      "                2.7,\n",
      "                3.9,\n",
      "                1.4\n",
      "            ],\n",
      "            [\n",
      "                5.0,\n",
      "                2.0,\n",
      "                3.5,\n",
      "                1.0\n",
      "            ],\n",
      "            [\n",
      "                5.9,\n",
      "                3.0,\n",
      "                4.2,\n",
      "                1.5\n",
      "            ],\n",
      "            [\n",
      "                6.0,\n",
      "                2.2,\n",
      "                4.0,\n",
      "                1.0\n",
      "            ],\n",
      "            [\n",
      "                6.1,\n",
      "                2.9,\n",
      "                4.7,\n",
      "                1.4\n",
      "            ],\n",
      "            [\n",
      "                5.6,\n",
      "                2.9,\n",
      "                3.6,\n",
      "                1.3\n",
      "            ],\n",
      "            [\n",
      "                6.7,\n",
      "                3.1,\n",
      "                4.4,\n",
      "                1.4\n",
      "            ],\n",
      "            [\n",
      "                5.6,\n",
      "                3.0,\n",
      "                4.5,\n",
      "                1.5\n",
      "            ],\n",
      "            [\n",
      "                5.8,\n",
      "                2.7,\n",
      "                4.1,\n",
      "                1.0\n",
      "            ],\n",
      "            [\n",
      "                6.2,\n",
      "                2.2,\n",
      "                4.5,\n",
      "                1.5\n",
      "            ],\n",
      "            [\n",
      "                5.6,\n",
      "                2.5,\n",
      "                3.9,\n",
      "                1.1\n",
      "            ],\n",
      "            [\n",
      "                5.9,\n",
      "                3.2,\n",
      "                4.8,\n",
      "                1.8\n",
      "            ],\n",
      "            [\n",
      "                6.1,\n",
      "                2.8,\n",
      "                4.0,\n",
      "                1.3\n",
      "            ],\n",
      "            [\n",
      "                6.3,\n",
      "                2.5,\n",
      "                4.9,\n",
      "                1.5\n",
      "            ],\n",
      "            [\n",
      "                6.1,\n",
      "                2.8,\n",
      "                4.7,\n",
      "                1.2\n",
      "            ],\n",
      "            [\n",
      "                6.4,\n",
      "                2.9,\n",
      "                4.3,\n",
      "                1.3\n",
      "            ],\n",
      "            [\n",
      "                6.6,\n",
      "                3.0,\n",
      "                4.4,\n",
      "                1.4\n",
      "            ],\n",
      "            [\n",
      "                6.8,\n",
      "                2.8,\n",
      "                4.8,\n",
      "                1.4\n",
      "            ],\n",
      "            [\n",
      "                6.7,\n",
      "                3.0,\n",
      "                5.0,\n",
      "                1.7\n",
      "            ],\n",
      "            [\n",
      "                6.0,\n",
      "                2.9,\n",
      "                4.5,\n",
      "                1.5\n",
      "            ],\n",
      "            [\n",
      "                5.7,\n",
      "                2.6,\n",
      "                3.5,\n",
      "                1.0\n",
      "            ],\n",
      "            [\n",
      "                5.5,\n",
      "                2.4,\n",
      "                3.8,\n",
      "                1.1\n",
      "            ],\n",
      "            [\n",
      "                5.5,\n",
      "                2.4,\n",
      "                3.7,\n",
      "                1.0\n",
      "            ],\n",
      "            [\n",
      "                5.8,\n",
      "                2.7,\n",
      "                3.9,\n",
      "                1.2\n",
      "            ],\n",
      "            [\n",
      "                6.0,\n",
      "                2.7,\n",
      "                5.1,\n",
      "                1.6\n",
      "            ],\n",
      "            [\n",
      "                5.4,\n",
      "                3.0,\n",
      "                4.5,\n",
      "                1.5\n",
      "            ],\n",
      "            [\n",
      "                6.0,\n",
      "                3.4,\n",
      "                4.5,\n",
      "                1.6\n",
      "            ],\n",
      "            [\n",
      "                6.7,\n",
      "                3.1,\n",
      "                4.7,\n",
      "                1.5\n",
      "            ],\n",
      "            [\n",
      "                6.3,\n",
      "                2.3,\n",
      "                4.4,\n",
      "                1.3\n",
      "            ],\n",
      "            [\n",
      "                5.6,\n",
      "                3.0,\n",
      "                4.1,\n",
      "                1.3\n",
      "            ],\n",
      "            [\n",
      "                5.5,\n",
      "                2.5,\n",
      "                4.0,\n",
      "                1.3\n",
      "            ],\n",
      "            [\n",
      "                5.5,\n",
      "                2.6,\n",
      "                4.4,\n",
      "                1.2\n",
      "            ],\n",
      "            [\n",
      "                6.1,\n",
      "                3.0,\n",
      "                4.6,\n",
      "                1.4\n",
      "            ],\n",
      "            [\n",
      "                5.8,\n",
      "                2.6,\n",
      "                4.0,\n",
      "                1.2\n",
      "            ],\n",
      "            [\n",
      "                5.0,\n",
      "                2.3,\n",
      "                3.3,\n",
      "                1.0\n",
      "            ],\n",
      "            [\n",
      "                5.6,\n",
      "                2.7,\n",
      "                4.2,\n",
      "                1.3\n",
      "            ],\n",
      "            [\n",
      "                5.7,\n",
      "                3.0,\n",
      "                4.2,\n",
      "                1.2\n",
      "            ],\n",
      "            [\n",
      "                5.7,\n",
      "                2.9,\n",
      "                4.2,\n",
      "                1.3\n",
      "            ],\n",
      "            [\n",
      "                6.2,\n",
      "                2.9,\n",
      "                4.3,\n",
      "                1.3\n",
      "            ],\n",
      "            [\n",
      "                5.1,\n",
      "                2.5,\n",
      "                3.0,\n",
      "                1.1\n",
      "            ],\n",
      "            [\n",
      "                5.7,\n",
      "                2.8,\n",
      "                4.1,\n",
      "                1.3\n",
      "            ],\n",
      "            [\n",
      "                6.3,\n",
      "                3.3,\n",
      "                6.0,\n",
      "                2.5\n",
      "            ],\n",
      "            [\n",
      "                5.8,\n",
      "                2.7,\n",
      "                5.1,\n",
      "                1.9\n",
      "            ],\n",
      "            [\n",
      "                7.1,\n",
      "                3.0,\n",
      "                5.9,\n",
      "                2.1\n",
      "            ],\n",
      "            [\n",
      "                6.3,\n",
      "                2.9,\n",
      "                5.6,\n",
      "                1.8\n",
      "            ],\n",
      "            [\n",
      "                6.5,\n",
      "                3.0,\n",
      "                5.8,\n",
      "                2.2\n",
      "            ],\n",
      "            [\n",
      "                7.6,\n",
      "                3.0,\n",
      "                6.6,\n",
      "                2.1\n",
      "            ],\n",
      "            [\n",
      "                4.9,\n",
      "                2.5,\n",
      "                4.5,\n",
      "                1.7\n",
      "            ],\n",
      "            [\n",
      "                7.3,\n",
      "                2.9,\n",
      "                6.3,\n",
      "                1.8\n",
      "            ],\n",
      "            [\n",
      "                6.7,\n",
      "                2.5,\n",
      "                5.8,\n",
      "                1.8\n",
      "            ],\n",
      "            [\n",
      "                7.2,\n",
      "                3.6,\n",
      "                6.1,\n",
      "                2.5\n",
      "            ],\n",
      "            [\n",
      "                6.5,\n",
      "                3.2,\n",
      "                5.1,\n",
      "                2.0\n",
      "            ],\n",
      "            [\n",
      "                6.4,\n",
      "                2.7,\n",
      "                5.3,\n",
      "                1.9\n",
      "            ],\n",
      "            [\n",
      "                6.8,\n",
      "                3.0,\n",
      "                5.5,\n",
      "                2.1\n",
      "            ],\n",
      "            [\n",
      "                5.7,\n",
      "                2.5,\n",
      "                5.0,\n",
      "                2.0\n",
      "            ],\n",
      "            [\n",
      "                5.8,\n",
      "                2.8,\n",
      "                5.1,\n",
      "                2.4\n",
      "            ],\n",
      "            [\n",
      "                6.4,\n",
      "                3.2,\n",
      "                5.3,\n",
      "                2.3\n",
      "            ],\n",
      "            [\n",
      "                6.5,\n",
      "                3.0,\n",
      "                5.5,\n",
      "                1.8\n",
      "            ],\n",
      "            [\n",
      "                7.7,\n",
      "                3.8,\n",
      "                6.7,\n",
      "                2.2\n",
      "            ],\n",
      "            [\n",
      "                7.7,\n",
      "                2.6,\n",
      "                6.9,\n",
      "                2.3\n",
      "            ],\n",
      "            [\n",
      "                6.0,\n",
      "                2.2,\n",
      "                5.0,\n",
      "                1.5\n",
      "            ],\n",
      "            [\n",
      "                6.9,\n",
      "                3.2,\n",
      "                5.7,\n",
      "                2.3\n",
      "            ],\n",
      "            [\n",
      "                5.6,\n",
      "                2.8,\n",
      "                4.9,\n",
      "                2.0\n",
      "            ],\n",
      "            [\n",
      "                7.7,\n",
      "                2.8,\n",
      "                6.7,\n",
      "                2.0\n",
      "            ],\n",
      "            [\n",
      "                6.3,\n",
      "                2.7,\n",
      "                4.9,\n",
      "                1.8\n",
      "            ],\n",
      "            [\n",
      "                6.7,\n",
      "                3.3,\n",
      "                5.7,\n",
      "                2.1\n",
      "            ],\n",
      "            [\n",
      "                7.2,\n",
      "                3.2,\n",
      "                6.0,\n",
      "                1.8\n",
      "            ],\n",
      "            [\n",
      "                6.2,\n",
      "                2.8,\n",
      "                4.8,\n",
      "                1.8\n",
      "            ],\n",
      "            [\n",
      "                6.1,\n",
      "                3.0,\n",
      "                4.9,\n",
      "                1.8\n",
      "            ],\n",
      "            [\n",
      "                6.4,\n",
      "                2.8,\n",
      "                5.6,\n",
      "                2.1\n",
      "            ],\n",
      "            [\n",
      "                7.2,\n",
      "                3.0,\n",
      "                5.8,\n",
      "                1.6\n",
      "            ],\n",
      "            [\n",
      "                7.4,\n",
      "                2.8,\n",
      "                6.1,\n",
      "                1.9\n",
      "            ],\n",
      "            [\n",
      "                7.9,\n",
      "                3.8,\n",
      "                6.4,\n",
      "                2.0\n",
      "            ],\n",
      "            [\n",
      "                6.4,\n",
      "                2.8,\n",
      "                5.6,\n",
      "                2.2\n",
      "            ],\n",
      "            [\n",
      "                6.3,\n",
      "                2.8,\n",
      "                5.1,\n",
      "                1.5\n",
      "            ],\n",
      "            [\n",
      "                6.1,\n",
      "                2.6,\n",
      "                5.6,\n",
      "                1.4\n",
      "            ],\n",
      "            [\n",
      "                7.7,\n",
      "                3.0,\n",
      "                6.1,\n",
      "                2.3\n",
      "            ],\n",
      "            [\n",
      "                6.3,\n",
      "                3.4,\n",
      "                5.6,\n",
      "                2.4\n",
      "            ],\n",
      "            [\n",
      "                6.4,\n",
      "                3.1,\n",
      "                5.5,\n",
      "                1.8\n",
      "            ],\n",
      "            [\n",
      "                6.0,\n",
      "                3.0,\n",
      "                4.8,\n",
      "                1.8\n",
      "            ],\n",
      "            [\n",
      "                6.9,\n",
      "                3.1,\n",
      "                5.4,\n",
      "                2.1\n",
      "            ],\n",
      "            [\n",
      "                6.7,\n",
      "                3.1,\n",
      "                5.6,\n",
      "                2.4\n",
      "            ],\n",
      "            [\n",
      "                6.9,\n",
      "                3.1,\n",
      "                5.1,\n",
      "                2.3\n",
      "            ],\n",
      "            [\n",
      "                5.8,\n",
      "                2.7,\n",
      "                5.1,\n",
      "                1.9\n",
      "            ],\n",
      "            [\n",
      "                6.8,\n",
      "                3.2,\n",
      "                5.9,\n",
      "                2.3\n",
      "            ],\n",
      "            [\n",
      "                6.7,\n",
      "                3.3,\n",
      "                5.7,\n",
      "                2.5\n",
      "            ],\n",
      "            [\n",
      "                6.7,\n",
      "                3.0,\n",
      "                5.2,\n",
      "                2.3\n",
      "            ],\n",
      "            [\n",
      "                6.3,\n",
      "                2.5,\n",
      "                5.0,\n",
      "                1.9\n",
      "            ],\n",
      "            [\n",
      "                6.5,\n",
      "                3.0,\n",
      "                5.2,\n",
      "                2.0\n",
      "            ],\n",
      "            [\n",
      "                6.2,\n",
      "                3.4,\n",
      "                5.4,\n",
      "                2.3\n",
      "            ],\n",
      "            [\n",
      "                5.9,\n",
      "                3.0,\n",
      "                5.1,\n",
      "                1.8\n",
      "            ]\n",
      "        ],\n",
      "        \"initial_weights\": [\n",
      "            [\n",
      "                [\n",
      "                    -1.9407156760215802,\n",
      "                    1.444835710018092,\n",
      "                    0.19286086320953463,\n",
      "                    -0.4208648204222881\n",
      "                ],\n",
      "                [\n",
      "                    1.7402534609379348,\n",
      "                    -0.3640867757814793,\n",
      "                    1.343954419584932,\n",
      "                    -0.8182209613286916\n",
      "                ],\n",
      "                [\n",
      "                    0.08270993579791167,\n",
      "                    -1.291058455346977,\n",
      "                    -0.6611042318158495,\n",
      "                    -1.1801910156259166\n",
      "                ],\n",
      "                [\n",
      "                    0.197642638146143,\n",
      "                    0.4138999766425578,\n",
      "                    1.1973219806427609,\n",
      "                    1.8833538579153102\n",
      "                ],\n",
      "                [\n",
      "                    0.7142238203796749,\n",
      "                    2.2843333544703444,\n",
      "                    1.5641025797286243,\n",
      "                    0.6111037098657089\n",
      "                ]\n",
      "            ],\n",
      "            [\n",
      "                [\n",
      "                    -0.8773633217919377,\n",
      "                    -1.6210874964712072,\n",
      "                    -0.5816729970873883,\n",
      "                    -0.5378339530983239,\n",
      "                    -1.5560236776953074\n",
      "                ],\n",
      "                [\n",
      "                    -0.05446484378125436,\n",
      "                    -1.8112787909180463,\n",
      "                    -0.6311752177478401,\n",
      "                    -0.928159184570213,\n",
      "                    1.4907219169590233\n",
      "                ],\n",
      "                [\n",
      "                    0.1954993352753277,\n",
      "                    -0.47160433354782627,\n",
      "                    1.8123546515314615,\n",
      "                    -2.2941374787994175,\n",
      "                    0.6512093458423532\n",
      "                ]\n",
      "            ]\n",
      "        ],\n",
      "        \"target\": [\n",
      "            [\n",
      "                0\n",
      "            ],\n",
      "            [\n",
      "                0\n",
      "            ],\n",
      "            [\n",
      "                0\n",
      "            ],\n",
      "            [\n",
      "                0\n",
      "            ],\n",
      "            [\n",
      "                0\n",
      "            ],\n",
      "            [\n",
      "                0\n",
      "            ],\n",
      "            [\n",
      "                0\n",
      "            ],\n",
      "            [\n",
      "                0\n",
      "            ],\n",
      "            [\n",
      "                0\n",
      "            ],\n",
      "            [\n",
      "                0\n",
      "            ],\n",
      "            [\n",
      "                0\n",
      "            ],\n",
      "            [\n",
      "                0\n",
      "            ],\n",
      "            [\n",
      "                0\n",
      "            ],\n",
      "            [\n",
      "                0\n",
      "            ],\n",
      "            [\n",
      "                0\n",
      "            ],\n",
      "            [\n",
      "                0\n",
      "            ],\n",
      "            [\n",
      "                0\n",
      "            ],\n",
      "            [\n",
      "                0\n",
      "            ],\n",
      "            [\n",
      "                0\n",
      "            ],\n",
      "            [\n",
      "                0\n",
      "            ],\n",
      "            [\n",
      "                0\n",
      "            ],\n",
      "            [\n",
      "                0\n",
      "            ],\n",
      "            [\n",
      "                0\n",
      "            ],\n",
      "            [\n",
      "                0\n",
      "            ],\n",
      "            [\n",
      "                0\n",
      "            ],\n",
      "            [\n",
      "                0\n",
      "            ],\n",
      "            [\n",
      "                0\n",
      "            ],\n",
      "            [\n",
      "                0\n",
      "            ],\n",
      "            [\n",
      "                0\n",
      "            ],\n",
      "            [\n",
      "                0\n",
      "            ],\n",
      "            [\n",
      "                0\n",
      "            ],\n",
      "            [\n",
      "                0\n",
      "            ],\n",
      "            [\n",
      "                0\n",
      "            ],\n",
      "            [\n",
      "                0\n",
      "            ],\n",
      "            [\n",
      "                0\n",
      "            ],\n",
      "            [\n",
      "                0\n",
      "            ],\n",
      "            [\n",
      "                0\n",
      "            ],\n",
      "            [\n",
      "                0\n",
      "            ],\n",
      "            [\n",
      "                0\n",
      "            ],\n",
      "            [\n",
      "                0\n",
      "            ],\n",
      "            [\n",
      "                0\n",
      "            ],\n",
      "            [\n",
      "                0\n",
      "            ],\n",
      "            [\n",
      "                0\n",
      "            ],\n",
      "            [\n",
      "                0\n",
      "            ],\n",
      "            [\n",
      "                0\n",
      "            ],\n",
      "            [\n",
      "                0\n",
      "            ],\n",
      "            [\n",
      "                0\n",
      "            ],\n",
      "            [\n",
      "                0\n",
      "            ],\n",
      "            [\n",
      "                0\n",
      "            ],\n",
      "            [\n",
      "                0\n",
      "            ],\n",
      "            [\n",
      "                1\n",
      "            ],\n",
      "            [\n",
      "                1\n",
      "            ],\n",
      "            [\n",
      "                1\n",
      "            ],\n",
      "            [\n",
      "                1\n",
      "            ],\n",
      "            [\n",
      "                1\n",
      "            ],\n",
      "            [\n",
      "                1\n",
      "            ],\n",
      "            [\n",
      "                1\n",
      "            ],\n",
      "            [\n",
      "                1\n",
      "            ],\n",
      "            [\n",
      "                1\n",
      "            ],\n",
      "            [\n",
      "                1\n",
      "            ],\n",
      "            [\n",
      "                1\n",
      "            ],\n",
      "            [\n",
      "                1\n",
      "            ],\n",
      "            [\n",
      "                1\n",
      "            ],\n",
      "            [\n",
      "                1\n",
      "            ],\n",
      "            [\n",
      "                1\n",
      "            ],\n",
      "            [\n",
      "                1\n",
      "            ],\n",
      "            [\n",
      "                1\n",
      "            ],\n",
      "            [\n",
      "                1\n",
      "            ],\n",
      "            [\n",
      "                1\n",
      "            ],\n",
      "            [\n",
      "                1\n",
      "            ],\n",
      "            [\n",
      "                1\n",
      "            ],\n",
      "            [\n",
      "                1\n",
      "            ],\n",
      "            [\n",
      "                1\n",
      "            ],\n",
      "            [\n",
      "                1\n",
      "            ],\n",
      "            [\n",
      "                1\n",
      "            ],\n",
      "            [\n",
      "                1\n",
      "            ],\n",
      "            [\n",
      "                1\n",
      "            ],\n",
      "            [\n",
      "                1\n",
      "            ],\n",
      "            [\n",
      "                1\n",
      "            ],\n",
      "            [\n",
      "                1\n",
      "            ],\n",
      "            [\n",
      "                1\n",
      "            ],\n",
      "            [\n",
      "                1\n",
      "            ],\n",
      "            [\n",
      "                1\n",
      "            ],\n",
      "            [\n",
      "                1\n",
      "            ],\n",
      "            [\n",
      "                1\n",
      "            ],\n",
      "            [\n",
      "                1\n",
      "            ],\n",
      "            [\n",
      "                1\n",
      "            ],\n",
      "            [\n",
      "                1\n",
      "            ],\n",
      "            [\n",
      "                1\n",
      "            ],\n",
      "            [\n",
      "                1\n",
      "            ],\n",
      "            [\n",
      "                1\n",
      "            ],\n",
      "            [\n",
      "                1\n",
      "            ],\n",
      "            [\n",
      "                1\n",
      "            ],\n",
      "            [\n",
      "                1\n",
      "            ],\n",
      "            [\n",
      "                1\n",
      "            ],\n",
      "            [\n",
      "                1\n",
      "            ],\n",
      "            [\n",
      "                1\n",
      "            ],\n",
      "            [\n",
      "                1\n",
      "            ],\n",
      "            [\n",
      "                1\n",
      "            ],\n",
      "            [\n",
      "                1\n",
      "            ],\n",
      "            [\n",
      "                2\n",
      "            ],\n",
      "            [\n",
      "                2\n",
      "            ],\n",
      "            [\n",
      "                2\n",
      "            ],\n",
      "            [\n",
      "                2\n",
      "            ],\n",
      "            [\n",
      "                2\n",
      "            ],\n",
      "            [\n",
      "                2\n",
      "            ],\n",
      "            [\n",
      "                2\n",
      "            ],\n",
      "            [\n",
      "                2\n",
      "            ],\n",
      "            [\n",
      "                2\n",
      "            ],\n",
      "            [\n",
      "                2\n",
      "            ],\n",
      "            [\n",
      "                2\n",
      "            ],\n",
      "            [\n",
      "                2\n",
      "            ],\n",
      "            [\n",
      "                2\n",
      "            ],\n",
      "            [\n",
      "                2\n",
      "            ],\n",
      "            [\n",
      "                2\n",
      "            ],\n",
      "            [\n",
      "                2\n",
      "            ],\n",
      "            [\n",
      "                2\n",
      "            ],\n",
      "            [\n",
      "                2\n",
      "            ],\n",
      "            [\n",
      "                2\n",
      "            ],\n",
      "            [\n",
      "                2\n",
      "            ],\n",
      "            [\n",
      "                2\n",
      "            ],\n",
      "            [\n",
      "                2\n",
      "            ],\n",
      "            [\n",
      "                2\n",
      "            ],\n",
      "            [\n",
      "                2\n",
      "            ],\n",
      "            [\n",
      "                2\n",
      "            ],\n",
      "            [\n",
      "                2\n",
      "            ],\n",
      "            [\n",
      "                2\n",
      "            ],\n",
      "            [\n",
      "                2\n",
      "            ],\n",
      "            [\n",
      "                2\n",
      "            ],\n",
      "            [\n",
      "                2\n",
      "            ],\n",
      "            [\n",
      "                2\n",
      "            ],\n",
      "            [\n",
      "                2\n",
      "            ],\n",
      "            [\n",
      "                2\n",
      "            ],\n",
      "            [\n",
      "                2\n",
      "            ],\n",
      "            [\n",
      "                2\n",
      "            ],\n",
      "            [\n",
      "                2\n",
      "            ],\n",
      "            [\n",
      "                2\n",
      "            ],\n",
      "            [\n",
      "                2\n",
      "            ],\n",
      "            [\n",
      "                2\n",
      "            ],\n",
      "            [\n",
      "                2\n",
      "            ],\n",
      "            [\n",
      "                2\n",
      "            ],\n",
      "            [\n",
      "                2\n",
      "            ],\n",
      "            [\n",
      "                2\n",
      "            ],\n",
      "            [\n",
      "                2\n",
      "            ],\n",
      "            [\n",
      "                2\n",
      "            ],\n",
      "            [\n",
      "                2\n",
      "            ],\n",
      "            [\n",
      "                2\n",
      "            ],\n",
      "            [\n",
      "                2\n",
      "            ],\n",
      "            [\n",
      "                2\n",
      "            ],\n",
      "            [\n",
      "                2\n",
      "            ]\n",
      "        ],\n",
      "        \"learning_parameters\": {\n",
      "            \"learning_rate\": 0.1,\n",
      "            \"batch_size\": 2,\n",
      "            \"max_iteration\": 1,\n",
      "            \"error_threshold\": 0.0\n",
      "        }\n",
      "    },\n",
      "    \"expect\": {\n",
      "        \"stopped_by\": \"max_iteration\",\n",
      "        \"final_weights\": []\n",
      "    }\n",
      "}\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"Y6uTvCHlacPKb5B4hTEdKl",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "import numpy as np\n",
    "\n",
    "# Fungsi Aktivasi Turunan\n",
    "\n",
    "activation_functions = {\n",
    "        \"linear\": lambda x: x,\n",
    "        \"relu\": lambda x: np.maximum(0, x),\n",
    "        \"sigmoid\": lambda x: 1\/(1+np.exp(-x)),\n",
    "        \"softmax\": lambda x: np.exp(x)\/np.sum(np.exp(x))\n",
    "    }\n",
    "\n",
    "def softmax_derivation(x: np.array, i: int):\n",
    "    result = np.array(np.vectorize(lambda y: y-1)(x))\n",
    "    result[i] = x[i]\n",
    "    return result\n",
    "\n",
    "derived_activation_functions = {\n",
    "        \"linear\": lambda x: np.ones(len(x)),\n",
    "        \"relu\": lambda x: np.array(np.vectorize(lambda y: 1 if y >= 0 else 0)(x)),\n",
    "        \"sigmoid\": lambda x: activation_functions[\"sigmoid\"](x) * (1 - activation_functions[\"sigmoid\"](x)),\n",
    "        \"softmax\": softmax_derivation\n",
    "}\n",
    "\n",
    "# Fungsi Loss\n",
    "def sse(o: np.array, t: np.array):\n",
    "    return 0.5*np.sum((t-o)**2)\n",
    "\n",
    "def softmax_loss(o: np.array, i: int):\n",
    "    return o[i]\n",
    "\n",
    "notusedloss_functions = {\n",
    "        \"linear\": sse,\n",
    "        \"relu\": sse,\n",
    "        \"sigmoid\": sse,\n",
    "        \"softmax\": softmax_loss\n",
    "}"
   ],
   "execution_count":403,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"kdfFq3T6jZJXyfh1akIKWS",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "import json\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple\n",
    "\n",
    "@dataclass\n",
    "class TestCase:\n",
    "    expected_output: List[List[float]]\n",
    "    input_size: int\n",
    "    layers: List[Tuple[int, str]]\n",
    "    weights: List[np.array]\n",
    "    input_data: np.array\n",
    "    max_sse: float\n",
    "\n",
    "    def print_info(self):\n",
    "        print(f\"Layers\\n{self.layers}\\n\\nWeights{self.weights}\\n\\n\")\n",
    "        print(f\"Input size\\n{self.input_size}\\n\\nInput data\\n{self.input_data}\\n\\n\")\n",
    "        print(f\"Expected output\\n{self.expected_output}\\n\\nMax SSE\\n{self.max_sse}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def from_file(filename: str):\n",
    "        json_read = json.load(open(filename, \"r\"))\n",
    "        max_sse = json_read[\"expect\"][\"max_sse\"]\n",
    "        output = json_read[\"expect\"][\"output\"]\n",
    "        input_size = json_read[\"case\"][\"model\"][\"input_size\"]\n",
    "        layers_raw = json_read[\"case\"][\"model\"][\"layers\"]\n",
    "        layers = []\n",
    "        for element in layers_raw:\n",
    "            layers.append(tuple([element[\"number_of_neurons\"], element[\"activation_function\"]]))\n",
    "\n",
    "        weights_raw = json_read[\"case\"][\"weights\"]\n",
    "        weights = []\n",
    "\n",
    "        for weight_raw in weights_raw:\n",
    "            weight = []\n",
    "            for j in weight_raw:\n",
    "                weight.append(np.array(j))\n",
    "            weights.append(np.array(weight).T)\n",
    "\n",
    "        input_data = np.array(json_read[\"case\"][\"input\"])\n",
    "\n",
    "        return TestCase(\n",
    "            expected_output=output,\n",
    "            input_size=input_size,\n",
    "            layers=layers,\n",
    "            weights=weights,\n",
    "            input_data=input_data,\n",
    "            max_sse=max_sse\n",
    "        )"
   ],
   "execution_count":404,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"8FApE31lfnh8gZwVyXtoX0",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "- learning_rate (during fit)\n",
    "- activation (per layer)\n",
    "- epochs (during fit)\n",
    "- batch_size (during fit)\n",
    "- threshold (during fit)"
   ],
   "attachments":{},
   "metadata":{
    "datalore":{
     "node_id":"Wg7MGc1j2BAVxgJwqIzCue",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "import graphviz as gv\n",
    "\n",
    "class Neuron:\n",
    "    weight: np.array\n",
    "    d_weight: List[np.array]\n",
    "    result: float\n",
    "\n",
    "    def __init__(self, weight: np.array):\n",
    "        self.weight = weight\n",
    "        self.d_weight = []\n",
    "\n",
    "    \"\"\"\n",
    "    return bias + wt•x\n",
    "    \"\"\"\n",
    "    def compute(self, input_data: np.array):\n",
    "        self.result = self.weight[0] + np.dot(input_data, self.weight[1:])\n",
    "        return self.result\n",
    "    \n",
    "    def init_d_weight(self, batch_size: int):\n",
    "        self.d_weight = [np.arange(len(self.weight)) for _ in range(batch_size)]\n",
    "\n",
    "    def _reset_d_weight(self):\n",
    "        self.d_weight = []\n",
    "\n",
    "    def persist_d_weight(self):\n",
    "        for i in range(len(self.weight)):\n",
    "            self.weight[i] += np.sum(self.d_weight[i]) \/ len(self.d_weight[i])\n",
    "        self.reset_d_weight()\n",
    "\n",
    "class Layer:\n",
    "    neurons: List[Neuron]\n",
    "    raw_result : np.array\n",
    "    result : np.array\n",
    "    # As long as the amount of neuron in the layer before it\n",
    "    do_c_do_a: List[float]\n",
    "    result_derivative : np.array\n",
    "    activation_function_name: str\n",
    "    activation_functions = {\n",
    "        \"linear\": lambda x: x,\n",
    "        \"relu\": lambda x: np.maximum(0, x),\n",
    "        \"sigmoid\": lambda x: 1\/(1+np.exp(-x)),\n",
    "        \"softmax\": lambda x: np.exp(x)\/np.sum(np.exp(x))\n",
    "    }\n",
    "\n",
    "    activation_functions_derivative = {\n",
    "        \"linear\": lambda x: np.ones(len(x)),\n",
    "        \"relu\": lambda x: np.array(np.vectorize(lambda y: 1 if y >= 0 else 0)(x)),\n",
    "        \"sigmoid\": lambda x: activation_functions[\"sigmoid\"](x) * (1 - activation_functions[\"sigmoid\"](x)),\n",
    "        \"softmax\": softmax_derivation\n",
    "    }\n",
    "\n",
    "    def __init__(self, n_neurons: int, activation_function_name: str, weights: List[np.array]):\n",
    "        self.neurons = []\n",
    "        # Initialize neurons in the layer\n",
    "        for i in range(0, n_neurons):\n",
    "            self.neurons.append(Neuron(weights[i]))\n",
    "        self.activation_function_name = activation_function_name\n",
    "\n",
    "    def predict(self, input_data: np.array) -> np.array:\n",
    "        self.raw_result = np.array([neuron.compute(input_data) for neuron in self.neurons])\n",
    "        self.result = self.activation_functions[self.activation_function_name](self.raw_result)\n",
    "        return self.result\n",
    "    \n",
    "    def predict_derivative(self):\n",
    "        self.result_derivative = self.activation_functions_derivative[self.activation_function_name](self.raw_result)\n",
    "        return self.result_derivative\n",
    "\n",
    "    def persist_d_weight(self):\n",
    "        for neuron in self.neurons:\n",
    "            neuron.persist_d_weight()\n",
    "    \n",
    "    def init_d_weight(self, batch_size: int):\n",
    "        for neuron in self.neurons:\n",
    "            neuron.init_d_weight(batch_size)\n",
    "        \n",
    "class Model:\n",
    "    input_size: int\n",
    "    layers: List[Layer]\n",
    "    learning_rate: int\n",
    "\n",
    "    @staticmethod\n",
    "    def from_test_case(test_case: TestCase):\n",
    "        return Model(test_case.input_size, test_case.layers, test_case.weights)\n",
    "\n",
    "    def __init__(self, input_size: int, layers_attr: List[Tuple[int, str]], weights: List[np.array]):\n",
    "        self.input_size = input_size\n",
    "        # Initialize layers\n",
    "        self.layers = []\n",
    "        for i in range(0, len(layers_attr)):\n",
    "            layer: Layer = Layer(layers_attr[i][0], layers_attr[i][1], weights[i])\n",
    "            self.layers.append(layer)\n",
    "    \n",
    "    def init_d_weight(self, batch_size: int):\n",
    "        for layer in self.layers:\n",
    "            layer.init_d_weight(batch_size)\n",
    "\n",
    "    def persist_d_weight(self):\n",
    "        for layer in self.layers:\n",
    "            layer.persist_d_weight()\n",
    "\n",
    "    def _predict(self, input_data: np.array) -> np.array:\n",
    "        layer: Layer = self.layers[0]\n",
    "        temp_array = layer.predict(input_data)\n",
    "        for i in range(1, len(self.layers)):\n",
    "            layer: Layer = self.layers[i]\n",
    "            temp_array = layer.predict(temp_array)\n",
    "        \n",
    "        return temp_array\n",
    "        \n",
    "    def predict_batch(self, input_data: List[np.array]) -> List[np.array]:\n",
    "        # Batch output\n",
    "        final_output = []\n",
    "        for data in input_data:\n",
    "            final_output.append(self._predict(data))\n",
    "        return final_output\n",
    "\n",
    "    def visualize(self):\n",
    "        graph = gv.Digraph(filename=\".\/output\/graph.gv\")\n",
    "        for i in range(self.input_size):\n",
    "            graph.node(f\"IN{i}\", f\"Input Neuron-{i + 1}\")\n",
    "        graph.render()\n",
    "\n",
    "    def visualize_print(self):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            print(f\"\/* Hidden Layer-{i + 1} ({layer.activation_function_name}) *\/\")\n",
    "            for j, neuron in enumerate(layer.neurons):\n",
    "                print(f\"\/* Neuron-{j + 1} *\/\")\n",
    "                if i == 0:\n",
    "                    source_layer = \"Input Layer\"\n",
    "                else:\n",
    "                    source_layer = f\"Hidden Layer-{i}\"\n",
    "                for k, w in enumerate(neuron.weight):\n",
    "                    if k == 0:\n",
    "                        continue\n",
    "                    if w != 0:\n",
    "                        print(f\"{source_layer} Neuron-{k} -> Hidden Layer-{i + 1} Neuron-{j+1} ({w})\")\n",
    "                print(\"\\n\\n\")\n",
    "            print(\"\\n\\n\")\n",
    "\n",
    "    def _generate_mini_batches(self, input_data: List[np.array], expected_output: List[np.array], batch_size: int):\n",
    "        start_index = 0\n",
    "        input_size = len(input_data)\n",
    "        mini_batches_input = []\n",
    "        mini_batches_output = []\n",
    "        while start_index < input_size:\n",
    "            end_index = min(start_index + batch_size, input_size)\n",
    "            mini_batches_input.append(input_data[start_index:end_index])\n",
    "            mini_batches_output.append(expected_output[start_index:end_index])\n",
    "            start_index += batch_size\n",
    "        \n",
    "        return mini_batches_input, mini_batches_output\n",
    "    \n",
    "    def _backpropagate(self,expected_output: np.array, nth_batch: int):\n",
    "\n",
    "        for i in range(len(self.layers) - 1, 1, -1):\n",
    "            layer_current = self.layers[i]\n",
    "            layer_previous = None if i == 0 else self.layers[i - 1]\n",
    "            layer_next = None if i == len(self.layers) - 1 else self.layers[i + 1]\n",
    "\n",
    "            if layer_next is not None :\n",
    "                # Update weight of hidden layer\n",
    "\n",
    "                # Calculate do_c_do_a to be used in calculating do_c_do_w\n",
    "                # do_c_do_a of output layer has a special value, calculated separately below\n",
    "                # this is for when layer next is hidden layer\n",
    "                if i != len(self.layers) - 2:\n",
    "                    layer_next.predict_derivative()\n",
    "                    for nth_neuron, neuron_current in enumerate(layer_next.neurons):\n",
    "                        layer_next.do_c_do_a[nth_neuron] = np.sum(neuron_current.weight  * layer_next.result_derivative[nth_neuron] * self.layers[i + 2].do_c_do_a[nth_neuron])\n",
    "                \n",
    "                # Calculate do_c_do_w\n",
    "                for nth_neuron, neuron_current in enumerate(self.layers[i].neurons):\n",
    "                    for nth_weight, weight_current in enumerate(neuron_current.weight):\n",
    "                        do_c_do_w = layer_previous.result[nth_weight] * layer_current.result_derivative[nth_neuron] * layer_next.do_c_do_a[nth_neuron]\n",
    "                        neuron_current.d_weight[nth_batch][nth_weight] = self.learning_rate * do_c_do_w\n",
    "            else:\n",
    "                # Update weight of output layer\n",
    "                layer_current.do_c_do_a = np.subtract(layer_current.result, expected_output)\n",
    "                for nth_neuron, neuron_current in enumerate(layer_current.neurons):\n",
    "                    for nth_weight, weight_current in enumerate(neuron_current.weight):\n",
    "                        neuron_previous_activation = layer_previous.result[nth_weight]\n",
    "                        do_c_do_w = neuron_previous_activation * layer_current.result_derivative[nth_neuron] * layer_current.do_c_do_a[nth_neuron]\n",
    "                \n",
    "                        neuron_current.d_weight[nth_batch][nth_weight] = self.learning_rate * do_c_do_w * neuron_previous_activation\n",
    "            \n",
    "    # Update weight for batches\n",
    "    # run the backpropagation for all the data in the current batch\n",
    "    # after it is done, we aggregate the weight changes from the _backpropagate function, then we apply to the model\n",
    "    def backpropagate(self, batch_result_expected: np.array):            \n",
    "        for i, result_expected in enumerate(batch_result_expected):\n",
    "            self._backpropagate(result_expected, i)\n",
    "        self.persist_d_weight()\n",
    "\n",
    "    def train(self, input_data: List[np.array], expected_output: List[np.array], error_threshold: float, max_iteration: int, batch_size: int, learning_rate: int):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.init_d_weight(batch_size)\n",
    "        # Epoch\n",
    "        for _ in range(max_iteration):\n",
    "            for mini_batch_input, mini_batch_output in self._generate_mini_batches(input_data, expected_output):\n",
    "                # Get the output first\n",
    "                self.predict_batch(mini_batch_input)\n",
    "                self.backpropagate(mini_batch_output)\n",
    "\n",
    "                # TODO: Define error\n",
    "                error = 0\n",
    "                if (error < error_threshold): \n",
    "                    break\n",
    "        pass"
   ],
   "execution_count":405,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"f1NNnhNCLhfuc1Q7y5GaDV",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "def calculate_sse(output_data: List[np.array], expected_data: List[np.array]):\n",
    "    if len(output_data) != len(expected_data):\n",
    "        raise ValueError(\"Output and Expected Data length doesn't match.\")\n",
    "    \n",
    "    sses = []\n",
    "\n",
    "    for output, expected in zip(output_data, expected_data):\n",
    "        delta = output - expected\n",
    "        squared_delta = delta ** 2\n",
    "        sses.append(np.sum(squared_delta))\n",
    "\n",
    "    return sses\n",
    "\n",
    "def evaluate_result(output_data: List[np.array], output_reference_data: np.array, expected_data: List[np.array], max_sse: float):\n",
    "    sses = calculate_sse(output_data, expected_data)\n",
    "\n",
    "    for i in range(len(output_data)):\n",
    "        predicted = output_data[i]\n",
    "        predicted_reference = output_reference_data[i]\n",
    "        expected = expected_data[i]\n",
    "        sse = sses[i]\n",
    "\n",
    "        print(f\"Prediction result:\\n{predicted}\")\n",
    "        print(f\"Prediction reference result:\\n{predicted_reference}\")\n",
    "        print(f\"Expected output:\\n{expected}\")\n",
    "\n",
    "        print(f\"sse {sse}\\tmax sse{max_sse}\")\n",
    "        print(f\"Is below error? {sse < max_sse}\")"
   ],
   "execution_count":406,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"sN2CI0xDQ4Vxyol0nAqe1B",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "Setelah dilakukan Feed Forward yang memprediksi hasil dari sebuah data, dilakukan Backpropagation untuk mengupdate weight dan juga aktivasi dari neuron dalam model.\n",
    "\n",
    "Untuk mengupdate weight, digunakan persamaan\n",
    "\n",
    "\\begin{equation}\n",
    "w_{jk} = w_{jk} - \\Delta w_{jk}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\Delta w_{jk} = -n \\frac{\\partial C}{\\partial w^{(l)}_{jk}}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "w_{jk} = w_{jk} + n \\frac{\\partial C}{\\partial w^{(l)}_{jk}}\n",
    "\\end{equation}\n",
    "\n",
    "dimana\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial C}{\\partial w^{(l)}_{jk}} = \\frac{\\partial z^{(l)}\\_{j}}{\\partial w^{(l)}\\_{jk}} \\frac{\\partial a^{(l)}\\_{j}}{\\partial z^{(l)}\\_j} \\frac{\\partial C}{\\partial a^{(l)}_j}\n",
    "\\end{equation}\n",
    "\n",
    "dan\n",
    "\n",
    "\\begin{equation}\n",
    "z{^{(l)}_j} = \\Sigma{^n\\_{k=0}} w^{(l)}\\_{jk} a^{(l -1)}_k + b_j^{(l)} \\rightarrow \\\\ \\frac{\\partial z^{(l)}\\_{j}}{\\partial w^{(l)}\\_{jk}} = a_k^{(l-1)}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "a{^{(l)}_j} = \\sigma ( z{^{(l)}_j} ) \\rightarrow \\\\ \\frac{\\partial a^{(l)}\\_{j}}{\\partial z^{(l)}\\_j} = \\sigma '(z_j^{(l)})\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}$\n",
    "\\frac{\\partial C}{\\partial a^{(l)}_j}\n",
    " = \\left\\{ \n",
    "  \\begin{array}{ c l }\n",
    "    \\Sigma{_{j=0}^{n_{(l+1)}-1}} w^{(l+1)}_{jk} \\sigma '(z_j^{(l+1)}) \\frac{\\partial C}{\\partial a^{(l+1)}_j}  & \\quad \\textrm{for hidden layer}\\\\\n",
    "    (a_j^{(l)} - y_j)                 & \\quad \\textrm{for output layer}\n",
    "  \\end{array}\n",
    "\\right.$\n",
    "\\end{equation}\n",
    "\n",
    "sehingga didapatkan\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial C}{\\partial w^{(l)}_{jk}} = a_k^{(l-1)} \\sigma '(z_j^{(l)}) \\frac{\\partial C}{\\partial a^{(l)}_j}\n",
    "\\end{equation}\n",
    "\n",
    "maka untuk setiap layer, dapat dihitung \n",
    "\\begin{equation} \n",
    "\\frac{\\partial C}{\\partial a^{(l)}_j}\n",
    "\\end{equation}\n",
    "\n",
    "yang akan digunakan untuk menghitung\n",
    "\\begin{equation} \n",
    "\\frac{\\partial C}{\\partial w^{(l)}_{jk}}\n",
    "\\end{equation}\n",
    "yang akan digunakan untuk menghitung bobot, dimana bobot akan di-persist setiap mini-batch"
   ],
   "attachments":{},
   "metadata":{
    "datalore":{
     "node_id":"31B4b89eh2pTv5BQOVPZ6G",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[],
   "execution_count":0,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"I0YkAozM4AtxktKdJ25J2e",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "# Iris"
   ],
   "attachments":{},
   "metadata":{
    "datalore":{
     "node_id":"Iris",
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "sheet_delimiter":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "import tensorflow as tf\n",
    "from typing import List\n",
    "from dataclasses import dataclass\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD\n",
    "from keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "import numpy as np\n",
    "import json"
   ],
   "execution_count":407,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"V5X543jL3DMSmSQAdiQx6K",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "# Setup\n",
    "\n",
    "# np.random.seed(0)\n",
    "\n",
    "# DEBUG_MODE: bool = True\n",
    "# TESTCASE_MODE: bool = True\n",
    "# TESTCASE_REFERENCE_MODE: bool = True\n",
    "\n",
    "# def run_test_case_mode(filename):\n",
    "#     if TESTCASE_MODE:\n",
    "#         run_test_case(filename)\n",
    "\n",
    "# def run_test_case_reference_mode(filename):\n",
    "#     if TESTCASE_MODE:\n",
    "#         run_test_on_reference(filename)\n",
    "\n",
    "# def print_debug(value: str):\n",
    "#     if DEBUG_MODE:\n",
    "#         print(value)"
   ],
   "execution_count":408,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"bNdUPaKElx6aRDVXiE1TMK",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[],
   "execution_count":0,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"oiLPqnKUEOEGiFX0WqOS6k",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[],
   "execution_count":0,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"oHIDD75JrPwFouADjEmAko",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[],
   "execution_count":0,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"Zq0IeqWvQsSVeTEAw5FCUq",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  }
 ],
 "metadata":{
  "kernelspec":{
   "display_name":"Python",
   "language":"python",
   "name":"python"
  },
  "datalore":{
   "computation_mode":"JUPYTER",
   "package_manager":"pip",
   "base_environment":"default",
   "packages":[
    {
     "name":"graphviz",
     "version":"0.20.2",
     "source":"PIP"
    }
   ],
   "report_row_ids":[],
   "version":3
  }
 },
 "nbformat":4,
 "nbformat_minor":4
}